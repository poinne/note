训练一个模型，输入一个三维离散网格，以及一些控制点（插值点和障碍点），输出一条满足控制点约束和光滑性约束的曲线。

思想：利用深度学习+水平集来实现。

通过深度学习来解定义在网格上的水平集函数，得到最终的零水平集，并采样输出。

---

**水平集方法：**

水平集方法通过比目标维度高一维的水平集函数（LSF）的零水平集隐式地表示曲线，能够自然地处理目标的复杂拓扑变化。

一般来说，水平集方法需要在目标周围初始化水平集函数，再基于精心涉及的能量函数，通过求解偏微分方程迭代更新水平集函数实现曲线演变，达到最小化能量函数的目标，此时LSF的零水平集即为目标边界。

水平集函数的演变通常由一阶PDE描述：
$$
\frac{\partial \phi}{\partial t} + F(\nabla\phi) = 0
$$
其中，$\phi$是水平集函数，$F$是水平集的几何性质相关的函数，在这个任务中，表示水平集各点的速度。

当涉及到界面或边界的曲率时，可能会引入高阶导数。

我的理解：上述PDE表示 $\phi$ 随着时间 $t$ 的变化，梯度 $\nabla\phi$ 的变化情况。 $\nabla\phi$ 表示的是各个位置变化的方向，$F$ 则是表示各点变化的速度。

---

**深度学习求解PDE：**

目前，深度学习应用于偏微分方程的求解已经有了大量的文章，针对目前已经看过的文章，总结如下：

对于低维的PDE，learning方法通常没有优化方法快，但是由于简单性以及适用于解决大量偏微分方程问题的多功能性，learning方法在低维PDE中也有很多应用，最常用的方法是FINN。

**FINNs**

[[博客](https://zhuanlan.zhihu.com/p/411843646)]

FINNs全称为 （Physics-informed neural networks），与传统的数据驱动的神经网络不同，PINNs 在学习过程中利用物理法则对模型进行指导，从而提高模型泛化能力，特别是在数据较少或噪声较大的情况下。

PINNs 通过训练神经网络来最小化一个损失函数，从而生成 PDE 的近似解。

PINN可以通过少量的数据进行PDE的求解，具体地，可以只通过初值和边界值以及PDE方程进行学习和求解。

为了实现只有初边值条件和方程即可约束神经网络，需要在损失函数上花心思，毕竟损失函数决定了神经网络训练的方向。PINNs的损失函数分为两块，一块是初始条件和边界条件，一块是方程。

- 表示空间-时间域边界上初始条件和边界条件偏差的项，

  使用初值和边界条件做约束条件比较简单，直接做一个MSE即可。

- 表示域内选定点上 PDE 残差的项。

  这部分是PINN的关键，即偏微分方程的残差，当输出的u和对应的导数满足方程时，f是等于0的。因此目标是让f尽可能的接近0。这样就实现了即使不知道真值u，也能计算出方程的Loss用于指导神经网络参数的更新，实现了非监督学习的效果。

通过最小化这些偏差，神经网络能够逼近 PDE 的解。

PINNs 的优势在于其灵活性，因为它们可以应用于各种具有挑战性的偏微分方程，而经典数值近似通常需要根据特定偏微分方程的具体情况进行定制。

损失函数中PDE残差的项需要对t进行求导，以及计算一些偏微分算子，这些都可以通过pytorch库提供的自动微分得到。

举例如下：

对于偏微分方程：

![image-20241012150835037](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241012150835037.png)

可以构造对应的损失函数：

![image-20241012150858220](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241012150858220.png)

因为PDE本身就是0，因此只需要最小化PDE的值的平方即可，而初始和边界条件则需要最小化其与gt值之差的平方。

**PINN训练过程**

PINN 训练时，输入的数据主要分为两类：

1. **边界条件和初始条件的数据**：
   - PINN的一个关键特点是，它可以处理具有**小数据**的PDE问题。因此，网络的训练数据通常是偏微分方程的**边界条件**和**初始条件**.
   - 这部分的数据属于有监督训练，即对于每个预测的值，都会提供对应的真值用于loss计算。
   - 这部分的数据，训练时初边值的loss和PDE残差的loss都会计算。

2. **内部点的采样数据**：
   - 为了使神经网络逼近PDE的解，PINN在**空间-时间域的内部**随机选择一些点。在这些点上，网络通过自动微分来计算PDE的残差（例如偏导数），并将PDE的残差引入到损失函数中进行优化。
   - 这些内部点并不需要提供确切的解值，而是通过自动微分评估解是否符合PDE的要求。因此，PINN能够在缺乏大量训练数据的情况下通过物理约束来指导网络的训练。
   - 这部分的数据属于无监督训练，即对于每个预测的值，没有对应的真值用于loss计算。
   - 这部分的数据，训练时只会计算PDE残差的loss。

PINN的输入特征：

- **时空坐标点**：即 \( (t, x) \) 的值，这些点可以是时间和空间域内的任意位置。
- **边界条件和初始条件**：这些条件会提供在特定时刻和特定空间边界处的已知解，作为训练的监督数据。

PINN 的训练过程主要是通过将输入的时空坐标和边界/初始条件结合在一起，并通过最小化损失函数（包含PDE残差、边界条件、初始条件的偏差）来优化神经网络的参数。



PINN模型也有许多明显的不足：

- 由于模型内部缺乏偏微分方程信息，PINN 模型的预测可能不符合物理原理。因此，这些方法不适用于构建可概括形状和边界条件变化的模型。（即模型的泛化能力很差)

---

**深度学习处理网格：**

深度学习处理网格可以分为两类：

基于图结构的方法和基于网格结构的方法。



**基于图结构的方法：**

又可以分为两类，基于图谱理论、使用图邻接对欧式空间卷积进行扩展。

这两种方式的区别主要是对于卷积的定义不同

- 基于图谱理论的卷积

  在图谱理论中，图的卷积基于图的**拉普拉斯矩阵**的特征向量（即傅里叶基）。

  首先通过傅里叶变换将信号和卷积核映射到频域，然后在频域中逐元素相乘，最后通过逆变换将结果映射回时域。

  这种方式的卷积需要求解全图的拉普拉斯矩阵并对其进行分解，在图规模很大时，代价很大，所以有很多工作对这种卷积进行了修改。

- 基于空间域的卷积

  基于空间域的卷积通常是欧几里得域中卷积的模仿。因为图节点的邻居节点数量不一致，有各种的方法进行处理。

**基于网格结构的方法：**

 基于图结构的方法只考虑了图结构，而对于 3D 空间中的网格，还有其他独特的属性可以使用。

研究人员提出了许多网格学习模型，它们可以根据处理策略分为两类。一种是在3D欧氏空间中定义卷积，完全忽略网格结构，称为外在卷积。另一种是根据mesh代表2-流形的点，模仿2D平面上的定义来定义卷积，通常称为内在卷积。

**外在卷积**

通过外部的视角和规则对网格模型进行卷积。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20241010203140425.png" alt="image-20241010203140425" style="zoom:67%;" />

- 通过多个视角的2D图像对3D空间中的表面进行学习。

- 将网格对象转为体素进行卷积。

这类方法无法得到和利用网格内部的属性，只能对网格的外在进行处理。


**内在卷积**

因为mesh 的局部是2-manifold，与平面局部同胚，可以使用类似欧式空间中的卷积。

**GCNN（Geodesic Convolutional Neural Networks）**

- **定义**：GCNN 使用定义在测地盘（geodesic disk）上的函数来提取局部特征。
- 工作原理
  - 测地盘是以某个顶点为中心，按照测地距离选择邻域内的点。
  - 在这个测地盘上，GCNN 定义卷积操作，以捕捉该区域内的特征。
  - 这种方法强调了点之间的测地距离，使得卷积操作能够更好地反映流形的几何特性。

**ACNN（Algebraic Convolutional Neural Networks）**

- **定义**：ACNN 利用热核（heat kernel）来提取特征。
- 工作原理
  - 热核是描述在流形上热传导过程的一种数学工具，它能有效地捕捉局部和全局特征。
  - ACNN 通过使用热核作为卷积核，计算流形上点的特征，重点关注点之间的关系和相互影响。
  - 这种方法有助于捕捉流形的平滑性和连续性，适用于处理具有复杂结构的场景。

**DCNN（Diffusion Convolutional Neural Networks）**

- **定义**：DCNN 基于概率转移矩阵进行特征提取。
- 工作原理
  - 概率转移矩阵描述了在图上信息如何通过边传播。这可以视为信息在图中的扩散过程。
  - DCNN 利用这一扩散过程来定义卷积操作，将来自邻居的信息聚合到中心点。
  - 这种方法可以捕捉到网络中信息传播的动态过程，适用于处理具有复杂连接关系的图结构。

___

**深度学习在网格上求解PDE**



---

**深度学习求解水平集**



---

##### 整理1

1. 如何表示mesh上的lsf？

- dl中如何处理mesh的输入
  - 基于图结构的方法，输入为顶点之间的邻接关系，通过邻接矩阵的方式作为输入。
    - 在模型内部，图的表示可以包含邻接矩阵和特征矩阵，经过图卷积操作后得到新的顶点特征。
    - 每个节点（顶点）在模型中拥有一个特征向量，经过多个卷积层和非线性激活后，节点特征会被逐步更新，反映与邻居的关系。
    - 在某些情况下，可以通过全局池化层（如最大池化或平均池化）来获得整个图的全局特征表示，用于后续的分类或回归任务。
    - **卷积的方法也是使用图卷积的方法来做。**
  - 基于网格结构的方法，输入为顶点坐标(N * 3)，面信息（M * K）[K边形]，特征信息（附加的顶点或面信息）。
    - 输入数据在模型中通常以顶点坐标和面索引的形式进行处理。每个顶点都有对应的特征向量，模型通过卷积操作在三角面上提取特征。
    - 网络可能会对每个顶点的邻域（局部补丁）进行特征提取，通过对邻域内的顶点特征进行聚合来更新每个顶点的特征。
    - **卷积的方法会利用网格的内在属性。**
  - **这两种方式的区别主要是对于卷积的定义不同。**
- 如何将lsf加入mesh输入中
  - 各顶点的特征向量多一维，存lsf值
  - 将特征向量通过FC层降维，得到lsm \\ 体素
  - 根据输入构建水平集图（二维）
  - nerf的方式，将一个场景模型的表面存储在神经网络中。缺点是一个模型只能保存一个曲面

2. 如何训练：

- 用sdf？用sdf loss做监督
- 在输入的对象（图像\模型）上定义sdf，并设计sdf loss来进行模型训练。训练过程和一般的模型训练过程相同。
- 用lsf？解pde，类似PINN
  - 在输入的对象（图像\模型）上定义lsf，将要求解的水平集函数和一些约束条件作为loss，并使用自动微分和手动推导的水平集梯度进行梯度计算，从而更新水平集。

---

##### 整理2

1. 如何隐式表示曲线曲面？
   - 在输入的图像或模型上定义水平集，更新水平集，最终使水平集函数$\phi(x) = 0$的点即为曲线或曲面。
   
   - 用大量数据来训练一个过拟合的模型，来隐式表示曲线或曲面(NeuroGF,Nerf)。
   
     

---

1. 使用GNN处理mesh输出，并提取特征，结合RNN（LSTM）进行水平集的更新，但是PhyCRNet 模型使用CNN+LSTM，相比于传统方法，并没有什么亮点，不过比PINN方法的外推性和泛化性好一点。如果使用GNN+LSTM的话，也要找到优势才行，比如速度快，精度高（不太现实），泛化性能强等等。																				





























