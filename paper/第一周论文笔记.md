##### 论文1：Learning high-order geometric flow based on the level set method

这篇文章提出了一个新的框架，来求解高阶几何流。然后用提出的框架求解两个GPDE，并用实验验证了该框架在不同的模型上的效果。

作者将求解PDE问题看作PDE约束的优化问题，结合PINN和LSM，求解高阶的GPDE（几何偏微分方程）

【(该框架就是PINN + LSM)，整体的思想和做法和PINN一样，不同点在于：

1. PINN求解的一般都是简单且低阶的PDE，该框架求解的是复杂且高阶的几何流(GPDE)
2. PINN对数据的需求比较少，只需要初边值即可，该框架则是通过数据驱动。

该框架的算法步骤如下：

![image-20241014123255074](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014123255074.png)

其中，梯度下降的公式为：

![image-20241014123452603](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014123452603.png)

需要减去两项，分别是要初边值以及各个约束项。



###### High-order quasi Xuguo flow

根据相关论文和LSM，可得，需要求解的高阶GPDE为：

![image-20241014130213151](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014130213151.png)

结合LBO公式：（以平均曲率为例）

![image-20241014130256131](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014130256131.png)

可得：

![image-20241014130316462](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014130316462.png)

最终可以得到：

![image-20241014130330326](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014130330326.png)

对于每个$e_i$，第一项为已经预计算出的值，即ground true，第二项为模型预测出的值。

要求解的损失函数为：

![image-20241014130351418](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014130351418.png)

求解过程参考上面的算法1。

对于这个损失函数，我不理解$e_1\sim e_7$这几项的意义，从公式上看，是将复杂的GPDE进行一步一步的分解，分步计算最终的PGDE，$e_1\sim e_7$ 则是每一步产生的误差。

但是前面的一项是怎么来的？代码写的和论文也并不相同，代码中同时对两个要处理的GPDE进行求loss，并且有多项减号前后是相同的。


###### High-order surface diffusion flow of Cahn–Hilliard model

使用LSM和k的高阶LBO来近似表面扩散流，以平滑PDE系统。

引入 Heaviside function：

![image-20241014141347598](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014141347598.png)

new Cahn-Hilliard model reads as:

![image-20241014141437791](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014141437791.png)

论文中还对一些方法进行了整理：

关于流形学习：

| 作者与年份                | 目标                                         | 优点                                                         | 缺点                                                 | 应用                     |
| ------------------------- | -------------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------- | ------------------------ |
| Lustig et al. (2020)      | 使用流形学习识别实验中的拓扑相变             | 一种非局部无监督机器学习方法，用于识别拓扑相变。             | 使用扩散图识别量子多体系统中的相变非常具有挑战性。   | 拓扑相变的识别           |
| Eo et al. (2020)          | 通过域转换流形学习加速笛卡尔MRI相位编码方向  | 提出了基于域转换的框架，用于重建图像。                       | k空间的属性使得该算法在重建结果方面仍然面临挑战。    | 加速重建笛卡尔磁共振成像 |
| Ma et al. (2020)          | 基于直线状测地线和局部坐标的流形学习         | 开发了一种流形学习算法。                                     | 计算成本相对较高。                                   | 数据分类                 |
| Pourmame et al. (2021)    | 用于光谱多模态流形学习和对齐的半监督图谱绘制 | 提出了用于不同模态数据的半监督学习模型。                     | 缺乏多模态图像配准、跨模态图像检索等应用。           | 多模态数据               |
| Mehrdad and Kahaei (2021) | 使用流形学习进行矩阵补全的深度学习方法       | 新算法同时解决了数据矩阵中的线性和非线性关系。               | 超参数需要自动调整，如果模型能自动学习超参数会更好。 | 使用流形学习进行矩阵补全 |
| Rodrigues et al. (2021)   | 流形学习用于理解真实世界的事件               | 提出了一种从数据中学习的动态方法，用于学习不同组件的贡献，以获得更有效的事件表示。 | 缺乏对排名背景的分析。                               | 真实世界事件理解         |
| Chen et al. (2021)        | 用结构化流形学习进行半监督特征选择           | 解决了多模态问题，其性能优于最先进的方法。                   | 该新方法不适用于大规模数据。                         | 特征选择                 |

关于水平集方法：

| 作者与年份                    | 目标                                                    | 优点                                                         | 缺点                                                       | 应用                       |
| ----------------------------- | ------------------------------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | -------------------------- |
| Yan et al. (2020)             | 基于水平集的图像分割方法中的凸性形状先验                | 提出了基于符号距离函数的凸性形状先验，新算法用于解决约束优化问题，改善了分割性能。 | 仍然依赖传统的优化方法，未结合深度学习等新型机器学习方法。 | 图像分割                   |
| Falcone et al. (2020)         | 用于图像分割的高阶方案，修改后的水平集方法              | 提出了新的水平集方法，能够解决带噪声的图像分割问题。         | 仅考虑经典的一阶方程，未解决二阶问题。                     | 图像分割                   |
| Liu et al. (2021)             | 用于电阻抗断层成像中的形状重建的B样条水平集方法         | 形状重建通过水平集函数隐式表示。                             | 重建问题需要迭代求解，控制点的选择并不理想。               | 电阻抗断层成像中的形状重建 |
| Howard and Tartakovsky (2021) | 用于N相流的保守水平集方法，基于自由能的11个表面张力模型 | 提出了一个解决多相流的方法。                                 | 计算成本高且耗时。                                         | 多相流                     |
| Luis and Gibou (2021)         | 基于深度学习的水平集方法计算曲率                        | 深度学习策略和水平集方法估计平均曲率。                       | 鲁棒性较传统方法差。                                       | 曲率计算                   |
| He et al. (2021)              | 用于非均匀SAR图像分割的水平集方法                       | 提出了整合局部强度和全局特征信息的新水平集方法，用于解决图像分割任务。 | 评估指标较少，缺乏测试数据。                               | 图像分割                   |

关于深度偏微分方程（PDE）求解器：

| 作者与年份                 | 目标                                              | 优点                                                         | 缺点                                                   | 应用                                  |
| -------------------------- | ------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------- |
| Huang et al. (2020)        | 通过深度学习初始化的迭代方法解决非线性问题        | 开发了一种通过深度学习初始化的迭代方法，可以处理变分不等式和特征值问题。 | 求解过程的设计相对复杂。                               | 变分不等式和特征值问题                |
| Belbute-Peres (2020)       | 结合可微分PDE求解器和图神经网络进行流体流动预测   | 通过混合网络提供更高保真度的结果，该方法提高了计算性能。     | 缺少一些具体的实际应用。                               | 流体流动预测                          |
| Liang et al. (2020)        | 用于面部内在图像分析的任务感知滤波和传播的PDE学习 | 开发了一个结合滤波和传播操作的PDE学习框架，并推导出了不同滤波和传播模型的数学关系。 | 如果这个模型能完成多任务学习，将更加完美。             | 面部内在图像分析                      |
| So et al. (2021)           | 用于PDE发现的微分光谱归一化（DSN）方法            | 开发了一种新的正则化方法，专为矩约束滤波器设计。             | 缺乏在各学科中的具体应用。                             | PDE发现问题及现有数据驱动方法的局限性 |
| Bar and Sochen (2021)      | 使用无监督学习对基于PDE的断层扫描提供强解         | 提出了一个新的用于正向和反向问题的PDE求解器。                | 对更高维问题和动态非线性方程的两类问题的研究不足。     | 电阻抗断层扫描（EIT）                 |
| Lu et al. (2021)           | 深度算子网络                                      | 提出了深度算子网络，用于从相对较小的数据集中准确高效地学习算子。 | 目前还没有关于算子近似的网络规模的理论结果。           | 鉴别两种类型的算子                    |
| Thanasutives et al. (2021) | 多任务学习                                        | 开发了一种新的方法，用于解决多任务学习。                     | 该方法在处理混沌系统、参数化PDE和PDE系统方面有局限性。 | 多任务学习                            |



##### 论文2：Deep Level Sets for Salient Object Detection

这篇论文用一系列卷积层初始化显著图，然后在超像素级别对其进行细化。 水平集损失函数用于帮助学习二进制分割图。

模型的架构如下：

![image-20241014145556029](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241014145556029.png)

1. 首先将图片输入VGG16，通过BCE loss进行显著性目标检测， 得到检测结果的二值图。
2. 将二值图的值水平移动到[-0.5,0.5]，作为初始的水平集。
3. 然后将BCE loss改为作者定义的lsm loss对VGG16进行二次训练（微调）。
4. 之后通过GSF对结果进行进一步的处理。
5. 最终借助Heaviside function，将水平集图转换为最终的显著性图（二值图）。

**训练过程及输入输出**

首先训练VGG15个epoch，输入为224\*224的图像，通过BCE进行有监督训练，输出为代表显著性的56\*56二值图。
（水平集初始化）然后使用水平集方法，通过水平集loss对VGG进行无监督的训练（微调），水平集的更新借助自动微分，通过作者得出的对水平集的导数进行。输入为初始化后的水平集图，输出为优化后的水平集图。
最后将GSF层加入网络，进行最后的微调。输入为缩放到224\*224的水平集图和通过gslicr生成的有400~500个超像素的图，输出为经过过滤后的水平集图。



###### 水平集方法

水平集方法在这篇文章中的用法为：通过损失函数来帮助VGG学习二进制分割图。

水平集在模型中的表示方式为：VGG输出图像上每个像素的标量值经过水平位移到[-0.5,0.5]得到，并作为初始水平集。

作者通过定义水平集的损失函数，来约束和更新图像上的水平集：

![image-20241015094034617](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015094034617.png)

Length(C):

将对曲线的积分转换为对曲面的积分。

![image-20241015094229615](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015094229615.png)



最后两项强制显著性图在显著区域内部和外部均一致。常数$c_1、c_2$表示内部和外部的显著性均值。

![image-20241015103127712](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015103127712.png)

Heaviside 函数及其导数：

用于将最终的水平集转换为二值图（显著图）。【并不是只进行一次，而是每次训练都会参与】

![image-20241015094255258](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015094255258.png)

H函数会在$z = 0$处发生突变，且其导数只在0处有值，其余位置为0，会在优化过程中陷入局部极小值。作者使用改进的H函数来解决这个问题：

![image-20241015094805744](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015094805744.png)



Loss对水平集$\phi$的导数为：

![image-20241015103318058](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015103318058.png)

可以借助自动微分求梯度，进行计算。

损失可以分为三部分：

- **数据项**: 促使网络输出的显著性图与真实标签尽可能一致。
- **边界长度项**: 鼓励网络学习较长的边界，从而更好地表达显著性区域的形状和细节。
- **紧凑性项**: 使得显著性区域内部和外部的显著性值更加均匀，避免出现孤立点或模糊区域。

问题1：这里是自动微分完成，还是编写代码完成？



##### 论文3：DevelSet: Deep Neural Level Set for Instant Mask Optimization

该文章目的是进行掩码优化

模型由两部分组成，第一部分名为DSN，由ViT作为主干网络，并通过两个分支网络同时优化两个损失函数，第二部分名为DSO，通过CG法对DSN输出的水平集图在GPU上进行优化，得到最终的掩码图。

![image-20241015160037204](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015160037204.png)

训练过程：

1. 作者使用TSDF来作为水平集。
2. 作者没有使用Fast Matching 来生成初始TSDF，而是自己开发了一种基于 CUDA 并行性特性的 TSDF 算法。将输入的掩码图转换为初始水平集图。（当应用于神经网络创建的复杂掩模时，CUDA_TSDF可以减少98%以上的TSDF计算时间）
3. 将通过CUDA TSDF算法生成的初始水平集图作为输入，输入DSN。
4. DSN有两个输出分支，分别用于输出$m_{\theta}$ 矩阵（加权矩阵， 用于DSO优化时曲率使用）和准优化的水平集图 $\phi_{0,\theta}$。
5. DSO以 $\phi_{0,\theta}$ 作为输入，通过CG（共轭梯度法），结合CFL条件在GPU上对水平集图进行优化。其中，DSN输出的$m_{\theta}$用于水平集函数中速度的计算。



###### DSO（DevelSet-Optimizer）

DSO 的损失由两部分组成：

![image-20241015170536387](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015170536387.png)

第一项：

（ILT校正损失旨在最小化输出标称图像和输入目标之间基于像素的差异）

![image-20241015171555029](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015171555029.png)

第二项：

（最小化 PVBand 的面积，我们期望最小/最大工艺条件下最里面/最外面的晶圆尽可能接近目标图像）

![image-20241015171603753](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015171603753.png)

此时速度为：

![image-20241015171644587](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015171644587.png)



水平集的运动方程为：

![image-20241015171851147](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241015171851147.png)





引入曲率项来控制曲线形状，为了增加曲线形状的平滑度，同时保持一些尖锐的边缘，曲率权重在不同位置应该不同，这个问题通过添加加权矩阵 $m_0$ 来解决。

最终水平集的演化方程为：

![image-20241016101953850](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016101953850.png)

通过将对梯度和曲率的求解集成到cuda中，可以极大加快优化的速度。具体算法如下：

由于每个像素的计算都是独立的，因此可以多线程并行执行。

![image-20241016105548024](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016105548024.png)

优化DSO 的算法过程如下：

![image-20241016165937104](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016165937104.png)

因为这是涉及到光刻的优化过程，所以和一般的优化过程有一些不同，关键不同在于：

$Z、M、\phi$ 这三个变量，$Z$ 表示光刻图案，显示当前优化的效果。$M$ 表示光刻图案对应的掩膜，$\phi$ 为水平集函数。

一般的优化过程，直接根据当前水平集来计算速度，然后更新得到新的水平集。

但是这个优化过程在计算速度时需要计算两部分：

- 每个像素上水平集的曲率[直接计算水平集的梯度和曲率即可]【内部项】
- 损失函数的导数。需要先将水平集转换为掩膜（将[-x,y]范围的矩阵转换为二值图），然后根据掩膜来计算梯度。[通过自动微分进行]【外部项】

掩膜会影响光刻图案，水平集又用来更新掩膜，所以在每次迭代中，水平集更新后，需要根据当前的水平集来更新掩膜，进而更新光刻图案。

![image-20241016171719302](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016171719302.png)

###### DSN（DevelSet-Net）

DSO是优化的主体，DSN相当于一个预处理，用于生成准优化水平集和曲率的加权矩阵。

用于加速DSO的优化（减少优化次数）

DSN通过多分支的输出，同时优化两个损失函数：

![image-20241016110845754](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016110845754.png)

- 水平集分支预测

  将预测值和GT值的MSE作为目标函数。LSM的GT值是在DSN训练前，使用DSO迭代生成的。

  （DSO也可以直接使用DSN的输入作为输入，只不过训练时间更久，且可能陷入局部最优，使用DSN的输出作为输出，有更好的初始解，可以避免局部最优，且迭代次数减少90%）

![image-20241016111056018](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016111056018.png)

- 调制分支预测

  调制分支需要学习如何在曲率变化敏感的区域做出最合适的调整，以保持边界形状的平滑性和复杂度控制。

  通过给水平集$\phi$ 的 GT值加上一组小的偏移，来检测曲率敏感区域。

  （对比不同的偏移距离，以及TSDF的变化情况，可以判断曲率的敏感度）

  （对曲率比较敏感的区域一般是比较锐利或是拐角，在优化过程中需要避免这些区域过度光滑）

  ![image-20241016130324317](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016130324317.png)

  $H(\phi)$ 是 Heaviside 函数

  使用AHF 避免在$\phi = 0$ 时的不连续性：

  ![image-20241016131654286](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016131654286.png) 

  偏移量 h是从[−20,20] 的均匀分布中采样的。

  因此，该分支的目标函数为：

  ![image-20241016130603409](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016130603409.png)

  其中：

  ![image-20241016161716753](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016161716753.png)

  分支的输出为：$H_{\varepsilon }(\phi_{m,\theta})$

  

**参数选择器**

因为在优化过程中有很多可变的超参数，因此通过这个事先训练的参数选择器对参数组合进行最优的选择。



问题：

1. DSN两个分支的损失函数中的GT值是通过事先训练DSO得到的，但是训练DSO时$m_{\theta}$ 是怎么来的？事先训练DSO时没有使用曲率项？
2. 调制分支，计算损失函数时的$m_{gt}$ 是什么？





##### 小结

论文3和论文4都是借助水平集损失函数来优化掩码，且都提供真值进行有监督学习，虽然用到了水平集，但是只是对于每个输入预测的LS，只会与真值进行一次mse，用于更新模型的参数。没有水平集演变的过程。

论文2有演变的过程，作者提出一个类似PINN的框架，但是是有监督训练，并且求的是复杂高阶的GPDE，通过将高阶的GPDE拆分为多项，分别设为损失函数，进行有监督的迭代训练。



论文2提出的框架问gpt，训练过程中水平集是如何表示和更新的。



##### 论文4：Deep Level Sets: Implicit Surface Representations for 3D Shape Inference

将水平集函数隐式表示的曲面拟合为目标模型的表面。

这篇论文，作者提出了一个基于水平集函数的损失函数：

![image-20241016215317916](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241016215317916.png)

并设计了一个简单的CNN网络，分别使用水平集loss和以前的基于体素的loss，通过输入的多视角图像重建出3d模型，以证明作者提出的水平集loss的优势。

网络由两部分组成：

第一部分是一个resnet网格，输入为多视角2d图像，输出为64维嵌入向量。

第二部分是一个自编码器，用于3d模型的编解码，编码器将3d模型转换为64维嵌入向量，解码器学习将嵌入向量重建为3d模型。

这两部分在训练时会联合优化，即：

- 模型接收一个3D CAD模型和它的对应2D渲染图像作为输入。

- CNN负责处理2D图像，自编码器负责处理3D CAD模型。

- 两者在训练时通过一个共同的64维嵌入空间连接，并通过反向传播同时更新各自的参数。

- 目标是让整个模型能够更好地从2D图像中预测3D形状。

作者分别使用设计的sdf loss 和 交叉熵损失函数 来训练网络，使用交并集和chamfer来对比两种方式。

在这篇论文中，水平集同样以损失函数的形式通过GT值对生成的内容进行约束。

解码器最终的输出为三维张量，表示的是一个三维立方体网格中的sdf分布。通过差分来计算损失函数及其导数，从而更新网络参数。

因此在模型的前面是没有水平集的，只是将预测的结果用体素的方式表示，从而通过loss进行参数更新。

如何表示3d模型上的水平集



##### 论文5：DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation

[博客](https://zhuanlan.zhihu.com/p/418237767)

论文的目的是训练一个可泛化的模型，将3d模型映射为嵌入向量，并可以根据嵌入向量和给定坐标来查询sdf值。

相比于通过autoEncoder进行重建，这篇论文并没有使用encoder，只使用了decoder ，在训练阶段：

1. 对于每个3d模型，初始化一个对应的嵌入向量$z$.

2. 将$z$作为输入，通过decoder 得到预测，与gt值做loss，**反向传播更新嵌入向量$z$ 和decoder参数**

   <img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20241019133635509.png" alt="image-20241019133635509" style="zoom:67%;" />

3.  训练完成后（此时decoder 参数fix）

   - 对于训练集中的模型$M_i$，输入对应的$z_i$与查询点坐标即可预测对应3d模型该位置的值。
   - 对于不在训练集中的模型$M$，需要将根据在该模型上采样到的一些数据进行嵌入向量的训练，之后再进行预测。（因为模型再训练时已经学习到了大量的先验知识，因此只需要输入模型的少量采样点，即可得到较好的嵌入向量$z$, 从而可以预测模型任意位置的sdf值）

   <img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20241019133717175.png" alt="image-20241019133717175" style="zoom:67%;" />



##### Nerf 了解

传统的3d重建方法：

![image-20241019140348957](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241019140348957.png)



nerf通过体密度来隐式表示3d模型的形状。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20241019193045247.png" alt="image-20241019193045247" style="zoom:33%;" />

一个训练好的模型只能重建一个三维场景。

![image-20241019154703438](eeee'e)

![image-20241019153541269](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241019153541269.png)

![image-20241019153903480](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241019153903480.png)

![image-20241019154439388](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241019154439388.png)

nerf的训练过程如下：

![image-20241019195804681](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241019195804681.png)



