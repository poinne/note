#### 论文1：Current and emerging deep-learning methods for the simulation of fluid dynamics

这篇文章总结了当前利用深度学习（DL）解决流体动力学的问题，主要看了对物理驱动的方法，对于数据驱动的方法，涉及到很多流体动力学的问题，没有看。



文章将DL解PDE分为物理驱动方法和数据驱动方法。

分类的标准是是否学会满足控制方程或使用方程的解的结果。

- 物理驱动的方法通常会调整深度学习模型，通过**最小化控制偏微分方程的残差**，为给定的流体动力学问题提供解析和可微分的解决方案。
  - 以无监督的方式训练，以获得最小化控制方程残差的解决方案。
  - 可以学习流体动力学单个实例的准确解。
- 数据驱动方法为任何流体动力学问题提供了快速、近似的解决方案，这些问题与调整深度学习模型参数时使用的观察结果共享一些物理属性。
  - 数据驱动的方法主要是最小化预测值和实际值之间的差异。
  - 速度更快，泛化性更好，但是准确性不确定。

##### Physics-driven neural flow solvers

`修改的 method of weighted residuals (MWR)`

加权残差法（Method of Weighted Residuals, MWR）是一种用于求解偏微分方程（PDE）的数值方法。该方法的基本思想是将PDE的解表示为某种形式的试探函数，并通过引入加权函数，最小化方程在某种意义上的残差。

具体步骤如下：

1. **选择试探函数**：假设解可以用一组已知函数（试探函数）表示。

2. **定义残差**：计算将试探函数代入PDE后产生的残差，即原方程和试探解之间的差异。

3. **引入加权函数**：选择一组加权函数，这些函数通常与试探函数相关。

4. **形成弱形式**：通过将残差与加权函数相乘并在定义域内积分，得到一个方程组，这样可以“加权”不同区域的误差。

5. **求解方程**：通过调整试探函数的系数，使得加权残差为零，从而得到近似解。

将DL与MWR结合，通过MLP来代替试探函数

这个相关的论文都很老了。



物理驱动网络之间的主要区别体现在以下几个方面：

1. **初始和边界条件的施加方式**：有些网络采用“弱约束”（weak constraints），即通过最小化残差的方式来处理初始和边界条件；而另一些网络则使用“硬约束”（hard constraints），直接将条件强加于网络。
2. **残差的聚合方式**：网络计算偏微分方程（PDE）在每个采样点的残差时，可以选择直接求和（direct sum）或者对空间和/或时间进行积分。这种选择会影响网络的训练方式和最终解的质量。

**physics-informed neural networks (PINNs)**

PINNS可用于解决PDE的正向和逆向问题。

**正向问题**就是求解PDE，将PDE残差和初边值误差作为损失函数。

![image-20241021164104163](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241021164104163.png)

这种方法和早期的物理驱动的神经求解器类似，但是采样点是任意的，而不是通过网格划分算法生成的，属于meshfree的方法。



**逆问题**是指系统的偏微分方程（PDEs）中有一个或多个参数未知，而目标是通过已知的部分数据来推测这些未知参数的近似值。在流体力学中，PINN被用于参数识别（如确定未知的物理参数）和场重建（通过分散的实验测量数据重建流体的物理场）。这让PINN在处理不完全数据或实验数据重建方面非常有用。



当前物理驱动求解器的最大问题是收敛速度通常比数值求解器慢，并且PDE的残差仍然比较高。目前也有一些解决方法。【待看】



##### Data-driven neural solvers

通过数值求解器得到模型训练的数据集，包括变量和对应的观测值，用于模型的监督训练。以最小化损失函数，该损失函数量化网络的预测与这些量的地面实况观察之间的误差。经过训练后，神经网络模型就构成了一个插值器，能够在给定输入变量的情况下逼近解。

这种模型的推导速度很快（相比于数值求解器，快2到4个数量级）

**数值求解器的迭代特性**：隐式数值求解器需要通过反复迭代来解决大规模的方程组，每次迭代都需要耗费大量的计算资源。而神经网络通过训练后，只需要一次前向传播（一次评估）就能返回一个解，避免了迭代的过程。这使得神经网络推理速度显著更快。

**网格与时间步长的灵活性**：显式数值求解器通常需要严格的**稳定性约束**，比如网格尺寸和时间步长必须足够小，才能保证数值解的准确性和稳定性。这种约束导致数值求解器的计算成本很高。而神经网络在求解过程中没有这些严格的约束，允许使用更粗的网格和时间离散化，从而进一步减少计算量。

但是插值性质限制了模型的泛化性能。很多研究目的都是为了减轻这种限制，并将物理知识嵌入到模型中。综述中讨论了四类：

数据驱动部分主要讨论了几种解决流体PDE的方法

###### Data-driven flow inference on structured meshes



##### 总结

通常使用的模型为CNN、U-net、GNN、RNN。

这种方法推理的速度很快，但是准确性不能保证，且泛化能力不强。虽然推理的速度很快，但是模型训练数据的生成成本较高（需要数值求解器生成大量配对数据）

因此这种方法通常是针对特定的目标应用程序进行涉及和训练的。

**CNN架构**

CNN适合与几何与坐标系对其的模拟

**GNN架构**

GNN适合复杂的几何模拟和拉格朗日系统。





#### 论文2：Solving partial differential equations using large‐data models: a literature review

`Artificial Intelligence Review 2024`

##### 不同模型求解PDE

###### CNN

文中并没有说明是怎么做的，只是说结构对问题的适应性较差。

###### PINN



###### DeepONet CNN

DeepONet【待看】

![image-20241021212131377](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241021212131377.png)

###### RNN

PhyCRNet：结合CNN和LSTM求解PDE  【论文3】



###### LSTM

【待看（）】



###### GAN

WGAN solver PDE【待看】【】



###### Transformers

Choose a transformer: Fourier or Galerkin

###### DRLNNs

通过强化学习来求解PDE，看看怎么做的，效果如何，可不可以用【】



##### 当前趋势



#### 论文3：PhyCRNet Physics-informed convolutional-recurrent



看一下这部分在论文中的应用

![image-20241024223219395](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241024223219395.png)

整理看得有关PDE 的论文，结合之前的lsm，整理出几个方法

三种误差



这篇文章将CNN与RNN(LSTM)结合，并结合物理信息，来求解时空PDEs。

它是一种新的物理信息离散学习策略，用于在**没有任何标记数据的情况下求解偏微分方程**。

（感觉训练过程和数值优化的过程很像，通过RNN作为循环，使用有限差分来更新状态。作者也说不想与传统数值计算方法进行比较，而是提供一种新的方式来进行计算，并可以在数据驱动的科学计算中发挥作用。这个模型确实和传统方法类似，不过使用DL来做，（效率应该也没有传统的好，精度也没有传统的高，猜的）

这篇文章和PINN也很像，不同之处：1. 该模型将初值和边界值作为初始值和训练过程中的固定值，而不是固定在loss中，提高了模型的泛化能力。2. 该模型在处理时间维度时，是通过模块的循环来进行的（RNN），而不是单纯将时间作为一个额外的维度。）



模型架构如下：

![image-20241022164355840](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241022164355840.png)

**模型**

核心是ConvLSTM模块，相较于传统的LSTM，将门控部分由全连接网络(FC-NN)修改为CNN，以获得更强的空间表示能力。

PDE的初值条件作为模型的一个输入，用于提高模型的泛化能力。

PDE的边界值条件则是在每次循环的输出后直接对边界进行填充。以确保边界条件强制满足。填充的边界值可以通过有限差分进行计算。不同的边界条件填充的也不同（Dirichlet填充常量、Neumann填充内部计算出的值，每个循环不同）【待看】

**训练**

- 数据集生成

  通过数值方法进行数据集的生成（高阶有限差分法和四阶龙格-库塔时间积分），数据集的格式为：![image-20241022200051279](../../../AppData/Roaming/Typora/typora-user-images/image-20241022200051279.png)

  t：代表数值计算时的时间步

  c：通道，对于2d- burgers来说，是u、v两个通道，代表各自方向的速度。

  h、w：代表mesh的宽高

- 模型训练

  模型架构为CNN+RNN，并结合编码器-解码器结构。事先设置好训练时的时间步长度（和数据集生成时的时间步保持一致），CNN编码器用于将当前的状态编码为低维特征【通过卷积操作进行降维和特征处理（但是不明白这里的作用，以及如何进行训练）】，之后通过ConvLSTM根据状态特征和前一步的隐藏特征生成下一步的隐藏特征，再通过解码器得到下一步的状态。

  数据集中的数据只使用初始状态，即只使用时间步为0时的状态。作为输入。

  模型的loss为所求PDE的MSE。对应偏导通过有限差分计算。

  边界进行硬约束，保持边界条件强制满足。具体地：对于Dirichlet边界条件，边界用常量填充（代码没看懂），对于Neumann边界条件：

- 模型推理

  和训练的过程一样，此时可以用数据集中每个时间步的真实数据与预测数据进行可视化对比。

  

##### 模型细节

使用LSTM的原因：本质上，由于控制门的精细设计，存储单元通过被访问、累积和删除的输入和状态信息来更新。基于这样的设置，普通循环神经网络（RNN）梯度消失问题得到了缓解。

**ConvLSTM**

ConvLSTM的基本原理是继承LSTM控制信息流的基本结构（即单元和门），并考虑到其更好的空间表示能力，将门控操作中的全连接神经网络（FC-NN）修改为CNN。		

【待补充】

**Pixel shuffle**

目的是将低分辨率（LR）特征图升级为高分辨率（HR）输出。

该部分位于ConvLSTM模块后，通过添加该部分，可使用ConvLSTM对低分辨率的特征图进行时间建模，提高计算效率。

**Network architecture**

![image-20241022205237885](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241022205237885.png)

通过有限差分滤波器来求loss的梯度：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20241022205353019.png" alt="image-20241022205353019" style="zoom:67%;" />

论文中说有限差分滤波器无法处理边界的情况，边界处使用填充方式来进行硬约束。（这里如果不使用硬约束的话，应该可以用前向和后向差分来处理）

与传统数值方向相比，可以采样更大的时间步。



###### Efficient network architecture: PhyCRNet-s

为了提高计算效率，修改了网络架构，去除了一部分的编码器：

![image-20241022205907888](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241022205907888.png)

这种跳跃编码器方案可能会导致时间传播中的近似误差，为了效率和准确性的平衡，一般将跳跃间隔设为一个较小的值。



受前向欧拉方案的启发，在输入状态变量$u_i$和输出变量$u_{i+1}$ 之间附加了一个全局残差连接。时刻 $t_i$的学习过程可表述为$u_{i+1} = u_i + \delta t \cdot \mathcal{N}\mathcal{N}[u_i;\theta]$

**loss function**

![image-20241022211032743](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241022211032743.png)

![image-20241022211018376](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241022211018376.png)

###### 结果对比

与PINN方法对比几个方程的结果，在训练的时间步中效果相差不大，但是外推的时间步中，论文提出的模型明显效果好于PINN

![image-20241022211327970](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241022211327970.png)



###### 模型的泛化

该模型的泛化主要通过不同的初始值来进行实验，因为PINN将初始值作为loss的一部分，所以泛化能力很差。

这个模型则是将初始值作为输入，初始值的约束可以通过残差连接施加。从而确保模型的泛化性能。

我的课题所用模型可以参考这个模型，即搭建一个特征提取和循环神经网络，并结合物理信息对水平集函数进行求解。（相当于物理驱动的方法）

但是会有很多问题：

1. 模型使用mesh方式进行求解，水平集是否可用mesh方式？
2. 模型处理的是2D数据，如果涉及到3D的mesh模型，就不能用CNN了。可能需要GNN。这个模型核心的卷积也要大改。
3. 该模型的泛化性只是通过不同的初值体现，不同的mesh输入能否看作不同的初值？



#### 论文4：Physics-Embedded Neural Networks: Graph Neural  PDE Solvers with Mixed Boundary Conditions

将物理信息嵌入GNN，进行PDE 的学习和预测

该模型属于模型求解器，也是通过迭代优化来求解PDE，但是相比于传统的数值优化方法，该方法使用GNN模块来近似微分算子，通过多个GNN模块组合来模拟PDE方程，将PDE和边界约束嵌入到模型中，并借助audo-encoder 在嵌入空间中进行迭代过程。



**训练：**

在得到数据集后，通过encoder-process-decoder 架构进行训练。

对于一个时序PDE，输入为初始状态的信号值，通过encoder（MLP）编码为16维向量$h^{(0)}_u$，输入Neural Nonlinear Solver（类梯度下降算法）进行循环优化，内部信号通过NNS进行优化更新，更新公式中的偏微分算子通过GNN来近似得到。【GNN通过MSE来学习当前PDE 的偏微分算子（？）】 边界信号通过Dirichlet Layer保持不变，优化结束后通过伪逆编码器得到t时刻的信号值。

中间的NNS可以看作RNN，可以自定义循环次数，以获得任意时刻的信号值。GNN包含在NNS结构中。

例如，模拟如下PDE：

![image-20241024220111169](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241024220111169.png)

对应的架构为：

![image-20241024212740488](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241024212740488.png)

红色箭头对应的结构如下：

![image-20241024212634781](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241024212634781.png)

NNS结构如下：

![image-20241024212643903](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241024212643903.png)









特点：

1. 泛化性好，可以处理不同的输入

   因为模型并没有将初值和边界值固定在loss中，而是通过将物理信息和边界值嵌入模型中，这种设计允许模型在训练时自主学习如何适应不同的初始和边界条件。

2. 严格满足边界条件。可以处理混合边界条件（同时考虑多种边界）

   

3. 可以预测很长一段时间后的状态。

   模型学习到了如何预测信号的变化，只需提高GNN的循环次数，即可预测之后的变化。

4. 训练时输入是mesh形状的离散数据。

   从图中可以看出，输入的是t = 0时刻的信号值，编码后进行循环优化，t > 0时刻的信号值，可能也通过编码器进行编码，并作为对应时刻的gt值来训练GNN（$\mathcal{D}_{NIsoGCN}$ 基于另一篇论文提出的模型（简称B）：$\mathcal{D}_{IsoGCN}$，在模型B中添加了有关边界条件的项，模型B则是通过MSE进行训练的 ）

5. 通过神经非线性求解器（neural nonlinear solver）来在GNN中引入全局连接，以提高模型的表现和预测能力。



问题：

1. 对于论文中的NIsoGCN模块，直接看作与引用的论文相同，即通过MSE与真实值进行有监督训练。

2. 如果也是用优化的思想，为什么GNN要比传统方法快？

   训练好的模型，每次循环可以使用比传统方法更大的时间步，循环次数少。且每次循环是对特征图进行卷积和其它操作，不需要计算拉普拉斯、

   雅可比等，速度更快。

3. 训练时的输入输出是什么？

   训练时输入是时空网格以及对应的边界条件，输出是t时刻的网格信息。

   推理时只需要输入初始时刻的信号值。

4. 模型的状态更新公式？

   使用隐式欧拉法，并用类似于梯度下降法来进行迭代。

**表示temporal PDE**

D-dimensional temporal PDE 可以如下表示：

![image-20241023192543163](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023192543163.png)

这里的$\mathcal{D}$ 代表微分算子，这里的微分算子可以是复合的微分算子，表示一系列操作。

**离散PDE**

将连续的temporal PDE离散化，形成mesh，然后视为图，作为GNN输入。

对时间进行离散化，可以用隐式欧拉表示：

![image-20241023195352631](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023195352631.png)

可以看作非线性优化问题：

![image-20241023195542275](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023195542275.png)

$\mathcal{R}(v)$是残差，表示 “当前预测的状态” 和 “前一步的状态 + 一个时间步的变化量”  的差值。

目的是使残差$\mathcal{R}(v) = 0$



**等方差$\mathcal{D}_{NIsoGCN}$**

等方差（Equivariance）是指在某种变换下，系统或模型的输出会以特定的方式变化。具体来说，如果对输入应用某种变换（例如旋转、平移或反射），而模型的输出也随之以同样的方式进行相应的变换，那么我们就说这个模型是等方差的。

论文参考了另一篇论文提出的模型$\mathcal{D}_{IsoGCN}$，使用等方差来提高模型的泛化和预测能力。

模型$\mathcal{D}_{IsoGCN}$ 主要用于将n阶几何张量场转换为m阶几何张量场，并保证等距变换的不变和等变。如$IsoGCN_{0\rightarrow 1}$ 表示将一个标量场转换为1阶几何张量场（如梯度场），因此该模型可以用来近似PDE中的微分算子。相较于传统的计算方法（有限元等），可以进行更快速的预测，并且有较高的精度。

![image-20241023200733627](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023200733627.png)



**编码**

通过编码器将输入的mesh数据编码为特征向量，为了保持边界和内部数据的编码空间相同，对边界数据和内部数据使用相同的编码器。

![image-20241023202827162](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023202827162.png)

**处理**

Barzilai–Borwein 法：类似于梯度下降，区别在于会使用当前步和上一步的梯度来自适应选择步长，而不是固定步长。

使用 Barzilai–Borwein method 来编码空间中求解下面的优化问题：

![image-20241023203824586](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023203824586.png)

自适应的步长为：

![image-20241023204511743](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023204511743.png)

该步长的计算涉及对全局特征向量的运算（点积），所以可以看作是全局池化。

可以考虑到全局的信息。

使用上面的公式，通过 NIsoGCN 近似

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023205409970.png" alt="image-20241023205409970" style="zoom:67%;" />

 中的非线性微分算子 $\mathcal{D}$。

通过Dirichlet层来确保边界条件不变，Dirichlet层如下：

![image-20241023203124497](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023203124497.png)

该层对内部区域没有任意作用，只是强制边界“顶点”固定为$\hat{h_i}$。

最终，模型的状态更新公式为：

![image-20241023205606602](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023205606602.png)

因为$\mathcal{D}_{NIsoGCN}$ 用来近似PDE的微分算子，因此该迭代公式可以保证：

模型在每次迭代更新状态时，强制隐藏特征$h^{(i)}$满足物理偏微分方程（PDE）的要求，包括边界条件。

因此该模型是将物理信息嵌入模型中，而不是loss中。

**解码**

因为在高维空间中的编码，可能是不可逆的，所以使用伪逆解码器，同时，解码器需要确保解码后的边界信息和编码前相同：

![image-20241023203439273](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241023203439273.png)

对于这样的编码器（MLP）：$f(x) = \sigma(W_2\sigma(W_1x+b_1)+b_2$ , 对应的伪逆解码器为：$f^{+}(h) = W^+_1\sigma^{-1}_1(W^+_2\sigma^{-1}_2(h)-b_2)-b_1$

$W^+$ 是 $W$ 的伪逆矩阵，$\sigma$ 是可逆激活函数。在训练时，伪逆解码器与编码器共享权重参数。







##### 总结

模型学习 **mesh模型+约束条件** - -》**curve** 的映射。

输入mesh模型和约束条件，分别通过编码器得到嵌入向量（特征图）用于后续处理，

（或者只对模型进行编码，约束条件后续使用）

通过GNN循环进行水平集函数的演化。

GNN内部，用于近似微分算子的模型对水平集函数的