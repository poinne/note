## 网格压缩

### 几何压缩

#### bpc 、bpv

**Bits per coordinate (bpc)** 和 **bits per vertex (bpv)** 是用于描述3D网格压缩时不同层次的信息量度量指标。

1. **Bits per Coordinate (bpc)**:
   - bpc 表示编码每个坐标所需的比特数。由于每个顶点通常由三个坐标（x, y, z）表示，bpc 是对每个坐标轴单独压缩的度量方式。 
   - 比如，如果一个顶点的每个坐标都用10个比特来表示，那么 bpc 就是 10。

2. **Bits per Vertex (bpv)**:
   - bpv 表示编码每个顶点所需的总比特数。因为一个顶点通常由三个坐标组成，因此 bpv 通常是 bpc 的三倍。
   - 例如，如果每个坐标用10个比特来表示，bpv 就是 30（即 10 bpc × 3 ）。

总之，**bpc** 是对每个坐标的度量，而 **bpv** 则是对每个顶点整体的度量，通常来说 $ bpv = 3 \times bpc $（在三维空间中）。

#### 双平行四边形

1. 思想：预测新顶点时，分别用前向和后向平行四边形预测，取平均值作为预测值。
2. 局限性：当1邻域只有一个未遍历的顶点时，才可以使用，其余情况用平行四边形。根据网格结构的不同，一个网格中可用双平行四边形预测的顶点数也不相同。 
3. 为什么预测可以进行压缩？
因为误差通常比原始值要小得多，量化后可以用更少的比特数进行编码。

**压缩步骤如下：**

![d](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811110515669.png)

压缩方法：

![image-20240811110407418](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811104408133.png)

如果当前顶点的一邻域只有一个顶点未编码时，使用双平行四边形方法：

![image-20240811110515669](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240803095943459.png)

双可预测顶点数与总顶点数的比值取决于输入网格的拓扑特征。该比率被称为对偶比率。

实验中，可用DPP进行压缩的比率为0.75

![image-20240811110700354](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811110407418.png)



**Vertex degree warping**

可以有效地对不规则的网格的拓扑进行压缩：

不规则网格的度的分布范围较大，进行熵编码时用的位数多，通过一个函数对顶点的度进行扭曲，将度数映射到一个新的范围（更小的分布）进行压缩。

![image-20240811104102465](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811102726505.png)

映射过程如下：

![image-20240811104408133](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240802170637759.png)











#### 加权平行四边形

![image-20240803095943459](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240803095949521.png)

![image-20240803095949521](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811110700354.png)

此时，对于新顶点的预测，可以用加权的平行四边形方法来做。

![image-20240802170637759](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240803100303614.png)



$v_L和v_R$的权重可以用这两个顶点周围的四个内角得出：

![image-20240802170621594](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240803101107429.png)

作者在论文中提出了加权平行四边形的几种算法，不同在于计算权重的函数：wfa的参数用不同的方式得到。



**A1算法：**

![image-20240803100303614](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240802170621594.png)

用模板三角形的三个顶点的度来进行各个顶点内角的估计。

**A2' 算法：**

因为在实际计算中，模板三角形的三个顶点已知，所以模板三角形与实际三角形是一致的，因此无需$\alpha 、 \beta$参与对新的顶点的预测，只需要顶点$v_B$和预测的三角形的两个内角即可。但是效果不好。

**A2算法：**

结合A1算法和A2‘ 算法：

![image-20240803100724592](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240803101148256.png)

当$p = 1$时，A2算法转换为A2’ 算法。



**A3算法：**

考虑以$v_L,v_R$为顶点的其它三角形

![image-20240803101107429](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240803100724592.png)

![image-20240803101148256](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240803101353440.png)

其中$a_R$表示与$v_R$相关的解码三角形的数量，$a_R$是这些三角形中与$v_R$相邻的内角之和

![image-20240803101353440](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811104102465.png)



A3算法有两个参数$p,q$，这两个参数需要人为指定，对于不同的模型，或者同一模型的简化版本，相同的参数压缩的效率都不相同。论文中作者是通过设置间隔，用枚举的方式来查找合适的参数值。使用400个定义好的随机模板来获取最优的参数值。





在网格几何压缩中，线性预测和增量预测是两种常用的预测方法，它们用于减少存储顶点位置所需的数据量。

1. **线性预测 (Linear Prediction)**

- **定义**：线性预测是一种基于邻近顶点位置的加权组合来预测目标顶点位置的方法。具体来说，目标顶点的位置是其相邻顶点位置的线性组合，通常基于一定的加权系数。
- **原理**：假设我们要预测顶点 $v$ 的位置，线性预测会使用它周围顶点 $v_1, v_2, \dots, v_n$ 的已知位置，通过一个线性公式（如 $v_{\text{predicted}} = w_1 v_1 + w_2 v_2 + \dots + w_n v_n$）来得到预测值。权重 $w_1, w_2, \dots, w_n$ 可以根据某种规则或预先定义的方式确定。
- **优点**：线性预测方法可以根据局部几何结构灵活调整预测权重，因此在处理具有复杂几何形状的网格时效果较好。
- **缺点**：如果网格的几何形状变化较大或非线性较强，线性预测的效果可能不如其他方法。

2. **增量预测 (Delta/Incremental Prediction)**

- **定义**：增量预测是一种通过记录相邻顶点之间位置差异（增量）来压缩顶点位置的方法。增量预测关注的是顶点位置的变化而不是绝对位置。
- **原理**：在增量预测中，顶点 $v_i$ 的位置通常表示为前一个顶点位置 $v_{i-1}$ 和一个增量 $\Delta v_i$ 的和，即 $v_i = v_{i-1} + \Delta v_i$。压缩过程中，记录下来的不是顶点的绝对位置，而是每个增量 $\Delta v_i$ 的值。
- **优点**：增量预测特别适合处理相邻顶点之间位置变化较小的场景，能够显著减少数据量。如果增量值较小，压缩效率很高。
- **缺点**：对于几何变化较大的网格，增量值可能较大，导致压缩效果不佳。

**区别总结**

- **线性预测**：基于相邻顶点的线性组合来预测目标顶点的位置，适合局部几何结构较规则的网格。
- **增量预测**：基于顶点之间的位置增量来表示顶点位置，适合相邻顶点变化较小的网格。

**应用场景**

- **线性预测**：适用于需要精确捕捉几何形状细节的场景，如高精度的模型压缩。
- **增量预测**：适用于相邻顶点位置变化较小的场景，如平滑曲面或规则网格的压缩。



点云更适合于低带宽应用，而纹理网格提供了最好的可视化效果，更有利于高比特率带宽

![image-20240807195105977](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807192423163.png)



### 图神经网络

#### GCN[[博客](https://blog.csdn.net/fs1341825137/article/details/109783308)]

GCN神奇的地方在于，能够聚合一个node附近的node特征，通过加权聚合学习到node的feature从而去做一系列的预测任务。

![image-20240811140950886](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811140950886.png)

**网络的层数代表着结点特征所能到达的最远距离。比如一层的GCN，每个结点只能得到其一阶邻居身上的信息。对于所有结点来说，信息获取的过程是独立**、**同时开展**的。当我们在一层GCN上再堆一层时，就可以重复收集邻居信息的过程，并且收集到的邻居信息中已经包含了这些邻居结点在上一个阶段所收集的他们的邻居结点的信息。这就使得GCN的网络层数也就是每个结点的信息所能达到的**maximum number of hops**。因此，我们所设定的层的数目取决于我们想要使得结点的信息在网络中传递多远的距离。**需要注意的是，通常我们不会需要结点的信息传播太远。经过6~7个hops，基本上就可以使结点的信息传播到整个网络，这也使得聚合不那么有意义。**



1. **GCN的基本操作**

在GCN中，图卷积操作可以表示为：
$ H^{(l+1)} = \sigma \left( \hat{A} H^{(l)} W^{(l)} \right) $

其中：
- $ H^{(l)} $ 是第$ l $层的节点特征矩阵。
- $ \hat{A} $ 是归一化后的邻接矩阵。
- $ W^{(l)} $ 是第$ l $层的权重矩阵。
- $ \sigma $ 是激活函数（如ReLU）。

2. **归一化邻接矩阵和拉普拉斯矩阵**

在GCN的公式中，$\hat{A}$ 是归一化的邻接矩阵，它可以表示为：
$ \hat{A} = D^{-1/2} (A + I) D^{-1/2} $

其中：
- $A $ 是原始的邻接矩阵。
- $I $ 是单位矩阵（用于处理自连接）。
- $D $ 是度矩阵，对角线元素是邻接矩阵的行和。

归一化的邻接矩阵 $\hat{A}$ 与拉普拉斯矩阵有密切的关系。拉普拉斯矩阵 $ L $ 的定义为：
$ L = D - A $

归一化拉普拉斯矩阵是：
$ \hat{L} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} A D^{-1/2} $

因此，归一化的邻接矩阵 $\hat{A}$ 可以与归一化拉普拉斯矩阵 $\hat{L}$ 的关系写为：
$ \hat{A} = I - \hat{L} $

在GCN中，通过使用归一化的邻接矩阵 $\hat{A}$，实际上间接利用了图的拉普拉斯特征，这有助于平滑节点特征并捕捉局部结构信息。

3. **GCN与拉普拉斯矩阵**

- **拉普拉斯矩阵的作用**：拉普拉斯矩阵捕捉了图的全局结构和局部连通性。它有助于理解图的平滑性和聚合信息的方式。
- **归一化邻接矩阵**：GCN通过使用归一化的邻接矩阵（类似于使用拉普拉斯矩阵）来进行特征聚合，从而可以通过拉普拉斯矩阵的性质对图的结构进行建模。

### 总结

GCN利用归一化邻接矩阵进行图卷积操作，这种操作与拉普拉斯矩阵的归一化形式密切相关，旨在平滑和传播节点特征。尽管在公式中不直接出现拉普拉斯矩阵，但通过归一化邻接矩阵间接利用了其特性。GAT则通过注意力机制动态地计算邻居的重要性，从而在特征聚合时考虑了节点之间的相关性，因此在某些任务上可能表现更好。

**GCN的局限性**

GCN是处理transductive任务的一把利器（transductive任务是指：训练阶段与测试阶段都基于同样的图结构），然而GCN有两大局限性是经常被诟病的：

（a）无法完成inductive任务，即处理动态图问题。inductive任务是指：训练阶段与测试阶段需要处理的graph不同。通常是训练阶段只是在子图（subgraph）上进行，测试阶段需要处理未知的顶点。（unseen node）

（b）处理有向图的瓶颈，不容易实现分配不同的学习权重给不同的neighbor。这一点在前面的文章中已经讲过了，不再赘述，如有需要可以参考下面的链接。



#### GAT

[[博客](https://blog.csdn.net/xiao_muyu/article/details/121762806)1]

[[博客2](https://developer.aliyun.com/article/1144751)]

![image-20240811145239265](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807195105977.png)

![image-20240811145248973](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811145239265.png)

本质上而言：GCN与GAT都是**将邻居顶点的特征聚合到中心顶点上**（一种aggregate运算），利用graph上的local stationary学习新的顶点特征表达。不同的是**GCN利用了拉普拉斯矩阵，GAT利用attention系数**。一定程度上而言，**GAT会更强，因为顶点特征之间的相关性被更好地融入到模型中。**



### 图神经网络用于几何压缩

#### GCN压缩几何 (Graph U-net)

​	![image-20240807192423163](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240811145248973.png)

**GCL**

一般的GCN网络，每经过一次图卷积操作，每个顶点就可以多往周围看一层。并且每次卷积操作收集信息的同时，也会包括之前收集到的信息。

而这篇论文中使用的图卷积操作，每次可以获得K环邻域的信息：

对于每个顶点：

![image-20240807200010056](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807200010056.png)

一次完整的卷积操作：

![image-20240807195925514](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807202209494.png)

使用这个递归的切比雪夫多项式滤波器进行图卷积操作，可以获得每个顶点的 $ k $ 环邻域的信息，而不仅仅是一环邻域的信息

- **标准的一次图卷积** 通常只能获取每个顶点一环邻域（即直接相邻的节点）的信息。这是因为在标准的图卷积中，每个顶点的特征只是通过其直接邻居的特征来更新的。

- **使用递归切比雪夫多项式的图卷积** 通过递归的方式，将信息从更远的邻居传递到中心节点。例如，当 $ k = 2 $ 时，切比雪夫多项式会结合两步传播的特征，也就是将两环邻域的信息整合到节点特征中。当 $ k $ 更大时，则会包含更远的 $ k $ 环邻域的信息。

**Down-sampling layer**

主要通过变换矩阵来进行，下采样的变换矩阵$Q_d\in \{0,1\}^{nxm}$，其中$n$为采样后的顶点个数，$m$为采样前的顶点个数。

采样矩阵通过**曲面简化二次误差度量算法**计算。

例：网格包含5个顶点 $V = \{v_1, v_2, v_3, v_4, v_5\}$ 。这些顶点的坐标分别为：

$
V = \begin{bmatrix}
v_1 \\
v_2 \\
v_3 \\
v_4 \\
v_5 \\
\end{bmatrix}
=
\begin{bmatrix}
(1, 2, 3) \\
(4, 5, 6) \\
(7, 8, 9) \\
(10, 11, 12) \\
(13, 14, 15) \\
\end{bmatrix}
$

现在我们进行下采样，保留顶点 $v_1$ 和 $v_4$，则对应的下采样矩阵 $Q_d$ 为：

$
Q_d = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
$

我们可以用这个矩阵 $Q_d$ 对顶点矩阵 $V$ 进行矩阵乘法，得到下采样后的顶点坐标：

$
V_d = Q_d \cdot V = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
(1, 2, 3) \\
(4, 5, 6) \\
(7, 8, 9) \\
(10, 11, 12) \\
(13, 14, 15) \\
\end{bmatrix}
=
\begin{bmatrix}
(1, 2, 3) \\
(10, 11, 12) \\
\end{bmatrix}
$

因此，下采样后的网格顶点为 $V_d = \{v_1, v_4\}$，它们的坐标分别是 $(1, 2, 3)$ 和 $(10, 11, 12)$。

$V_d$为下采样后的顶点特征矩阵：

![image-20240807202209494](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807202517455.png)

**Up-sampling layer**

也是利用变换矩阵来进行，通过剩余的顶点的特征向量来恢复出原有的顶点。

具体为：通过距离缺失顶点最近的未缺失顶点的特征向量乘以缺失顶点对应的比例因子来获得，每个顶点的比例因子：

 ![image-20240807202517455](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807202635730.png)

对应的上采样矩阵通过以下步骤获得：

![image-20240807202624401](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807195925514.png)

最终：

![image-20240807202635730](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807202624401.png)

采样用的矩阵需要在训练之前生成好。

**所用数据集：**

利用 STAR 数据集中的数据进行3d网格生成，得到20000个有效的网格作为数据集，生成的模型无自交即为有效。

![image-20240807203515560](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809141529272.png)



**不同参数的实验**

![image-20240807203742330](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807203515560.png)

压缩率很高

与Google Draco的比较：压缩和解压缩的时间更短，但是压缩的精度不高（比draco误差更大）

**缺点**

只能处理一种拓扑结构，即只能在与训练样本相同的顶点数量和连通性的网格上工作。如果拓扑结构发生变化，必须重新生成相关的矩阵（如Qd和Qu），并重新配置网络。

如果输入网格的几何不同，则需要重新设计网络。

- 如果输入模型的**顶点个数和拓扑都不变**，即使顶点位置改变，也可以直接使用已经训练好的网络进行压缩。因为网络学习到的是基于固定拓扑结构的模型表示，新的顶点位置只是相同结构下的不同几何形状，网络依然能够处理。

- 如果**顶点数不变但拓扑改变**，则需要重新生成网络所需的拓扑相关矩阵（如Qd和Qu），并重新训练或重新配置网络。这是因为GCAE依赖于特定的拓扑结构来学习模型表示，因此拓扑改变后，原有的网络无法正确理解或处理新的连通性信息。

- 如果网格的**顶点数不同**，就无法直接使用已训练好的网络，因为网络的结构依赖于固定的顶点数。为了解决这个问题，你需要：

1. **重新设计网络**：构建适应新顶点数的网络架构，并使用新数据重新训练。
2. **预处理数据**：你可以对网格进行简化（减少顶点数）或细分（增加顶点数）处理，使得不同的模型在进入网络前达到相同的顶点数，然后使用同一个网络进行压缩。
3. **使用通用化的方法**：开发能够处理不同顶点数的自适应网络或使用不同的预处理技术，使网络可以在不同的顶点数之间灵活转换。



问题：

- 每次EU的矩阵一样吗

#### GAT 顶点预测

![image-20240809135748643](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809141848156.png)

思想：通过GAT网络预测顶点的位置，得到与实际值的差值，然后学习差值的分布，进行熵编码。



**做法：**

1. 输入一个网格，用之前的方法对连接性进行编码（预测网络使用），并将网格的顶点通过bfs遍历，每次输入一个顶点进入预测网络。

2. 顶点预测

   1. 根据已编码的连接性信息和已编码的顶点，构建出当前输入顶点$v_i$的的mask k-hop子图，如下：

      ![image-20240809140957354](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240807203742330.png)

   2. 此时，$v_i$的位置被初始化为mask k-hop子图中所有顶点的位置的平均值：

      ![image-20240809141529272](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809140957354.png)

      子图中每个节点的特征用该节点到$v_i$的向量来作为初始化表示：

      ![image-20240809141848156](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809143357199.png)

      子图中每个节点对$v_i$的注意力分数为：

      ![image-20240809143357199](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809143413178.png)

      归一化：

      ![image-20240809143413178](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809145001314.png)

      

   3. 使用注意力权重进行图卷积操作：

      ![image-20240809145001314](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809151949043.png)

      用两层多头注意力机制的图卷积操作，对相邻节点的特征进行聚合。

      输入的$N\times3$代表当前子图的顶点个数，以及对应的坐标。【不懂】

      ![image-20240809145251380](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809152555135.png)

      最终输出为代表$v_i$特征的$1\times8$的向量。

   4. 之后通过MLP将$v_i$的特征向量转换为$v_i$的预测坐标：

      ![image-20240809151949043](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809135748643.png)

3. 通过预测的顶点和实际顶点，可以得到每个顶点的残差。通过自编码器学习残差的概率分布，然后进行熵编码。【不懂】

4. 损失函数是预测误差和比特率的组合。

   ![image-20240809152555135](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809160053940.png)



**结果对比：**

![image-20240809160038504](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809145251380.png)

消融实验：

表4中n表示同时传入的顶点个数。

![image-20240809160053940](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240809160038504.png)

缺点：解压缩的时间久，（模型迁移到c++，选择多个初始顶点进行并行遍历）

优点：由于使用了算数编码，因此可以实现无损压缩，并且对于不同几何和拓扑的网格，无需重新训练网络。





如果压缩和解压缩不在同一端，则两端都需要有训练好的模型