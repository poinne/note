文本到图像代表性作品：

![image-20240414134427876](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240414134427876.png)

黄色：gan、蓝色：自回归方法、红色：基于扩散模型

在DM普及之前，零拍摄图像编辑一直被GAN反演方法与CLIP相结合所主导。然而，GAN通常具有有限的反演能力，导致图像内容的意外变化。

##### general image editing

扩散模型：

- **DiffusionCLIP** (103)

  DiffusionCLIP 是引入扩散模型（DM）来缓解这一问题的开创性工作。

  它首先采用一个预训练的扩散模型将输入图像转换为潜在空间，然后在反向路径上微调扩散模型，损失函数由两部分组成：局部方向**CLIP损失和身份损失**。前者用于引导目标图像与文本对齐，后者则减轻了不必要的变化。为了实现完全的反演，它采用了确定性的DDIM而不是DDPM的逆过程。由于DM具有出色的反演性能，DiffusionCLIP 在领域内和领域外的操作上均表现出了优越的性能。

  DiffusionClip的一个缺点是，它需要对模型进行微调以转移到新的领域。

- **LDEdit** (104)

  LDEdit提出了将DDIM和LDM结合的方法。具体而言，LDEdit 在潜在空间中采用确定性的正向扩散，然后根据目标文本进行反向过程。尽管它简单，但在广泛的图像编辑任务中表现良好，构成了一个通用的框架。

- **Prompt-to-Prompt** (126)

  Prompt-to-Prompt 在扩散过程中提出使用交叉注意力图，代表每个图像像素与文本提示中的单词之间的关系。在图像到图像的转换任务中，还在扩散潜在空间中的语义特征上进行了研究，并发现在模型内部操作空间特征和自注意力可以控制图像转换过程。

- **DiffusionIT**(105)

  DiffusionIT中还提出了一种无监督图像转换方法，可以分离样式和内容表示。

- **CycleDiffusion**（128）

  CycleDiffusion 通过重新制定扩散模型的潜在空间来统一生成模型，并表明扩散模型可以像GANs一样进行引导。

- **Direct Inversion**(129)

  Direct Inversion 应用了类似的两步过程，即将图像编码为其对应的噪声，然后使用反转的噪声生成编辑后的图像。然而，Direct Inversion 不需要优化或模型微调。在生成过程中，扩散模型从一个噪声向量开始，并通过迭代去噪生成图像。

- **other**

  对于图像编辑任务，从图像到噪声的精确映射过程以及反向过程是必要的。与DDPM [30] 不同，DDIM [125] 因其几乎完美的反演而被广泛应用 [103]。然而，由于局部线性化假设，DDIM [125] 可能会导致错误的图像重构和误差传播 [130]。为了缓解这个问题，Exact Diffusion Inversion via Coupled Transformations (EDICT) [130] 提出在扩散过程中维护两个耦合的噪声向量，并且实现了比DDIM [125] 更高的重构质量。然而，EDICT [130] 的计算时间几乎是DDIM [125] 的两倍。另一项工作 Null-text Inversion [131] 借鉴了在无条件扩散模型中，累积误差可以忽略不计的发现，但在应用具有较大引导尺度w的分类器引导的图像编辑时会被放大的发现，[131] 提出以w = 1的引导尺度初始DDIM反演作为关键轨迹，然后用标准引导w > 1 进行优化。[131] 还提出用优化的嵌入（Null-text optimization）替换空文本的嵌入，实现了真实图像的高保真编辑结果。

#####  Image editing with masks

- **Blended diffusion**（132）

  使用掩码对图像进行主要局部（掩码）区域的操作[132]构成了图像编辑任务的一个重要挑战。困难在于保证掩码区域与背景之间的无缝一致性。类似于[103]，混合扩散[132]基于预训练的CLIP，并采用两个损失项：一个用于促进掩码图像与文本标题之间的对齐，另一个用于保持未遮挡区域不偏离其原始内容。值得注意的是，为了保证编辑区域与其余部分之间的无缝一致性，它通过逐步地将噪声图像与本地文本引导的扩散潜在空间进行空间混合。

- 混合潜在扩散

  为了保证编辑区域与其余部分之间的无缝一致性，它通过逐步地将噪声图像与本地文本引导的扩散潜在空间进行空间混合。这种方法进一步与LDM [17]结合，形成混合潜在扩散，以加速局部文本驱动的图像编辑[133]。混合扩散的多阶段变体也被用于超高分辨率的设置[134]。上述工作[132]，[133]，[134]需要手动设计掩码，以便模型可以确定要编辑的部分。

- **DiffEdit**

  DiffEdit [135] 提出自动生成掩码来指示要编辑的部分。具体而言，掩码是通过查询文本和参考文本条件之间的噪声估计差异推断出来的。有了推断的掩码，DiffEdit [135] 将感兴趣的区域替换为与查询文本相对应的像素。



##### 3D object editing



##### text-based semantic edits to a single image

