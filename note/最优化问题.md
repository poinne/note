最优化问题三要素：决策变量，目标函数，约束条件



最优化问题四个方面：

- 凸 or 非凸
- 连续 or 离散
- 有限制条件 or 无限制条件
- 光滑 or 不光滑



**对于一个非凸的问题，有三种方法：**

1. 找尽可能好的局部最优
2. 松弛化这个问题，使其变为一个凸优化问题
3. （假设问题比较小）逐个遍历，得到最优解

**线性与非线性：**

非线性函数：不满足线性定义的函数即为非线性函数。
线性可以认为是1次曲线，比如比如y=ax+b ，即成一条直线；
非线性可认为是2次以上的曲线，比如y=ax2+bx+c，（x2是x的2次方），即不为直 线的即可。
两个变量之间的关系是一次函数的关系图像是直线，这样的两个变量之间就是“线性关系”如果不是一次函数关系.

在数学上，线性关系是指自变量x与因变量yo之间可以表示成y=ax+b ，（a，b为常数），即说x与y之间成线性关系。



### 凸集

##### 仿射集

定义：

![image-20240703093004314](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703093004314.png)

性质：





任意线性方程组的解集是仿射集。

**仿射包**

![image-20240613161442328](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240613164112058.png)

是从非仿射集合中构造出来包含该集合的最小的仿射集



**凸集：**

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240613161442328.png" alt="image-20240613162649805" style="zoom:67%;" />

仿射集属于凸集的特例。凸集比仿射集多一个条件，即每个$\theta \in [0,1]$，而仿射集没有这个要求。

**凸包**

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240613164034564.png" alt="image-20240613163217131" style="zoom:67%;" />

![image-20240613164034564](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240613165524130.png)

![image-20240613164112058](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240613162649805.png)

**锥**

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240606164117567.png" alt="image-20240613165100482" style="zoom:67%;" />

如果有多条射线的话， 射线的交点必须在原点，不然不满足条件，因为$\theta$可以取0

**凸锥**

![image-20240613165524130](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240613165100482.png)

**凸锥包**

凸锥包（Convex Cone Hull，也称为凸锥包络）是给定点集的最小凸锥。就像凸包是包含一个点集的最小凸集一样，凸锥包是包含一个点集的最小凸锥。它是将点集通过非负缩放和线性组合生成的所有点的集合。

**分类**

- 按约束分类

  - 约束优化问题 

    有约束

  - 箱子集约束优化问题

    L <= x <= U

  - 无约束优化问题

    $x \in R^n$

- 按线性性质分类

  **线性规划问题：**目标函数和约束条件都是线性函数的问题。

  **非线性规划问题：**目标函数和约束条件至少有一个是非线性的的问题。

- 按决策变量的连续性分类

  - 连续优化问题

    特点：目标函数一般可求导，相对比较简单

  - 离散优化问题
    - 整数规划问题
    - 0-1规划问题
  - 混合整数规划

- 按目标函数个数分类

  - 单目标规划问题
  - 多目标规划问题

- 按决策变量的随机性分类

  - 确定性优化问题

    变量x属于n维空间

  - 随机优化问题

    变量x为随机数，属于某个分布

  - 鲁棒优化问题

![image-20240606163944908](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240606163944908.png)

![image-20240606164117567](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703093821875.png)

![image-20240606164404782](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240613163217131.png)

##### 基本数学概念

- Hessian矩阵

  一个函数对各个自变量求二阶偏导所组成的矩阵：

  ![image-20240703093821875](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240606164404782.png)

  ![image-20240703093959599](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703093959599.png)

- 超平面

  超平面是n维欧氏空间中余维度等于一的线性子空间，也就是必须是(n-1)维度。
  这是平面中的直线、空间中的平面之推广（n大于3才被称为“超”平面），是纯粹的数学概念，不是现实的物理概念。因为是子空间，所以超平面一定经过原点。

- 半空间

  一个$R^n$空间，被一个超平面分为两半，每一半都是一个半空间。

- 凸函数

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240606171152745.png" alt="image-20240606165415402" style="zoom:50%;" />

所以，凸函数的图像是看起来凹的

- 凸组合

  凸组合 （convex combination ）指**点的 线性组合** ，要求所有系数都非负且和为 1。 此处的「点」可以是 仿射空间 中的任何点，包括 向量 和 标量 。

  $x = \lambda x_1 + (1- \lambda x_2)$

- 凸集中的极点

  若凸集S中存在某一点x，不能由S中两个点的凸组合表示，则该点x为凸集S的极点。一个凸集可能存在多个极点。（下图，$x_1 -- x_5$都是极点。

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240606165415402.png" alt="image-20240606170842689" style="zoom:50%;" />

- 紧凸集

  可以理解为有边界的凸集，边界的各个顶点就是极点。

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240606170842689.png" alt="image-20240606171152745" style="zoom:50%;" />

  因此一个紧凸集可以通过它的极点表示出来：即该紧凸集的各个极点的凸组合的形式。

- 极方向

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614110650113.png" alt="image-20240606171945881" style="zoom:67%;" />

  任何一个方向都可以由极方向的线性组合表示。

**球和椭球**

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614111056648.png" alt="image-20240614110427990" style="zoom:67%;" />

球和椭球都是凸集。

![image-20240614110650113](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614133101322.png)

椭球定义中的A是一个正定矩阵，它的奇异值是椭球的半轴长。

 ![image-20240614111056648](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614133112807.png)

##### 多面体

![image-20240614133101322](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240606171945881.png)

![image-20240614133112807](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614110427990.png)

##### 单纯形

维基百科： 

![image-20240614144910209](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614133927961.png)

![image-20240614133927961](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614133951990.png)

![image-20240614133951990](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614144910209.png)

##### 对称矩阵集合

- 对称矩阵集合$S^n = \{X \in R^{n \times n}|X = X^T\}$
- 对称半正定矩阵集合$S^n_+ = \{X \in R^{n \times n}|X = X^T, X \succeq 0\}$
- 对称正定矩阵集合$S^n_{++} = \{X \in R^{n \times n}|X = X^T, X \succ 0\}$

其中 $X\succ 0$表示矩阵X的奇异值大于0，而不是矩阵X的值大于0。

![image-20240614160829234](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240617104545480.png)

![image-20240614160840392](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614160829234.png)

##### 凸集的操作和保凸性

- 交集：若$S_1,S_2$为凸，则$S_1\bigcap S_2$为凸。

- 若$S \in R^n$为凸，$f:R^n ->R^m$为仿射函数，则$f(S) = \{f(x)|x\in S\}$为凸。

  ![image-20240614154121506](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614160840392.png)

  仿射的逆映射，将$f(S) -> S$也可以保持集合的凸性。

  缩放和移位是保持凸性的。	

- 两个凸集的和是凸的。
- 两个凸集拼接成的集合也是凸的。（新的集合的每个元素的前后两部分分别属于两个集合）

- 任意一个凸集经过透视函数后，仍然是个凸集。

##### 透视函数

定义是这样的：假设有一个n+1维向量，将它的前面n个维度除以最后一个维度，则得到了一个n维向量。得到的这个n维向量就是原来的n+1维向量的透视后的结果。

![image-20240617104545480](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614154121506.png)

- 一个线段经过透视函数后仍然会得到一个线段。

- 任意凸集的反透视映射后得到的仍然是一个凸集。

![w](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614163859522.png)

![image-20240614163859522](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614163919035.png)

![image-20240614163919035](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240617105808270.png)

##### 线性分数函数

线性分数函数是仿射映射函数和透视变换的复合函数，依然还是保凸运算，凸集在线性分数函数下的像和逆像都是凸的。从联合概率到条件概率的变换是一个线性分数函数。

![image-20240617105808270](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240614163843066.png)

![image-20240617110516185](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240617110540509.png)

![image-20240617110540509](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240617110516185.png)

### 凸函数

![image-20240617112824027](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240617112912473.png)

上面的公式，完全可以推广到多元函数。在数据科学的模型求解中，如果优化的目标函数是凸函数，则局部极小值就是全局最小值。这也意味着我们求得的模型是全局最优的，不会陷入到局部最优值。

![image-20240617112912473](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240618154810356.png)



![image-20240618154810356](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240618160445990.png)

![image-20240618155343868](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240617112824027.png)

![image-20240618160445990](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703095351252.png)

判断是否为凸函数：

![image-20240703095351252](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240731134226997.png)

![image-20240731145815603](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240731135559302.png)

##### 序列的极限

如果一个序列的子序列极限为a，则a为原序列的一个聚点。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240731145815603.png" alt="image-20240731134226997" style="zoom:67%;" />

**$R^n->R$的Taylor展开式**

**![image-20240731135559302](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703114925007.png)**



##### Jacobi矩阵

Jacobi矩阵是向量值函数的导数。

##### Hessian矩阵



正定与半正定的区别：

在求解优化问题时，Hessian矩阵的正定性和半正定性有不同的含义和影响。这些特性主要影响到目标函数的极值性质以及优化算法的收敛性。以下是两者的区别：

**正定 (Positive Definite)**

Hessian矩阵 \( H \) 是正定的，当且仅当对任意非零向量 \( x \)，都有 \( x^T H x > 0 \)。

**含义**

- 如果 Hessian 矩阵在某点 \( x^* \) 处是正定的，则目标函数在 \( x^* \) 处有一个**严格的局部极小值**。
- 这个性质确保了目标函数在 \( x^* \) 附近是“碗”形的，即函数在该点向各个方向的曲率都是向上的。

**优化问题中的影响**

- **局部极小值**：正定 Hessian 矩阵意味着该点是局部极小值。对于凸优化问题，全局最优点也是局部最优点，因此正定性可以用来验证全局最优点。
- **算法的收敛性**：基于二阶导数的优化算法（如牛顿法）在 Hessian 正定时通常能够很好地收敛到局部极小值。

 **半正定 (Positive Semidefinite)**

Hessian矩阵 \( H \) 是半正定的，当且仅当对任意向量 \( x \)，都有 \( x^T H x \geq 0 \)，但允许存在非零向量 \( x \) 使得 \( x^T H x = 0 \)。

**含义**

- 如果 Hessian 矩阵在某点 \( x^* \) 处是半正定的，则该点可能是**局部极小值**，但不一定是严格的。可能存在“平坦”区域，即函数在某些方向上是平的（零曲率）。
- 半正定 Hessian 矩阵意味着目标函数在该点附近不会“向下凹”，但可能会“水平”延伸。

**优化问题中的影响**

- **局部极小值或鞍点**：半正定 Hessian 矩阵可能对应局部极小值，但也可能对应鞍点。因此，仅凭 Hessian 半正定无法确定是否为局部极小值。
- **平坦区域**：在平坦区域，目标函数在某些方向上的导数可能为零，这会使得基于梯度的优化算法（如梯度下降法）在这些方向上表现出较慢的收敛。

**总结**

- **正定** Hessian 矩阵在优化问题中意味着我们找到了一个严格的局部极小值，这个点的极小性质在各个方向上都是确定的。基于二阶信息的优化算法通常在这样的条件下表现良好，能够快速收敛到最优解。

- **半正定** Hessian 矩阵表示目标函数在某个方向上可能“平坦”，这使得判断是否为极小值变得复杂，同时可能导致算法在这些方向上收敛速度较慢。在凸优化问题中，半正定性也可能对应全局最优解，但具体要看问题的结构。

因此，在处理优化问题时，Hessian 矩阵的正定性和半正定性给出了关于目标函数形状的重要信息，影响着求解过程和解的性质。



##### 单纯形法

![image-20240731154526501](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240618155343868.png)

单纯形法的核心思想是通过换入和换出基变量，使目标函数值在可行域的顶点之间不断提升，最终到达最优点。

##### 非线性规划的KKT条件



### 对偶





##### 弱对偶

弱对偶是优化理论中对偶性质的一种情况，指的是原始问题和对偶问题之间存在的一种基本关系。具体来说，对于一个优化问题，如果其对偶问题的最优解提供了原问题最优解的一个下界，但不一定相等，那么这种情况称为弱对偶。

![image-20240703114925007](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703211935277.png)

- 对偶间隙（Duality Gap）

对偶间隙是原问题最优解与对偶问题最优解之间的差距。如果对偶间隙为零，则原问题和对偶问题的最优解是相同的。这种情况称为强对偶性（strong duality）。如果对偶间隙不为零，则称为弱对偶性（weak duality）。

- 对偶问题的意义

1. **提供下界**：对偶问题的解通常提供原问题的一个下界（对于最小化问题）。
2. **简化问题**：对偶问题有时比原问题更容易求解。
3. **优化条件**：对偶问题提供了一种验证原问题最优性的工具（KKT条件）。



无约束优化，搜索条件



### 非线性最小二乘问题的方法

**收敛速度**

![image-20240704152945536](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704152945536.png)

不同的优化算法具有不同的收敛性质。例如，简单的梯度下降法通常具有线性收敛速度，而牛顿法在接近最优解时则具有二次收敛速度。高斯-牛顿法在某些特殊情况下（如目标函数在最优解处的残差很小）也可以达到超线性或二次收敛速度。

#### 常用方法

##### 最速下降法

每次选择梯度的反方向作为移动的方向

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703212017392.png" alt="image-20240703211935277"/>

![image-20240703212017392](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703212041805.png)

##### 牛顿法

牛顿法每次迭代都是找逼近当前点的二次函数的极小值点作为下一次的迭代点。

![image-20240703212041805](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703212056713.png)

![image-20240703212056713](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703212105797.png)

![image-20240703212105797](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703220553989.png)

**总结**

- **最速下降法和牛顿法就是泰勒的一阶和二阶展开式。**

- **两个方法的下降方向都可以写为同一个式子：**

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703215559606.png" alt="image-20240703220553989" style="zoom:33%;" />

  在最深下降(steepest descent)方法中，$B_k$是单位矩阵I，在牛顿方法(Newton)中$B_k$则是海森(Hessian)矩阵。

**通过上面的方法可以找到每次最优下降的方向，接下来是每次迭代的步长。**

#### line search

![image-20240703214652474](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703214652474.png)

这个软线搜索还是不太理解。

![image-20240703214708328](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240731154526501.png)



​		![image-20240703215559606](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703214708328.png)

算法如下：

![image-20240703215618997](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703223500046.png)

#### 信赖域方法（Trust Region Method）和阻尼方法（Damped Method）

![image-20240703223442235](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703223442235.png)

![image-20240703223500046](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703223621640.png)

![image-20240703223518326](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703215618997.png)

![image-20240703223614016](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703223518326.png)

![image-20240703223621640](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704094601923.png)

![image-20240703223632748](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703223614016.png)

#### 非线性最小二乘问题

![image-20240704094509109](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240703223632748.png)

##### 非线性最小二乘问题-数据拟合

![image-20240704094533470](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704094533470.png)

![image-20240704094601923](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704142304722.png)

![image-20240704094617707](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704094617707.png)

![image-20240704094628622](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704142149895.png)

##### 非线性最小二乘问题-非线性方程组

(牛顿法）Newton-Raphson’s method:

![image-20240704142149895](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704094509109.png)

其中，$f'(x)$就是函数f对应的雅可比矩阵，因此，上述公式还可以写为：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704094628622.png" alt="image-20240704142304722" style="zoom:66%;" />

但是，牛顿-拉夫逊并不总是正确的，对于特定例子，直接使用牛顿-拉夫逊方法可能会陷入非解的局部极小值，而线搜索法可以帮助改进收敛性能。

**高斯牛顿法(Gauss-Newton Method)**

最小二乘问题才能使用高斯牛顿法

通过求解一下公式，得到每次下降的方向：

![image-20240704151545305](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704151545305.png)

下降的距离可以通过line search找到，在高斯牛顿法中所有步都是用$\alpha = 1$，只要满足以下两个条件，就可以保证line search是收敛的：

1. $x|F(x) <= F(x_0)$ is bounded, and
2. $J(x)$ has full rank in all steps.

##### 牛顿法和高斯-牛顿法的区别

![image-20240704152516666](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704152516666.png)

![image-20240704152556581](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704152609771.png)

![image-20240704152609771](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704152556581.png)

##### Levenberg-Marquardt法（L-M法）

它结合了高斯-牛顿法和梯度下降法的优点，通过引入阻尼参数 $ \lambda $ 调整步长，解决了高斯-牛顿法可能出现的不收敛和牛顿法可能出现的震荡问题。

**方法概述**

给定向量值函数 $f(x)$ 和雅可比矩阵 $ J(x) $，Levenberg-Marquardt 方法通过求解以下线性系统来计算步长 $ h_{lm} $：

$$ (J^T J + \lambda I) h_{lm} = -J^T f $$

其中 $\lambda$ 是阻尼参数，$ I $ 是单位矩阵。

**关键步骤和效果**

1. **步长方向：**
   - 当 $\lambda$ 很大时，方法接近梯度下降法。步长 $ h_{lm} \approx -\frac{1}{\lambda} g $，其中 $ g = J^T f $ 是梯度。
   - 当 $ \lambda $ 很小时，方法接近高斯-牛顿法。步长 $h_{lm} \approx h_{gn}$。

2. **更新阻尼参数 $\lambda$：**
   
   - 使用增益比率 $ \rho $ 来更新 $ \lambda $。增益比率定义为：
   
     $$
     \rho = \frac{F(x) - F(x + h_{lm})}{L(0) - L(h_{lm})}
     $$
     

  其中 $L(h)$ 是线性模型的估计。

   - 如果  $\rho$ 很大，说明步长 $h_{lm}$的效果很好，可以减小 $\lambda $，使得方法更接近高斯-牛顿法。
   - 如果 $\rho$ 很小，说明步长 $ h_{lm} $ 的效果不好，应增加 $ \lambda $，使得方法更接近梯度下降法，减小步长。

**算法步骤**

1. 初始化 $x_0 $ 和 $ \lambda $。
2. 计算当前的雅可比矩阵 $J $ 和梯度 $ g = J^T f $。
3. 解线性系统 $(J^T J + \lambda I) h_{lm} = -g $ 得到步长 $ h_{lm} $。
4. 更新 $x $：
   
   $$
   x := x + \alpha h_{lm}
   $$
   
   其中 $\alpha $ 是通过线搜索确定的步长因子。
5. 计算增益比率 $\rho $：
   
   $$
   \rho = \frac{F(x) - F(x + h_{lm})}{L(0) - L(h_{lm})}
   $$
   
6. 根据 $ \rho $ 更新阻尼参数 $ \lambda $：
   - 如果 $\rho > 0.75 $，减小 $ \lambda $。 （一般令$\lambda_{i+1} = 0.5\lambda_{i}$)
   - 如果 $ \rho < 0.25 $，增加 $ \lambda $。（一般令$\lambda_{i+1} = 2\lambda_{i}$ or $\lambda_{i+1} = 2\lambda_{i}$)
   
7. 检查停止条件。如果满足停止条件（如梯度的范数小于某个阈值，步长变化很小或达到最大迭代次数），则终止；否则返回第2步。

**停止准则**

1. 梯度 $ g $ 的范数很小：

   $$
   \|g\|_1 \leq \epsilon_1
   $$

2. $ x $ 的变化很小：

   $$
   \|x_{\text{new}} - x\| \leq \epsilon_2 (\|x\| + \epsilon_2)
   $$

3. 迭代次数达到最大值：

   $$
   k \geq k_{\text{max}}
   $$

L-M法过程如下：

![image-20240704165750710](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704193516729.png)

通过正交变换相比迭代法求得的解更加精确，但是计算量更大，代价更高，所以一般还是使用迭代法求解。

**例子**

假设我们有以下非线性最小二乘问题：

$$ f(x) = \begin{bmatrix} x_1 + 10x_1 / (x_1 + 0.1) \\ x_2^2 \end{bmatrix} $$

用 Levenberg-Marquardt 方法解决时，我们通过计算雅可比矩阵 $ J(x) $：

$$ J(x) = \begin{bmatrix} 1 - 10 / (x_1 + 0.1)^2 & 0 \\ 0 & 2x_2 \end{bmatrix} $$

然后依次进行上述步骤，直到满足停止条件。

总结来说，Levenberg-Marquardt 方法通过平衡高斯-牛顿法的快速收敛性和梯度下降法的稳定性，提供了一种有效解决非线性最小二乘问题的方法。

##### 几种方法的对比

![image-20240704193516729](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704200342900.png)

###### $f(x)与F(x)的Jacobi矩阵关系$

![image-20240704200342900](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704203835370.png)

##### Powell’s Dog Leg Method

Powell’s Dog Leg算法是一种结合高斯-牛顿法和最速下降法的非线性最小二乘优化方法，通过信赖域（trust region）方法来控制步长。这个算法的名称来源于Powell提出的用来逼近信赖域步长的方法。

**1.高斯-牛顿步长和最速下降步长**

**高斯-牛顿步长**

高斯-牛顿法是基于最小化残差平方和的问题，公式如下：
$$
(\mathbf{J(x)}^T \mathbf{J(x)}) \mathbf{h}_{gn} = -\mathbf{J(x)}^T \mathbf{f(x)} 
$$
**最速下降步长**

最速下降法是基于梯度的方向进行迭代，公式如下：
$$
\mathbf{h}_{sd} = -\mathbf{J(x)}^T \mathbf{f(x)}
$$
**2. 线性模型与步长的选择**

线性模型用于估计沿最速下降方向的目标函数值：
$$
\mathbf{f(x + \alpha h_{sd})} \approx \mathbf{f(x)} + \alpha \mathbf{J(x)} \mathbf{h}_{sd} 
$$
$$
F(\mathbf{x + \alpha h_{sd}}) \approx \frac{1}{2} \|\mathbf{f(x)} + \alpha \mathbf{J(x)} \mathbf{h}_{sd}\|^2 
$$

该函数关于 $\alpha$ 的最小值出现在：
$$
\alpha = -\frac{\mathbf{h}_{sd}^T \mathbf{J(x)}^T \mathbf{f(x)}}{\|\mathbf{J(x)} \mathbf{h}_{sd}\|^2} = \frac{\|\mathbf{g}\|^2}{\|\mathbf{J(x)} \mathbf{g}\|^2} 
$$
**3. 步长选择策略**

根据信赖域半径 $\Delta$，选择步长：
- 如果 $\|\mathbf{h}_{gn}\| \leq \Delta$，选择高斯-牛顿步长：
  $$
  \mathbf{h}_{dl} := \mathbf{h}_{gn} 
  $$
- 如果 $\|\alpha \mathbf{h}_{sd}\| \geq \Delta$，选择缩放的最速下降步长：
  $$
  \mathbf{h}_{dl} := \left(\frac{\Delta}{\|\mathbf{h}_{sd}\|}\right) \mathbf{h}_{sd} 
  $$
- 否则，选择Dog Leg步长：
  $$
  \mathbf{h}_{dl} := \alpha \mathbf{h}_{sd} + \beta (\mathbf{h}_{gn} - \alpha \mathbf{h}_{sd}) 
  $$
  其中 $\beta$ 是使得 $\|\mathbf{h}_{dl}\| = \Delta$ 的系数，可以通过求解以下二次方程来确定：
  $$
  \hat{\psi}(\beta) = \|\mathbf{a} + \beta (\mathbf{b} - \mathbf{a})\|^2 - \Delta^2 = \|\mathbf{b} - \mathbf{a}\|^2 \beta^2 + 2c\beta + \|\mathbf{a}\|^2 - \Delta^2
  $$
  其中 $c = \mathbf{a}^T (\mathbf{b} - \mathbf{a})$。

**4. 算法步骤**

1. 初始化：给定初始点 $\mathbf{x_0}$ 和信赖域半径 $\Delta_0$。
2. 计算梯度：
   $$
   \mathbf{g} := \mathbf{J(x)}^T \mathbf{f(x)}
   $$
3. 检查停止条件：如果 $\|\mathbf{f(x)}\|_1 \leq \epsilon_3$ 或 $\|\mathbf{g}\|_1 \leq \epsilon_1$，则停止迭代。
4. 计算 $\alpha$ 和 $\mathbf{h}_{sd}$。
5. 求解高斯-牛顿步长 $\mathbf{h}_{gn}$。
6. 计算 Dog Leg 步长 $\mathbf{h}_{dl}$。
7. 更新当前点：
   $$
   \mathbf{x}_{new} := \mathbf{x} + \mathbf{h}_{dl}
   $$
8. 计算增益比：
   $$
   \rho := \frac{F(\mathbf{x}) - F(\mathbf{x}_{new})}{L(0) - L(\mathbf{h}_{dl})}
   $$
9. 更新信赖域半径 $\Delta$：
   - 如果 $\rho > 0.75$，则 $\Delta := \max(\Delta, 3 \|\mathbf{h}_{dl}\|)$。
   - 如果 $\rho < 0.25$，则 $\Delta := \Delta / 2$。
10. 检查步长 $\|\mathbf{h}_{dl}\|$ 是否足够小：如果 $\|\mathbf{h}_{dl}\| \leq \epsilon_2 (\|\mathbf{x}\| + \epsilon_2)$，则停止迭代。

**5. 算法总结**

Powell’s Dog Leg算法通过结合高斯-牛顿法和最速下降法，并利用信赖域来控制步长，来实现对非线性最小二乘问题的优化。该方法在高斯-牛顿法可能失败时，通过引入最速下降方向来增加鲁棒性，并通过调整信赖域半径来提高收敛效率。

##### 拟牛顿法



##### L-M的割线版（适用于雅可比未知）









![image-20240704203835370](https://raw.githubusercontent.com/poinne/md-pic/main/image-20240704165750710.png)