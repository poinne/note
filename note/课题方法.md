**数据集构造**



**输入数据格式：**

mesh数据：$(V,F)$

控制点：$V_{interp}、V_{obs}$

切法向：$list:(a_1,a_2...), a_i = \{v_i,n_i\}$(这里的顶点是插值点还是任意顶点？)

**根据控制点坐标初始化模型上的水平集值：**

在输入模型前，先对模型进行预处理，根据插值点生成初始水平集。

根据插值点，沿模型的edge生成一个圆环，然后使用fast marching 在顶点上构造初始水平集。

**mesh模型在网络中的表示形式：**

网格由$(V,F)$ 对定义，其中$V = \{v_1,v_2,...,\},v_n$是$R^3$中的顶点坐标集合。$F$定义连通性，是三角形网格的顶点三元组。边$E$可以由给定的$(V,F)$对表示。且$V,E,F$都可以与各种特征关联。（主要处理的是顶点的特征，即水平集标量值）

控制点可以通过输入顶点坐标，切法向输入为指定点归一化的向量（6维）。

**顶点特征向量生成：**

通过MLP将顶点水平集（标量）和约束编码为嵌入 向量。（非必须）

**梯度的计算：**

1. 使用GCN来近似离散流形上的微分算子。（ISoGCN在模型训练之前通过mse单独训练）

   $IsoGCN$是对欧式空间中进行的卷积，不能直接在2流形上使用，需要修改$IsoGCN$的卷积操作，使其可以处理网格，参考MeshCNN及其相关文章。

   优点：精度高，速度比有限元快。保证等方差变换，泛化能力强。（对于输入的一些变换，输入不变或等变）可以处理不完整的网格（？）

   缺点：需要将IsoGCN中的卷积操作从二维修改到流形

2. 使用有限差分来进行近似微分算子。

   使用网络来定义可学习的差分算子，来计算梯度和拉普拉斯。参考下面论文：

   `PHYSICS-AWARE DIFFERENCE GRAPH NETWORKS FOR SPARSELY-OBSERVED DYNAMICS`

   差分算子形式是固定的，里面的参数是学习出来的。

**水平集的更新：**

下面的方法使用的训练时都是有无监督训练。

1. **使用由IsoGCN模块搭建的神经求解器来对水平集函数进行数值求解。进行水平集更新。**

   模型为：循环的GNN

   求解器将各个近似微分算子和相应的系数组合起来，构成水平集演化的PDE，PDE中额外包括输入的约束，以及sdf约束、光滑性约束。

   水平集函数为：
   $$
   \frac{\partial \phi}{\partial t}  = - F(\nabla\phi) + \lambda_1E_{shape} +\lambda_2E_{topology}
   \\
   or \\
   \frac{\partial \phi}{\partial t}  = - \nabla(\lambda_1E_{shape} +\lambda_2E_{topology})(\nabla\phi)
   $$
   约束可以分为内部约束和外部约束，区别在于是否使用一二阶导。

   内部约束：

   eikonal约束：$E_{eikonal} = \int_M\frac{1}{2}(|\nabla\phi_i|^2 - 1)^2$

   光滑性约束：$E_{smooth} = |\nabla\phi_i|div(\frac{\nabla\phi_i}{|\nabla\phi_i|})$

   外部约束：

   插值点约束：$L_{interp} = \sum^{M}_{p}(\phi_i(x_p))$，M为插值点数量，$x_p$是插值点

   障碍点约束：$L_{obs} = - \sum^{N}_{p}(\phi_i(x_p))$ [?]，N为插值点数量，$x_p$是障碍点

   切线约束：$L_{tan} = \sum^{M}_{p}<\nabla\phi(x_p),\tau_p>$,M为插值点数量，$x_p$是插值点，$\tau_p$是插值点对应的输入切向量。

   eikonal约束：是否需要eikonal约束，需要的话怎么加？

   光滑性约束：可以直接加载水平集演化方程中。

   内部约束，不知道怎么加

   

   使用隐式欧拉来求解水平集函数，每一步使用求解器通过BB法来迭代求解下一个时间步的嵌入特征。最后解码输出。

   优点：用数值方法求解，但是使用GCN来计算微分算子，且迭代更新的是特征向量，比传统方法求解快,误差小。

   缺点：（约束不好加，训练过程不清晰。）

2. **使用优化的思想，将各种约束都加入能量函数，通过最小化能量函数，进行水平集更新。**

   模型为GNN+RNN

   将水平集演化看作优化问题，将各种约束作为损失函数：

   损失函数为：$\mathcal{E} = E_{shape} + E_{sdf} + E_{topology}$
   $$
   E_{shape} = w_{interp}E_{interp}(\phi) + w_{obs}E_{obs}(\phi) + w_{smooth}E_{smooth}(\phi) + w_{tan}E_{tan}(\phi)\\
   E_{sdf}(\phi) = \int_M \frac{1}{2}(|\nabla\phi|^2 - 1)^2dM
   $$
   

   用GCN的组合或有限差分求损失函数，使用梯度下降训练网络参数。可以通过RNN来循环表示水平集在每个时间步的值。

   两种方式：使用信号值计算损失函数、使用嵌入编码计算损失函数。

   1.  在RNN的每个循环中，将每个顶点的信号值编码，通过GNN对特征进行卷积（网格卷积），提取空间特征，然后解码为信号值，通过自定义差分操作来计算损失函数，更新水平集。
   2. 将每个顶点的信号值编码，在RNN的每个循环中，利用GCN或差分得到微分算子，计算损失函数，更新水平集的嵌入表示。在t个时间步后，解码输出。

   优点：无监督训练，不需要直接求解水平集函数方程。相关约束可以直接从优化的论文中迁移过来。

   缺点：

###### PENN与有限元方法的误差对比

![image-20241030171006324](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241030171006324.png)

![image-20241030171013541](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241030171013541.png)

![image-20241030171046332](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241030171046332.png)