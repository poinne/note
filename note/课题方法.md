

![image-20241120134508458](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241120134508458.png)



##### 模型训练1

这个方法的思想是，将外部约束转换为定义在网格顶点上的特征并结合网格的几何属性作为输入特征，训练一个GNN模型，根据网格对象和输入特征通过一次前向处理预测满足外部约束的定义在网格上的隐式曲线。

**数据集构造**



**输入数据格式：**

mesh数据：$(V,F)$

控制点：$V_{interp}、V_{obs}$

切法向：$list:(a_1,a_2...), a_i = \{v_i,n_i\}$(这里的顶点是插值点还是任意顶点？)

**mesh模型在网络中的表示形式：**

网格由$(V,F)$ 对定义，其中$V = \{v_1,v_2,...,\},v_n$是$R^3$中的顶点坐标集合。$F$定义连通性，是三角形网格的顶点三元组。边$E$可以由给定的$(V,F)$对表示。且$V,E,F$都可以与各种特征关联。（主要处理的是顶点的特征，即水平集标量值）

控制点可以通过输入顶点坐标，切法向输入为指定点归一化的向量（6维）。

**顶点输入特征向量构成：**

输入特征包括网格的几何特征以及根据外部约束构造的特征。

包括：坐标（三维）、法向（三维）、水平集值（1维）、趋近值（1维）

在输入模型前，先对模型进行预处理，根据插值点生成初始水平集。

根据插值点，通过bfs沿模型的edge生成一个无自交的闭环，然后使用fast marching 在顶点上构造初始符号距离场。闭环上顶点值为0，内部<0，外部>0。

趋近值根据插值点和障碍点，插值点小于0，障碍点大于0，其余位置通过高斯核函数平滑求出。

**模型架构**

![image-20241120135548751](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241120135548751.png)

模型架构为Unet结构，其中卷积模块为图卷积：
$$
F'_i = W_0 \times \mathcal{A}_{j \in \mathcal{N}(i)}(W_1 \times [F_j||v_{ij}||l_{ij}||sdf_{ij}||approch_{ij}]) \\
\mathcal{A} : max、mean、sum
$$
聚合算子使用max（？）

池化模块会同时考虑顶点的坐标和法向来进行顶点的聚合，顶点的法向可以防止薄面两侧的顶点被聚合。

同时会跟踪池化操作前后顶点之间的映射关系，用于上采样时顶点以连接关系的恢复。

顶点特征在通过unet结构卷积之后，会通过mlp得到预测的sdf增量值，通过与初始的sdf值进行相加，得到预测的sdf值。

**损失函数：**

水平集函数为：
$$
loss  = E_{sdf} + \lambda_1E_{shape} +\lambda_2E_{topology}
$$


符号距离约束：

$E_{sdf}$：$E_{eikonal} = \int_M\frac{1}{2}(|\nabla\phi_i|^2 - 1)^2dM$ ， 用于控制预测的水平集值的稳定性，防止标量场太陡或太平。

$E_{shape} = w_{interp}E_{interp}(\phi) + w_{obs}E_{obs}(\phi) + w_{smooth}E_{smooth}(\phi) + w_{tan}E_{tan}(\phi)$

形状约束分为内部约束和外部约束：

内部约束：

光滑性约束：$E_{smooth} = \int_{M}|\nabla\phi_i|div(\frac{\nabla\phi_i}{|\nabla\phi_i|})\delta(\phi)dM$ ， 用于控制零水平集构成的曲线的光滑性。其中，$\delta$ 为狄拉克函数，将所有非零的水平集点加权为零。

外部约束：

插值点约束：$L_{interp} = \sum^{I}_{p}(\phi_i(x_p))$，I为插值点数量，$x_p$是插值点

障碍点约束：$L_{obs} = - \sum^{O}_{p}(\phi_i(x_p))$ ，O为障碍点数量，$x_p$是障碍点

切线约束：$L_{tan} = \sum^{M}_{p}<\nabla\phi(x_p),\tau_p>$,M为插值点数量，$x_p$是插值点，$\tau_p$是插值点对应的输入切向量。

拓扑约束：
$$
E_{topology} = E_{noise} + E_{significant}\\
E_{noise} = \sum^N_{i = 1}b_i + \sum^N_{j = 1}(d_j - b_j),(b,d) \in 1-PD\\
E_{significant} = - \sum^S_{i = 1}(d_i - b_i) + \sum^N_{j = 1}(d_j - b_j),(b,d) \in 1-PD\\
$$


损失函数中包含水平集的梯度和拉普拉斯，在计算损失函数前需要先计算这些微分算子。

**微分算子的计算：**

1. 使用差分来近似微分

   水平集梯度：顶点的梯度为与顶点相连的边的梯度之和:
   $$
   V_i(\nabla) = \sum_j^{\mathcal{N}(i)}\frac{(sdf_j - sdf_i)\times (pos_j - pos_i)}{||(pos_j - pos_i)||}
   $$
   

   水平集的拉普拉斯：使用Laplace-Cotan公式:
   $$
   V_i(\Delta) = \frac{1}{2A_i} \sum_{j \in \mathcal{N}(i)} (\cot \alpha_{ij} + \cot \beta_{ij})(\text{sdf}_j - \text{sdf}_i),
   $$

2. 使用有限元计算

   精度高，但是速度慢，计算量大，且不确定能否用于模型训练过程。

3. 使用训练好的GCN近似这些微分算子。

   构造定义在mesh上的标量场，使用有限元得到对应的梯度场和拉普拉斯场作为ground true，通过有监督训练，训练IsoGCN，得到对应的模型近似对应的微分算子：

   根据水平集值预测对应梯度场：$IsoGCN_{0 \rightarrow 1}$

   根据水平集值预测对应拉普拉斯场：$IsoGCN_{0 \rightarrow 1 \rightarrow 0}$

   优点：精度高，速度比有限元快。保证等方差变换，泛化能力强。（对于输入的一些变换，输入不变或等变）可以处理不完整的网格（？）

   缺点：需单独训练IsoGCN

**水平集函数预测：**

根据得到的微分算子计算损失函数，通过pytorch框架的自动微分计算损失函数的求导，并进行梯度反向传播更新模型参数。

模型输出零水平集顶点满足条件的曲线通过水平集函数隐式表示，可通过marching  cubes算法提取出来。



##### 模型预测1

给定mesh网格、插值点序列、障碍点序列、顶点及预期切线序列，通过预处理操作得到输入的顶点特征向量，输入训练好的模型，通过一次前向过程，得到预测的水平集值，之后可以通过零水平集提取算法得到预测的曲线。





















**水平集的更新：**

下面的方法使用的训练时都是有无监督训练。

1. **使用由IsoGCN模块搭建的神经求解器来对水平集函数进行数值求解。进行水平集更新。**

   模型为：循环的GNN

   求解器将各个近似微分算子和相应的系数组合起来，构成水平集演化的PDE，PDE中额外包括输入的约束，以及sdf约束、光滑性约束。

   水平集函数为：
   $$
   \frac{\partial \phi}{\partial t}  = - F(\nabla\phi) + \lambda_1E_{shape} +\lambda_2E_{topology}
   \\
   or \\
   \frac{\partial \phi}{\partial t}  = - \nabla(\lambda_1E_{shape} +\lambda_2E_{topology})(\nabla\phi)
   $$
   约束可以分为内部约束和外部约束，区别在于是否使用一二阶导。

   内部约束：

   eikonal约束：$E_{eikonal} = \int_M\frac{1}{2}(|\nabla\phi_i|^2 - 1)^2$

   光滑性约束：$E_{smooth} = |\nabla\phi_i|div(\frac{\nabla\phi_i}{|\nabla\phi_i|})$

   外部约束：

   插值点约束：$L_{interp} = \sum^{M}_{p}(\phi_i(x_p))$，M为插值点数量，$x_p$是插值点

   障碍点约束：$L_{obs} = - \sum^{N}_{p}(\phi_i(x_p))$ [?]，N为插值点数量，$x_p$是障碍点

   切线约束：$L_{tan} = \sum^{M}_{p}<\nabla\phi(x_p),\tau_p>$,M为插值点数量，$x_p$是插值点，$\tau_p$是插值点对应的输入切向量。

   eikonal约束：是否需要eikonal约束，需要的话怎么加？

   光滑性约束：可以直接加载水平集演化方程中。

   内部约束，不知道怎么加

   

   使用隐式欧拉来求解水平集函数，每一步使用求解器通过BB法来迭代求解下一个时间步的嵌入特征。最后解码输出。

   优点：用数值方法求解，但是使用GCN来计算微分算子，且迭代更新的是特征向量，比传统方法求解快,误差小。

   缺点：（约束不好加，训练过程不清晰。）

2. **使用优化的思想，将各种约束都加入能量函数，通过最小化能量函数，进行水平集更新。**

   模型为GNN+RNN

   将水平集演化看作优化问题，将各种约束作为损失函数：

   损失函数为：$\mathcal{E} = E_{shape} + E_{sdf} + E_{topology}$
   $$
   E_{shape} = w_{interp}E_{interp}(\phi) + w_{obs}E_{obs}(\phi) + w_{smooth}E_{smooth}(\phi) + w_{tan}E_{tan}(\phi)\\
   E_{sdf}(\phi) = \int_M \frac{1}{2}(|\nabla\phi|^2 - 1)^2dM
   $$
   

   用GCN的组合或有限差分求损失函数，使用梯度下降训练网络参数。可以通过RNN来循环表示水平集在每个时间步的值。

   两种方式：使用信号值计算损失函数、使用嵌入编码计算损失函数。

   1.  在RNN的每个循环中，将每个顶点的信号值编码，通过GNN对特征进行卷积（网格卷积），提取空间特征，然后解码为信号值，通过自定义差分操作来计算损失函数，更新水平集。
   2. 将每个顶点的信号值编码，在RNN的每个循环中，利用GCN或差分得到微分算子，计算损失函数，更新水平集的嵌入表示。在t个时间步后，解码输出。

   优点：无监督训练，不需要直接求解水平集函数方程。相关约束可以直接从优化的论文中迁移过来。

   缺点：

###### PENN与有限元方法的误差对比

![image-20241030171006324](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241030171006324.png)

![image-20241030171013541](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241030171013541.png)

![image-20241030171046332](https://raw.githubusercontent.com/poinne/md-pic/main/image-20241030171046332.png)

