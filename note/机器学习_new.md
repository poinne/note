# 11111机器学习

#### 机器学习分类

###### 有监督学习

有监督学习的一般流程：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/1a.png" style="zoom:50%;" />

**分类问题**：是什么？

判定对象的类别，实现从向量到整数的映射

**回归问题** ：是多少？

根据向量预测出一个是实数值，实现从向量到实数的映射

- 贝叶斯分类器
- 决策树
- logistic回归
- 支持向量机/SVM
- 人工神经网络

###### 无监督学习

- 主成分分析
  - 寻找样本的主成分等价于求解样本协方差矩阵的特征值和特征向量
- 流形学习 - t-SN
- 聚类 - k-均值算法

**降维**

**聚类**

###### 半监督学习

**强化学习**

**数据生成问题**

- 从一个数据集学习出一个模型，这个模型可以生成随机样本，它们与训练集相似但又不完全相同

**生成对抗网络 - Generative Adversarial Network**

- 生成器：G，根据随机噪声生成样本数据，实现概率分布变换 

- 判别器：D，判定样本是真实的还是由生成器生成的

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/15a.png" style="zoom:50%;" />

**变分自动编码器 - Variational Auto-Encoder**

#### 机器学习过程

#### 基本概念

机器学习的本质是什么︰$y=\sigma (wx+b)$，在做一件什么事情，非线性变换（把一个看起来不合理的东西，通过某个手段（训练模型)，让这个东西变得合理)
非线性变换的本质又是什么?改变空间上的位置坐标，任何一个点都可以在维度空间上找到,通过某个手段，让一个不合理的点（位置不合理)，变得合理
这就是词向量的本质

**正则化**

正则化是一种用于控制模型复杂度的技术，其目的是防止模型在训练数据上过度拟合（过拟合）的问题。正则化通过向模型的损失函数中添加额外的项或惩罚来实现，这些项与模型的参数相关。正则化的主要目的是在保持模型在训练数据上表现良好的同时，防止模型过分适应噪声或不必要的细节，从而提高模型的泛化能力，使其在未见过的数据上表现良好。

以下是正则化的一些主要概念和常见形式：

1. **L1 正则化（L1 Regularization）**：也称为Lasso正则化，它向损失函数中添加了参数的绝对值之和。L1正则化有助于促使模型的某些参数变为零，从而实现特征选择，减少不相关的特征对模型的影响。
2. **L2 正则化（L2 Regularization）**：也称为Ridge正则化，它向损失函数中添加了参数的平方和。L2正则化有助于控制参数的大小，防止它们过于大，从而减轻了过拟合的风险。
3. **L1/L2 组合正则化（L1/L2 Regularization）**：有时候，可以同时使用L1和L2正则化，称为弹性网络（Elastic Net）正则化，以综合利用它们的优点。
4. **Dropout**：Dropout是一种特殊的正则化技术，它在训练过程中随机关闭一些神经元节点，以减少神经网络的复杂性，防止过拟合。关闭的节点在每个训练迭代中都是不同的。

正则化的作用和影响包括：

- **降低过拟合风险**：正则化通过控制模型参数的大小或稀疏性来减少模型在训练数据上的过拟合风险。这使得模型更有可能在未见过的数据上泛化良好。
- **特征选择**：L1正则化有助于选择重要的特征，将不重要的特征的权重降低甚至置零，从而提高模型的简洁性和可解释性。
- **减小权重**：L2正则化通过减小权重的幅度来平滑模型，使模型的参数更均匀分布，有助于避免过大的权重值。
- **增加模型泛化能力**：正则化有助于模型更好地适应不同的数据，提高了模型的泛化能力，使其在多个数据集上表现更好。



**前向传播与反向传播**

**前向传播：** 前向传播是指将输入数据通过神经网络，从输入层经过隐藏层到输出层，计算出网络的预测值。这个过程涉及以下几个步骤：

1. 将输入数据传递到输入层的神经元。
2. 在每个隐藏层的神经元中，计算加权和（权重与输入的乘积之和），然后将结果输入激活函数中，得到激活后的输出。
3. 将每个隐藏层的输出传递到下一层的隐藏层，直到达到输出层。
4. 在输出层计算最终的预测值。

**反向传播：** 反向传播是训练神经网络的关键步骤，用于计算损失函数关于网络参数的梯度，以便更新参数以最小化损失。这个过程涉及以下几个步骤：

1. 计算损失函数：将网络的预测值与真实标签比较，计算出模型的损失。
2. 计算梯度：从输出层开始，使用链式法则逐层计算损失函数关于每个参数的偏导数，得到参数的梯度。
3. 参数更新：使用梯度下降法或其变种，通过将参数沿着梯度的反方向移动一小步，更新网络的参数。

整个反向传播过程可以看作是一个逐层传播梯度的过程，将输出层的误差逐层反向传播到输入层，以计算各层参数的梯度。

前向传播和反向传播是神经网络训练过程中紧密相连的步骤。通过多次迭代前向传播和反向传播，模型逐渐调整参数，使损失逐渐减小，从而提高模型的性能和泛化能力。

**偏置：**

- 在神经网络中，偏置（Bias）是一个用于调整神经元激活的参数。每个神经元都有一个偏置，它与神经元的权重一起影响神经元的输出。
- 偏置可以被视为神经元的一个额外输入，其值在训练过程中会被优化，以便神经网络可以更好地拟合数据。偏置的引入使得神经元不仅仅是对输入的线性组合，还可以通过调整偏置来实现对输出的平移。
- 数学上，对于一个神经元来说，它的输入是输入特征的线性组合加上偏置，然后通过激活函数进行变换。假设一个神经元的输入特征是 $x = (x_1,x_2,...,x_n)$，权重是 $w = (w_1,w_2,...,w_n)$，偏置是 b，那么神经元的加权求和可以表示为：$z = \sum_{i=1}^nw_ix_i+b$
- 在训练过程中，神经网络会自动调整每个神经元的权重和偏置，以最小化损失函数，从而实现对数据的拟合。偏置的引入允许神经元在激活函数上进行平移，使得神经网络可以适应不同的数据分布和模式。

**二次代价函数**

二次代价函数（Quadratic Cost Function），也被称为均方误差（Mean Squared Error，**MSE**），是在机器学习和神经网络中常用的一个代价函数。它用于衡量预测值与真实值之间的差异，从而评估模型的性能。

对于回归问题，二次代价函数的数学表达式如下：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/2a.png" style="zoom:50%;" />

其中：

- $C$ 是代价函数的值。
- $n$ 是样本的数量。
- $y_i$ 是第 $i$ 个样本的真实值。
- $\hat{y}_i$ 是模型预测的值。

二次代价函数计算每个样本预测值与真实值之间的平方误差，然后对所有样本求平均。这意味着差异较大的样本将对代价函数的值产生更大的影响。

使用二次代价函数的优点包括：

- 它是一个连续可导的函数，因此在训练过程中可以计算梯度，进行优化。
- 它惩罚预测值与真实值之间的大误差，从而有助于模型更加关注那些预测值接近真实值的情况。

然而，二次代价函数也有一些缺点：

- 对于一些问题，当误差较小时，平方项会导致梯度变得很小，从而在训练过程中收敛缓慢。
- 它对离群值（outliers）敏感，因为平方误差会放大离群值的影响。

**交叉熵函数**

交叉熵函数（Cross-Entropy Function）是在机器学习和神经网络中常用的一个代价函数，特别适用于分类问题。它用于衡量预测概率分布与真实概率分布之间的差异，从而评估模型的性能。

对于分类问题，交叉熵函数的数学表达式如下：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/16a.png" style="zoom:50%;" />

其中：

- $C$ 是代价函数的值。
- $n$ 是样本的数量。
- $m$ 是类别的数量。
- $y_{ij}$ 是第 $i$ 个样本属于类别 $j$ 的真实概率值（通常是0或1）。
- $\hat{y}_{ij}$ 是模型预测的属于类别 $j$ 的概率值。

交叉熵函数的核心思想是：当真实概率分布与预测概率分布之间的差异越大时，交叉熵的值会变得越大。因此，模型在优化过程中会尝试降低交叉熵，从而更好地拟合真实数据的分布。

交叉熵函数的优点包括：

- 它对预测概率与真实概率的差异敏感，能够有效地引导模型进行分类任务。
- 它的梯度计算相对简单，有助于优化算法的收敛。

需要注意的是，交叉熵函数常用于多**分类问题**，而对于二分类问题，通常会将其简化为二分类交叉熵（Binary Cross-Entropy），也被称为对数损失函数（Logarithmic Loss）或逻辑回归损失函数（Logistic Loss）。

**抵抗过拟合的方法**

- 增大数据集

- Early stopping ：记录到目前为止最好的validation accuracy，当连续10个没有达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了

- Dropout：在深度学习训练时，每次训练时在隐藏层随机选取若干个神经元不参与训练（不生效），测试时全部都工作。

- 正则化项：在代价函数后面加上正则项。

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/21a.png" style="zoom:50%;" />

  参数稀疏化：一些参数为0，即可以看作不存在该参数。

**全连接**

在深度学习中，全连接（Fully Connected）也被称为密集连接（Dense Connection）或全连接层（Fully Connected Layer），是神经网络中最基本和常见的一种层类型。全连接层的作用是将输入数据的所有特征与每个神经元连接起来，实现从输入到输出的全连接关系。

全连接层的工作原理如下：

1. 每个输入特征都与网络中的每个神经元相连接，形成一个权重连接。
2. 每个连接都有一个对应的权重，用于调整输入特征对神经元的影响。
3. 神经元将所有输入特征与对应的权重相乘，然后将乘积相加，形成一个加权和。
4. 加权和通常会通过一个激活函数，如Sigmoid、ReLU等，产生神经元的输出。

在全连接层中，每个神经元都可以学习不同的特征组合，因为每个连接的权重都可以进行训练，从而使网络能够自动地从数据中学习到特征的表达方式。全连接层在图像识别、自然语言处理、推荐系统等各种深度学习任务中都有广泛的应用。

然而，随着神经网络的深化，全连接层可能会引入大量的参数，导致模型过于复杂，容易出现过拟合问题。因此，现代深度学习中，常常采用卷积层、池化层等结构来提取局部特征，然后再通过全连接层来进行最终的特征组合和分类。

**嵌入向量**

嵌入向量是一种将高维数据映射到低维连续向量空间的表示方法，用于捕捉数据的语义关系和相似性，以支持各种机器学习任务。

嵌入向量的主要特点是将高维离散数据（例如单词、图像像素等）映射到低维连续向量空间中，从而更好地捕捉对象之间的语义关系和相似性。在这个低维向量空间中，距离和方向都可以用来表示对象之间的关系，这有助于模型更好地理解数据的内在结构。

例如，在自然语言处理中，Word2Vec 模型将单词映射到连续向量空间中的嵌入向量，使得具有相似语义的单词在向量空间中的距离更近。同样，在计算机视觉中，图像嵌入向量可以将图像表示为一个连续的向量，从而实现图像的相似性比较和检索。

**插值**

插值是一种将图像从一个分辨率调整到另一个分辨率的常见方法，无论是放大还是缩小分辨率。

有多种插值方法可供选择，其中最常见的包括最近邻插值、双线性插值和双三次插值。选择哪种方法取决于具体的应用和需求。

- 最近邻插值：对于每个新像素，它选择最接近它的原始像素的颜色。这种方法计算速度快，但可能导致图像的锯齿状边缘，特别是在放大图像时。
- 双线性插值：对于每个新像素，它使用其周围4个原始像素的颜色的加权平均。这种方法比最近邻插值更平滑，通常会产生更高质量的结果。
- 双三次插值：对于每个新像素，它使用更多原始像素的颜色进行复杂的插值计算。这是一种高质量的插值方法，适用于需要高度保真度的图像调整。

**端对端训练（学习）**

**过度参数化（Over-parameterization）问题**

- **过度参数化** 是指神经网络的参数数量（例如权重和偏差）远远超过了数据的实际需求或训练样本的数量。这种情况通常会导致模型变得非常灵活，甚至可以在没有足够约束的情况下拟合任何数据。
- 当网络过度参数化时，它有太多自由度来适应训练数据，可能导致**欠约束优化问题**，即优化问题没有足够的约束来确保唯一的、物理上合理的解。这种情况下，网络可能会找到多个可能的解，导致训练不稳定或出现过拟合。

#### Diffusion Model

`基于denoising diffusion probabilistic models`

1. simple 一个充满噪音的图片，从normal distribution sample 出来的那个vector和要生成的图片维度一样（如

2. 通过Denoise不断生成逐渐清晰的图像，这个过程叫做reverse process。

   <img src="https://raw.githubusercontent.com/poinne/md-pic/main/3a.png" style="zoom:50%;" />

​	这里的Denoise Model用的是同一个，但是每次去噪不能只用同一个模型，因为传入的图片差异很大（如step1000 和step 2），需要再引入一个参数（表明是denoise的哪一步）

​	<img src="https://raw.githubusercontent.com/poinne/md-pic/main/17a.png" style="zoom:50%;" />

Denoise 内部

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/27a.png" style="zoom:50%;" />





diffusion framework共分为三部分：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/4a.png" style="zoom:50%;" />

分别为：Text Encoder 、 Diffusion Model(Generation Model) 、 Dencoder

- Text Encoder：
- Diffusion Model：
- Decoder：
  - Decoder

4. Frechet Inception Distance(FID)

   先将图像通过CNN生成对应的representation，然后将diffusion 生成的representation与之对比，如果

   <img src="https://raw.githubusercontent.com/poinne/md-pic/main/5a.png" style="zoom:50%;" />

### 线性模型

- 线性模型是机器学习中最简单、也是最常用的模型。多用于监督学习。

#### 线性预测模型

已知$x和t$都是可见变量，$t \approx y = \Theta(x)$,通过该公式表示一种$x$对$t$的预测模型。这一模型称为线性预测模型。$\Theta(.)$通常是非线性的。y为预测值，t为目标值。$\Theta$通常是一组基函数，目标是代入数据，求出方程组中的待定系数，使得预测值与目标值尽可能接近。

换句话说就是：输入变量和目标变量是可见的，建模的目的是基于输入变量对目标变量进行预测。

###### 线性回归

**线性回归模型的数学表示**如下：

$y = \beta_0 + \beta_1x_1 + \beta_1x_2 + ...+ \beta_nx_n + \varepsilon$

其中：

- $y$ 是因变量（要预测的值）。
- $x_1, x_2, \ldots, x_n$ 是自变量（特征）。
- $\beta_0, \beta_1, \beta_2, \ldots, \beta_n$ 是模型的参数，表示各个特征对应的权重。
- $\varepsilon$ 表示误差项，代表了模型不能完全解释的部分，即预测值与真实值之间的差异。

线性回归的目标是找到最优的参数值，使得模型的预测值与实际观测值之间的平方误差最小化。这个过程通常通过最小化残差平方和（Residual Sum of Squares，RSS）或最大化似然函数来实现。

**该模型的最小平方误差估计等价于假设目标变量的观察值t为以y为中心的高斯分布时的最大似然估计。**

- 引入**概率模型**来帮助我们描述实际问题中的不确定性，并由此得到**基于概率的最优解**。

- 输入变量x经过非线性映射生成特征向量$\Theta$，再经过线性映射生成预测y，最后加入一个高斯噪声得到观察值t，这以模型称为非线性回归模型。即$t = w^T \Theta(x) + \epsilon$.$\epsilon$ 是误差

- 误差是**独立**并且**具有相同的分布**，并且服从均值为0方差为$\Theta^{2}$的高斯分布(正态分布)    :（独立同分布）

- **似然函数**：什么样的参数跟我们的数据组合后恰好是真实值，$L(\Theta)=\prod_{i=1}^m p(y^{(i)}|x^{(i)};\Theta)$

- 对于单一样本点$(x_i,y_i)$的似然函数：

  $L(\beta_0, \beta_1, \sigma^2 | x_i, y_i) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2\sigma^2}\right)$

  模型对某一数据集 的描述能力越强，则该模型生成这一数据集的概率越大，似然函数的值也越大。

- **最大似然估计**的目标是寻找一组参数 $\beta_0, \beta_1, \sigma^2$，使得给定观测数据的似然函数最大化。通常我们会求解参数使似然函数取对数之后的对数似然函数最大化，这样可以方便计算和求导。(对数似然）
- 当数据具有明显的非高斯性时，线性回归模型中的高斯分布假设会产生较大误差，导致回归模型失效。e.g.数据时离散分布。这时可以考虑Logistic回归。

###### Logistic回归

- Logistic回归是一种用于解决二分类问题的线性分类模型。尽管名字中包含“回归”，但实际上它用于**分类任务**。在Logistic回归中，通过线性组合对输入特征进行加权求和，然后将这个线性组合通过一个称为函数（又叫Sigmoid函数）的激活函数来映射到0和1之间的概率值。这个概率值表示样本属于某个类别的可能性。
- 尽管Logistic函数是非线性的，但在逻辑回归中使用Logistic函数的模型仍然被称为线性模型，是因为模型的线性组合部分仍然是线性的，而Logistic函数的作用是在线性组合之后引入非线性变换。

- Logistic函数，也称为Sigmoid函数，是一种常用的激活函数。它将输入值映射到一个介于0和1之间的输出值，这使其特别适用于处理二元分类问题或概率估计。

- Logistic函数的数学表示为：

​		$\sigma(x) = \frac{1}{1 + e^{-x}}$

​		其中，$x$ 是输入值，$\sigma(x)$ 是对应的Logistic函数输出值。

在二分类问题中的Logistic回归模型如下式所示：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/6a.png" style="zoom:50%;" />

与线性回归模型相比，Logistic回归模型在线性模型基础上增加了个Logistic变换。如果假设观察值（分类标记）是以y为参数的伯努利分布，则该模型的最大似然估计**等价于一个以交叉嫡为准则的优化问题**。

**交叉熵 ：** 交叉熵函数（Cross-Entropy Function），也称为交叉熵损失（Cross-Entropy Loss）或对数损失（Logarithmic Loss），是在分类问题中常用的损失函数之一。它衡量了模型的预测结果与实际标签之间的差异，用于评估分类模型的性能。

在机器学习和深度学习中，交叉熵函数通常与 softmax 激活函数一起使用，以衡量模型在多类别分类问题中的预测结果与实际标签之间的差异。交叉熵函数的形式如下：

对于单个样本：	$L(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)$

其中：

- $y_i$ 是实际标签中第i类的值（0 或 1，表示是否属于该类别）。
- $\hat{y_i}$是模型预测输出中第i类的值，通常是 softmax 函数的输出。

对于整个训练集的交叉熵损失，可以将所有样本的交叉熵损失相加，然后取平均。

交叉熵函数的特点：

- 预测错误时，交叉熵损失会迅速增大，这对于梯度下降等优化算法来说是有利的，因为它可以加快参数的更新。
- 交叉熵损失在分类问题中通常比均方误差等损失函数更适用，因为它在模型的预测结果与实际标签差异较大时，对模型进行更强的惩罚。

**Logistic回归模型**：首先对输入$x$经过一个非线性映射$\Theta(.)$生成特征，再经由一个线性映射$w^T\Theta$投影到一个标量空间，再经过$\sigma(.)$压缩到（0，1）之间，最后以该压缩值作为伯努利分布的参数生成目标t。

- 与线性回归模型相比，Logistic回归模型增加了一个非线性映射函数$\sigma(.)$，并用伯努利分布代替了高斯分布假设。
- 利用GD对Logistic回归中的交叉嫡函数$L(w)$进行优化,求$L(w)$函数的最小值。

**先验知识**：是指在进行推断、预测或分析之前已经存在的关于某个事物、问题或参数的知识或信息。这种知识通常是基于以前的经验、领域专业知识、先前的观察、数据分析等得出的，而不是直接从当前的数据或观察中得出的。可以将这些知识纳入模型中，以更准确地预测。

**梯度下降法**（Gradient Descend, GD）：是一种通用的函数优化方法。设有函数$f(w)$，优化的目标是找到一个$w*$使得该函数的取值最小。梯度下降法从一个随机的w开始进行迭代优化，每一步t选择一个使$f(w)$下降最大的方向，并往该方向前进步长$nt$。因为使f(w)下降最大的方向即是$f(w)$在$w^t$点的梯度方向，因此该方法称为梯度下降法。如果步长$nt$,选的合理，梯度下降法可以保证收敛到局部最优。

- GD 的变体有 BGD（批量梯度下降，慢、准）、SGD（随机梯度下降，快、不准）、MBGD（小批量梯度下降，较快、较准）

- 学习率:梯度每次变化的多少。因为学习率的大小对梯度下降的影响非常大，因此有许多针对学习率的算法：
  - AdaGrad - 动态学习率
  - RMSProp - 优化动态学习率
  - AdaDelta - 无需设置学习率
  - Adam - 融合AdaGrad和RMSProp
  - Momentum - 模拟动量（带惯性的）
  - FTRL

**Softmax函数:**

- 是一种常用的激活函数，通常用于多类别分类问题中的输出层。
- 它将一组实数转换为表示概率分布的正数值，使得每个类别的预测概率在0到1之间，并且所有类别的概率之和为1，从而方便进行多类别分类。
- 将输入值映射为非负数，并保证了**所有类别的概率之和为1**，从而可解释为**概率分布**。
- 倾向于放大输入向量中最大的元素，同时压制其他元素，使得最大元素对应的类别概率更接近1，其他类别概率更接近0。
- Softmax函数与交叉熵损失（Cross-Entropy Loss）常常结合使用，用于计算模型预测与实际标签之间的差异。

#### 线性概率模型

在很多推理任务中x是不可见的，即隐变量，此时不能直接根据目标变量t和输入变量x建立模型。

定义x和t之间具有如下线性关系：

$ t = Wx + \epsilon$

其中W是参数矩阵，$t,x,\epsilon$都是随机变量，分别代表观察量、隐变量和随机噪声。这一模型称为线性概率模型。

线性概率模型和线性预测模型具有类似形式，区别在于**线性概率模型中x是不可见的隐变量，而在线性预测模型中x是可见的输入值**。这一区别很重要:当x变成隐变量以后，我们能观察到的数据只有t，因此学习方式由监督学习变成了无监督学习，推理过程也由前向预测变成了反向推理。

线性概率模型建立了一种对观察变量t进行概率描述的框架，在这一框架下，每当观察到一个t时，可以依定义好的线性关系去推理数据背后的隐藏原因x。

###### 主成分分析（PCA）

**概念**

PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。

PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。

PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。

**如何获得主成分方向**

通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。

由于得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：**基于特征值分解协方差矩阵**实现PCA算法、**基于SVD分解协方差矩阵**实现PCA算法。

- PCA是经典的无监督学习方法，广泛应用于降维、正规化、流形学习等任务中。

- PCA希望找到若干相互正交的方向，使得观察数据在这些方向上的映射最大可能地代表原数据的分布性质。这些方向称为主成分(Principle Caompo-nent，PC)。一般来说，选择有限的几个主成分即可很好地代表原数据，因此PCA常用在数据降维中。
- 主成分可以基于方差最大准则得到。这一准则的思路是，数据在一个映射空间中的方差越大，则说明该空间对数据的代表性越强。因此，主成分应该是使得数据在由其所构成的映射空间中方差最大的方向。
- 所有主成分都是协方差矩阵的特征向量，且加入每一个主成分后所增加的方差等于该主成分对应的特征值。
  因此，若要求前K个主成分，只需选择特征值最大的K个特征向量即可。
- 概率主成分分析（PPCA）：用线性概率模型来解释PCA

###### LDA

- LDA是基于监督学习方法，用于分类任务。

**PCA与LDA对比**

- 解释：
  - PCA 是一种降维技术，旨在将原始特征空间映射到一个新的低维特征空间，使得数据在新空间中的**方差最大化**。它通过计算数据的主成分（principal components）来实现，主成分是原始特征的线性组合，按照数据方差的降序排列。
  - LDA 也是一种降维技术，但它更加关注**在降维的同时保留类别信息**。它寻求在新的低维特征空间中**最大化类别之间的差异，最小化类别内部的差异**。LDA 通过计算**类别之间散布矩阵和类别内部散布矩阵**的特征向量来实现。
- 相同点：PCA 和 LDA 都可以用于降维，从而减少特征的数量，简化模型和数据分析。
- 不同点：
  - PCA 是一种**无监督降维方法**，不考虑样本的类别信息，仅关注数据的方差分布。它适用于**无监督学习**任务，如数据压缩和可视化。
  - LDA 是一种**有监督降维方法**，利用类别信息，适用于**分类任务**。它可以用于减少特征维度并增强分类辨别能力。

**参考文章**

[主成分分析（PCA）原理详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/37777074)

###### SVD

[机器学习中SVD总结 (qq.com)](https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg)

###  神经网络

**神经元（Neuron）：**

- 神经元是神经网络的基本组成单元，模拟了生物神经元的工作原理，用于处理和传递信息。
- 在神经网络中，神经元接收输入数据（通常是来自前一层的输出），对输入数据进行线性组合（乘以权重并加上偏置），然后将这个线性组合通过激活函数进行非线性变换。
- 神经元的工作类似于一个加权求和器，它将输入特征与相应的权重相乘，然后将它们的和进行激活函数处理，产生输出。

**激活函数（Activation Function）：**

- 激活函数是应用于神经元的输出的非线性函数，它的作用是为神经网络引入非线性变换，从而使神经网络能够学习和表示更复杂的模式。
- 激活函数将神经元的加权和进行变换，产生一个新的输出。这个输出将作为下一层神经元的输入。
- 常见的激活函数包括Sigmoid函数、ReLU函数、tanh函数等。

**联系：**

- 神经元的输出经过激活函数的变换后，成为下一层神经元的输入。因此，激活函数起到了将线性组合的结果转化为非线性输出的作用。
- 激活函数的引入使得神经网络具备了处理复杂非线性关系的能力，从而能够拟合更广泛的数据模式。
- 在神经网络的前向传播过程中，每个神经元的输出都经过激活函数的处理，然后传递给下一层神经元，最终形成网络的输出。
- 一般情况下，**在神经网络的训练过程中，激活函数的参数是保持不变的**，而神经网络的参数（如权重和偏置）会根据损失函数进行优化，以使网络能够更好地拟合训练数据。

###### 人工神经网络（ANN)

ANN通过一系列嵌套的非线性函数来学习复杂的非线性变换，使得经过该变换后得到的变量得以通过较简单的模型（特别是线性模型）进行预测和分类。

将Logistic回归模型中的Logistic函数替换为如下阶跃函数：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/7a.png" style="zoom:50%;" />

则对二分类问题的预测为：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/23a.png" style="zoom:50%;" />

其中$y \in \{-1,+1\}$代表预测结果。这一模型即是**感知器模型**。Logistic回归模型和感知器模型本身都是非线性模型，但和线性模型很接近，可称为**近线性模型**。

线性回归模型存在闭式解，即通过数学公式可直接求出参数**$w$**的最优值。对于近线性模型或更复杂的模型，一般不存在闭式解，这时通常采用数值解法，通过迭代逐渐逼近最优解。梯度下降法是最常用的数值解法，通过求目标函数（平方误差或交叉嫡)对参数的梯度，选择合适的步长，将参数沿梯度方向做小量变动，可得到比当前解更优的解。这一过程迭代进行，当步长选择合理时，可得到局部最优解。

**线性不可分问题**：在特征空间中，无法通过一个单一的线性超平面将不同类别的数据完全分开的问题。换句话说，无法找到一个线性决策边界来将不同类别的数据点正确分类。

在二元分类问题中，如果数据点在特征空间中的分布呈现出复杂的几何形状，如环形、螺旋状等，那么很可能**无法使用一个简单的直线、平面或超平面将这些数据点正确分类。这种情况下，称数据是线性不可分的。**

对于线性不可分问题，传统的线性分类器（如线性回归、感知机）无法有效地进行分类，因为它们只能找到一个线性的决策边界。

解决方法：对传统线性模型（包括感知器）进行非线性扩展，将输入变量通过非线性变换映射到变换空间，并在变换空间建立线性模型，即 ： 

$y(x) = w^T\phi(x)$

其中$\phi(x)$是对$x$的非线性变换函数。根据变换函数的形式不同，可以得到不同的非线性模型。几种典型的非线性变换和其对应的模型如下：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/8a.png" style="zoom:50%;" />

###### 多层感知器

将传统的一层感知器模型扩展到多层即得到多层感知器（Multi-layer Perceptron，MLP）模型。

多层感知器又叫多层前馈网络

MLP基于函数嵌套(Function Composition）设计非线性变换$\phi(x)$，每一层变换函数包括一个简单的线性映射和一个非线性激发函数。

当前通用的MLP模型不再采用阶跃函数作为输出函数，而是采用线性或Logistic函数，这些函数是连续可导的，因此可基于梯度下降算法进行优化;同时，这些输出函数对应不同的数据分布假设，可分别对回归任务和分类任务进行建模。因此，与其说MLP是一层感知器的扩展，不如说是对线性回归和Logisitc/Softmax回归的扩展。

在MLP中，标准的目标函数是对训练数据的生成概率最大化。

**模型结构**

多层感知器是对线性回归模型和近线性分类模型的扩展。从结构上看，MLP将线性模型的一层网络扩展到多层，每一层输出经过一个非线性变换后作为下一层的输入，由此得到一个信息逐层传导的前向网络(Feed-Forward Network)

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/28a.png" style="zoom:80%;" />

该模型的计算过程可形式化如下：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/13a.png" style="zoom:50%;" />

其中$g(·)$和$\widetilde{g}(.)$分别为隐藏层和输出层的**激发函数**(Activation Function)，$z_j$为第j个隐藏节点的激发值。注意该网络第二层相当于以隐藏层激发值为输入的线性或近线性模型，因此，在回归任务中，激发函$\widetilde{g}$一般取线性函数，在分类任务中，一般取Logistic函数或Softmax函数。

多层结构和非线性激发函数使得MLP的目标函数变得非常复杂，一般不能得到解析解，因此通常采用数值解法。最常用的方法是梯度下降(GD)法。但由于GD算法每次迭代都要在整个数据集上计算梯度，效率较低，因此实际应用中多采用随机梯度下降法( Stochastic Gradient Descent，SGD)。SGD和GD类似，都是依目标函数的梯度方向对参数进行迭代调整。不同的是，**SGD每次迭代并非基于全体数据集计算目标函数的梯度，而是随机选择一部分数据进行学习。** 注意：不论是GD还是SGD，都只能达到局部最优。

MLP进行参数优化时可以使用反向传递算法（Backpropagation，BP），BP算法利用了MLP的层次结构、导数的链式法则和动态规划算法对参数进行顺序求导，避免了重复计算。

BP算法的基本步骤如下：

1. **前向传播：** 输入一个样本数据，通过网络的每一层进行计算，得到输出结果。每一层的输出是下一层的输入。
2. **计算误差：** 计算网络的输出值与真实标签之间的误差，通常使用损失函数来衡量误差。
3. **反向传播误差：** 从输出层开始，计算每一层的误差贡献，然后将这些误差传播回到前一层，一直传播到输入层。这是 BP 算法的核心步骤，其中每一层的误差贡献计算依赖于权重和激活函数。
4. **更新权重和偏置：** 使用计算得到的误差梯度，根据梯度下降等优化算法，更新网络的权重和偏置，以减小误差。这使得网络的预测逐渐趋近于真实标签。
5. **重复训练：** 重复进行前向传播、误差计算、反向传播和权重更新，直到网络收敛或达到预定的训练迭代次数。

BP 算法使神经网络能够通过多次迭代来逐渐调整权重和偏置，从而逼近目标函数最小值，即最小化预测值与真实标签之间的误差。虽然 BP 算法在训练神经网络中非常有效，但也需要注意过拟合、梯度消失等问题。

###### 径向基函数网络

MLP基于函数嵌套设计非线性变换$\phi(x)$，每一层变换函数包括一个简单的线性映射和一个非线性激发函数。

径向基函数(Radio Basis Function,RBF)网络基于另一种思路实现这一线性变换。该方法在变换空间设计一系列标识点(Anchor Points）$\{v_j\}$，基于这些标识点，可以将每一个采样点**x**用该点到$\{v_j\}$之间的距离表示出来，从而实现由原始空间到变换空间的非线性变换。每个标识点$v_j$代表变换空间中的一个基(Basis)，以$v_j$为参数的距离函数称为一个径向基函数，即RBF。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/10a.png" style="zoom:50%;" />

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/11a.png" style="zoom:50%;" />

###### 先验知识引入神经网络模型

标准MLP模型是一种全连接结构，具有强大的学习能力，但这一模型缺少先验知识，是一种纯数据驱动方法。这意味着网络训练需要大量数据，且容易发生过训练或欠训练。

将先验知识引入神经网络可以降低模型复杂度，提高训练效率。

**结构化模型与卷积神经网络**

很多数据具有结构化特征（如图像、音频），可用来设计更有针对性的网络。

卷积神经网络(Convolutional Neural Network, CNN）具有很强的代表性。CNN是利用上述结构化特性设计局部的、共享的网络子结构，该设计使得每个子结构可学习某种局部模式，且不同空间、时序、频域位置的子结构共享网络参数。

CNN的相关概念如下：

1. **卷积层：** 卷积层使用**卷积操作**对输入数据进行**特征提取**。卷积操作可以理解为在输入数据上滑动一个小窗口（卷积核）来进行**局部特征提取**。这样，卷积层可以捕捉图像中的边缘、纹理等局部特征，因此在图像处理中表现出色。
2. **池化层：** 池化层用于**降低卷积层输出的空间维度**，同时保留重要信息。最常见的池化操作是最大池化，即在每个小窗口中选择最大的值作为池化结果。池化可以减少计算量，增加模型的鲁棒性。
3. **激活函数：** CNN 中的激活函数（如ReLU）引入了非线性变换，使网络能够学习复杂的非线性模式。
4. **卷积核和滤波器：** 卷积核是卷积层的参数，它们是小型的矩阵，用于在输入数据上进行卷积操作。通过学习，网络可以自动学习提取不同特征。每个卷积核在不同位置都共享相同的权重。这使得卷积神经网络能够处理大规模的图像数据，同时保持较少的计算开销。
5. **多通道：** 在卷积神经网络中，输入数据通常由多个通道组成（例如，RGB 图像具有红、绿、蓝三个通道）。卷积核也是多通道的，这使得网络能够同时处理不同的特征。
6. **卷积层堆叠：** CNN 可以通过堆叠多个卷积层来提取不同层次的特征，从低级的边缘特征到高级的语义特征。

CNN特点如下：

1. 使用卷积核可以学习重复性局部模型，因而可起到特征提取作用。
2. 降采样增加对空间、时间、频域上轻微形变的鲁棒性。
3. 参数量少，训练容易。
4. 可以基于先验知识（如模式的大小）设计卷积核，因而有利于将知识结合到网络结构中，避免盲目学习。

**混合密度网络**（Mixture Density Network, MDN）

**定义**：基于对数据分布的先验知识建立适当的概率模型，用神经网络来预测模型的参数。

这种方式可以将概率模型的描述能力和神经网络的学习能力相结合，由概率模型表达关于数据分布的先验知识，由神经网络增强参数估计能力。

**贝叶斯方法**[未看]

贝叶斯方法可以通过对模型权重引入不同的先验概率，在模型训练过程中引入对应的先验知识，以减少神经网络在建模和训练中的盲目性。



###### 基于记忆的的神经模型

**定义:**在一些问题中仅有数据x而没有明确的数据标记t，我们希望设计一个模型可以描述x的分布，或者得到代表x的抽象特征。对这类任务，基于映射的神经网络并不适合。研究者提出各种基于记忆的神经网络来处理这一问题。当网络训练完成后，该网络即可代表数据的隐藏结构。

**作用：**

- 对一个含有噪声的测试样本，通过提取近似样本，可以起到去噪效果;
- 如果训练样本足够多，这一网络可以学习样本中有价值的模式，进而提取有效特征。

**典型的神经记忆模型：**

**自编码器**（Auro Encoder，AE）

一个标准AE包括两个部分:一个编码器(Encoder）和一个解码器(Decoder)，其中编码器$f_{\phi}(x)$将原始数据$x$编码到一个特征空间，生成特征$h$，解码器$g_{\phi}(h)$基于特征$h$对原始数据进行重构$\widehat{x} = g(h)$。

一个典型的AE结构如下所示。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/56a.png" style="zoom:50%;" />



**Kohonen网络** [未看]

**Hopfield网络**[未看]

**玻尔兹曼机**[未看]

**受限玻尔兹曼机**[未看]

###### 基于过程的模型[未看]

###### 序列对序列网络[未看]

###### 基于Attention模型的诗词生成[未看]

###### 神经图灵机[未看]

**总结**

- 神经模型分为四种:

  - 用来学习输入-输出关系的映射模型

  - 用来学习内部模式的记忆模型

  - 用来学习时序过程的动态模型

  - 用来学习复杂操作的神经图灵机

### 深度学习

###### 深度学习优势

1. 深层网络有比浅层网络更强大的函数表达能力。

2. 深层网络的层次学习方式可以从原始数据中抽象出典型特征，这和人类神经系统处理信息的方式一致。

3. 深度学习提供了基于非监督方法进行特征学习的有效方式。

4. 深度学习已经超越特征学习的范畴，成为一种知识积累和过程学习的有效工具。

**优势1：网络表达能力**

通用近似定理：在很宽松的假设下，一个包含一个隐层的神经网络能近似任何连续函数。

虽然根据通用近似定理，可以用简单的单隐藏层网络就可以做所有事情，但要逼近一个复杂函数，仅用一层网络往往需要大量的隐藏节点，这会带来参数量的大幅提高，不仅增加了对训练数据的需求，也会导致模型泛化能力的下降。

深度学习通过加深网络层数，可以用相对较少的参数量对函数进行近似。这是因为每一层是一个函数映射，层与层之间的关系相当于函数嵌套，这种嵌套可实现网络表达能力的指数级提高。

**优势2：层次表示与特征学习**

层次性学习是非常重要的，一个深层神经网络确实能学习到层次性特征：在网络底层可能只是些原始特征，越往高层越抽象，越具有不变性。并且这种自动学习的特征往往比人为设计的特征更有任务相关性，对环境变化的鲁棒性也更强。

**优势3：显著特征的非监督学习**

显著特征 ： 具有明显一致性，不会发生太大变化的特征。

非显著特征 ： 有很强的变动性的特征。

端到端学习：深度学习是一种将特征提取和分类模型有机结合在一起的学习方式，是一种将非监督学习和监督学习统一在一起的学习方式。这一特性显著区别于传统机器学习中将特征提取和统计建模独立对待的二分思路，特征和模型不再有清晰的分界，而是对数据进行逐层递进处理，直到任务目标得以完成。这种将特征和模型统一到一起的学习方式常被称为端到端学习。

**优势4：复杂结构与数据驱动**

**复杂结构：**今天的深度学习已经远远不止多层前馈网络(MLP)，而是包括卷积网络（CNN)、递归网络（RNN)、记忆网络(Memory Net）等各种丰富的网络结构。这些各异的网络结构可以灵活组合在一起，形成更为复杂和强大的神经模型。这一模型不仅可以学习特征映射，还可以记忆知识、刻画动态过程、学习推理方法等。

**数据驱动：**不确定性进行刻画和分解。深度学习的成功减少了认为知识在建模中的参与程度，让机器从数据中自动发现知识和规律，实现了更普遍的数据驱动。这一观念上的转变显然源于神经模型的普适性:将一些同质神经元通过链接组合在一起，只要数据足够充分，即可超过人们精心设计的各种特征提取方法和复杂的模型结构，这事实上表明数据中蕴含的知识有可能超过人为设计所能覆盖的范围。

**端到端学习：**"端到端学习"（End-to-End Learning）是一种方法，旨在通过单个统一的模型来直接从原始输入数据中学习任务的映射，而无需手动设计多个阶段的特征提取或处理过程。这意味着整个学习过程从输入开始，一直到输出结束，形成了一个“端到端”的过程，减少了人工干预和手动特征工程的需求。

端到端学习的优点包括：

1. **简化流程：** 端到端学习可以减少手动设计特征和流程的需求，使整个系统更加简化。
2. **自动特征学习：** 深度神经网络可以自动学习从输入到输出的复杂映射，以及中间层的特征表示。
3. **更少的人工干预：** 由于不需要多个阶段的设计和调整，端到端学习可以减少人工干预，减轻了构建复杂系统的负担。
4. **更好的性能：** 在某些情况下，端到端学习可以实现更好的性能，因为它可以在数据中发现更复杂的模式和关系。

###### 深度神经网络训练[未看]

###### DNN训练技巧[未看]

###### 神经网络的正则化



###### 核函数

- 核方法（Kernel Method）是另一种对映射函数$\phi$进行设计的方法。与特征学习不同，核方法不对$\phi$做显式的表示或学习，而是通过数据间的相关性函数$k(x,x')$对$\phi$进行隐式定义，即：

  $k(x,x') = \phi(x)^T\phi(x')$

  相关性函数$k(x,x')$称为**核函数**。

优势如下：

- 该方法只关注数据之间的关系，而不是数据本身，因此特别适合数据对象难以用向量明确表达的任务。
- 由$k(x,x')$引导出来的特征空间p可能具有非常高的维度，甚至是无限维，因此可以满足复杂数据在特征空间中线性化的要求。
- 特征空间中的模型是线性的，因此模型训练是一个凸优化问题，可保证得到全局最优解。



###### 聚类-k-均值算法

k-均值算法是一种常用于聚类分析的无监督机器学习算法。它的目标是将一组数据点划分为不同的簇，使得同一簇内的数据点彼此相似，而不同簇之间的数据点相似度较低。

算法的基本步骤如下：

1. **初始化**：选择要形成的簇的数量 k，然后随机选择 k 个数据点作为初始的簇中心，中心向量为$\mu_1,...\mu_k$。
2. **分配数据点**：对于每个数据点$x_i$，计算其与各个簇中心$\mu_j$的距离，$d_{ij} = ||x_i - \mu_j||$，并将其分配到距离最近的簇。
3. **更新簇中心**：对于每个簇，计算簇内所有数据点的平均值，将这个平均值作为新的簇中心。
4. **重复步骤2和3**：重复执行步骤2和3，直到簇中心不再发生显著变化或者达到预定的迭代次数。

k-均值算法的优点包括简单易懂、易于实现和计算效率高。

**缺点：**对初始簇中心的选择敏感，可能会陷入局部最优解，对于非凸形状的簇分布效果可能不佳，还可能受到异常值的影响。

###### CNN

传统BP网络处理图像时的问题：

1. 权值太多，计算量太大
2. 权值太多，需要大量样本进行训练

CNN通过**局部感受野**和**权值共享**减少了神经网络需要训练的参数个数

- 局部感受野：每个神经元并不与全部的输入参数相连，只连一部分（类似于人看一张大的图片，不能同时看到全部，只能是看清一部分，其余部分是不清晰的）
- 权值共享：所有神经元的权值是共享的（每个卷积核中的权值共享，但卷积核之间的权值不共享，如果对多个卷积核进行卷积操作，则这几个卷积核之间的权值共享）

因为是权值共享的，所以需要训练的参数个数与卷积窗口的大小有关，和图片的像素多少无关

**卷积计算**(提取特征)

![](https://raw.githubusercontent.com/poinne/md-pic/main/57a.png)

步长：每次卷积窗口走的距离（走的时候线行后列）

不同的卷积核在同一张图片上提取出的特征不同

卷积核上的参数不需要人为计算，是通过大量训练得出的。

卷积操作（Convolution）：

- 卷积操作是一种特征提取操作，用于从输入数据中提取局部信息。
- 在图像处理中，卷积操作通过将一个小的滤波器（也称为卷积核）在输入图像上滑动，将滤波器的权重与输入图像的局部区域进行点积操作，从而生成一个新的输出特征图。
- 卷积操作有助于捕捉输入数据中的空间局部关系，例如图像中的边缘、纹理和特定形状。
- CNN中通常包括多个卷积层，每个卷积层可以使用不同大小的卷积核来提取不同尺度和抽象级别的特征。

**池化Poling**（在保持特征的同时降维）

可以看作进一步的特征提取。

Pooling常用的三种方式：

1. max-pooling (提取特征窗口中的最大值，得到 feature map)
2. mean-pooling (提取特征窗口中的平均值，得到 feature map
3. stochastic pooling (随机提取特征窗口中的一个值，得到 feature map)

池化操作（Pooling）：

- 池化操作用于减小数据的空间维度，降低计算复杂度，并提高模型的鲁棒性。
- 最常见的池化操作是最大池化（Max Pooling）和平均池化（Average Pooling）。在最大池化中，池化层将每个局部区域的最大值作为新的特征，而在平均池化中，池化层将每个局部区域的平均值作为新的特征。
- 池化操作有助于保持特征的平移不变性，使模型对输入数据的微小位置变化具有鲁棒性。
- 池化操作还可以减小特征图的维度，减少后续神经网络层的参数数量，从而降低过拟合的风险。

**卷积操作的优势：**

1. 特征提取：卷积操作是主要用于特征提取的操作，它可以捕捉输入数据中的局部模式和结构，这对于图像、语音和文本等数据类型都很重要。例如，图像中的边缘、纹理和形状，文本中的n-gram特征等。
2. 参数共享：卷积操作通过在输入数据的不同位置共享权重来减少模型参数数量。这减轻了模型训练的负担，降低了过拟合的风险，提高了模型的泛化能力。
3. 空间局部性：卷积操作保留了输入数据的空间局部性，使模型能够感知输入数据中的空间关系。这在处理图像和序列数据时非常有用。

**池化操作的优势：**

1. 降低计算复杂度：池化操作通过减小特征图的尺寸，降低了后续神经网络层的计算复杂度。这使得神经网络更容易训练和部署。
2. 平移不变性：池化操作有助于保持特征的平移不变性，即输入数据的微小平移不会显著改变池化后的特征。这有助于模型在处理不同位置的特征时保持一定的鲁棒性。
3. 减少过拟合：通过降低特征图的尺寸，池化可以减少模型的参数数量和模型的容量，有助于降低过拟合的风险，特别是在数据量有限的情况下。

**Padding**

1. same padding:
   - 给平面外部补0，卷积窗口采样后得到一个跟原来大小相同的平面
   - 如果步长为2，可能会给平面外部补0，得到比原来平面大的平面。
2. valid padding:不会超出平面外部，卷积窗口采样后得到一个比原来平面小的平面

**feature map**

在CNN中，特征图是指由卷积层生成的二维数据结构，它是输入图像经过卷积操作后的输出。每个特征图都代表了一个特定的特征检测器或过滤器在输入图像上的响应模式。这些特征检测器可以捕捉到图像中的不同特征，如边缘、纹理、形状等。在卷积层中，通常会使用多个不同的过滤器来生成多个不同的特征图，每个特征图都捕捉了输入图像的不同特征信息。

特征图的大小通常取决于卷积操作的参数设置，包括卷积核的大小、步幅（stride）和填充（padding）等。特征图的深度（通道数）等于卷积核的数量。

在卷积神经网络中，特征图的生成是逐层进行的，通过多次卷积、池化（pooling）和激活函数等操作，网络逐渐学习到更抽象和高级的特征表示，从而使网络能够有效地识别和分类图像中的对象或模式。

**Upsampling**

"Upsampling"（上采样）是在计算机视觉、图像处理和深度学习中常用的操作，用于增加图像或特征图的尺寸。这个操作通常用于将图像或特征图从低分辨率放大到高分辨率，或者在神经网络中进行一些上采样操作以增加特征图的空间维度。

有几种常见的上采样技术：

1. **最近邻插值（Nearest Neighbor Interpolation）**：在最近邻插值中，对于每个要上采样的像素，将其值设置为在原始图像或特征图中最接近的像素的值。这是一种简单的上采样方法，但可能导致图像的块状伪影，特别是在大幅度上采样时。
2. **双线性插值（Bilinear Interpolation）**：双线性插值是一种更平滑的上采样方法，它会考虑到每个像素周围的四个最近像素，通过加权平均来计算新像素的值。这通常比最近邻插值产生更平滑的结果。
3. **双三次插值（Bicubic Interpolation）**：双三次插值是一种更高阶的插值方法，它考虑了每个像素周围的16个最近像素，并使用更复杂的加权平均来计算新像素的值。这通常用于更高质量的上采样需求，但计算代价也更高。
4. **转置卷积（Transpose Convolution，也称为反卷积或分数步幅卷积）**：在神经网络中，转置卷积操作可以用于进行上采样。它会根据网络的参数来学习如何将低分辨率特征图映射到高分辨率特征图。转置卷积操作允许更灵活的上采样，但需要更多的参数和计算资源。

上采样通常用于图像分割、对象检测、语义分割等计算机视觉任务中，以便将低分辨率的特征图扩展到与输入图像相同的分辨率，或者用于生成高分辨率图像。它还用于生成生成对抗网络（GANs）中的图像生成任务，以及在一些神经网络架构中进行特征融合操作。

**Skip-connection**

Skip-connection（跳跃连接）是深度学习中的一种重要结构，它通常用于改善神经网络的性能和训练稳定性，特别是在**处理深层网络**时。Skip-connections 也被称为跳跃链接、残差连接或快捷连接。跳跃连接是一种让深度神经网络更聪明的方法。

Skip-connections 的主要思想是**在神经网络的不同层次之间建立直接的连接**，允许信息在网络中跳过一些层次并更快速地传递。这有助于解决深层网络训练中的梯度消失和梯度爆炸问题，使得网络更容易训练。最常见的 Skip-connection 结构是残差连接（Residual Connection）。

在残差连接中，网络的输入（或特征图）与中间层的输出相加，而不是简单地将它们连接在一起。这个操作的数学表示如下：

**输出=输入+中间层的输出**

这意味着网络可以通过学习如何更正中间层的输出，而不是完全替代它们，从而更轻松地学习到残差（剩余）信息。这有助于网络更好地捕捉输入和目标之间的差异，特别是在深层网络中，因为信息可以通过跳跃连接迅速传播回网络的浅层。

Skip-connections 在一些流行的深度学习架构中广泛使用，包括：

1. **ResNet（Residual Networks）**：ResNet 是最早引入残差连接的深层网络之一，它在图像分类等任务中取得了巨大的成功。
2. **U-Net**：U-Net 是用于图像分割任务的架构，它使用跳跃连接来将低分辨率特征图与高分辨率特征图相结合，以更好地捕捉对象的细节和上下文信息。
3. **DenseNet（Densely Connected Convolutional Networks）**：DenseNet 使用密集块（Dense Blocks）和跳跃连接，使每一层都与前面的所有层连接，以促进信息流动和梯度传播。

**Downsampling**

**下采样（Downsampling）** 是一个更一般的概念，用于减小数据的分辨率或尺寸。下采样可以采用不同的方法，包括池化（Pooling）操作、平均化、子采样等。在深度学习和计算机视觉中，通常使用池化操作作为下采样的一种方式，但下采样并不仅限于池化。

下采样通常用于以下几种情况：

1. **减小分辨率**：下采样可以将图像或信号的分辨率降低到更粗糙的级别，从而减小数据的大小。这对于在资源受限的环境中存储或传输图像或信号时非常有用。
2. **特征提取**：在某些情况下，下采样可以用于提取图像或信号中的关键信息，忽略掉一些不重要的细节。这有助于降低数据的复杂性，同时保留对问题有用的信息。
3. **降低计算成本**：在深度学习和机器学习中，下采样也可以用于降低计算成本。通过在网络的早期层次中对输入图像进行下采样，可以减少后续层次的计算负担，从而提高训练速度。

下采样操作通常包括以下几种方法：

1. **平均池化（Average Pooling）**：在图像处理中，平均池化将图像的小块区域（通常是2x2或3x3的窗口）的像素值取平均，然后将结果作为新的像素值。这降低了图像的分辨率。
2. **最大池化（Max Pooling）**：最大池化也是在小块区域内操作，但它选择窗口中的最大像素值作为新的像素值，从而保留了更显著的特征，同时降低了分辨率。
3. **子采样（Subsampling）**：在信号处理中，子采样通常是通过将信号中的每隔几个采样点的值保留下来，而跳过中间的值，从而降低信号的采样率。
4. **卷积操作的步幅设置（Stride）**：在卷积神经网络中，通过将卷积操作的步幅设置为大于1的值，可以实现下采样，从而减小特征图的尺寸。

###### RNN

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/58a.png" style="zoom:50%;" />

#### 三种 Diffusion Model

###### DDPM 

**DDPM的本质作用，就是学习训练数据的分布，产出尽可能符合训练数据分布的真实图片**

[相关文章](https://zhuanlan.zhihu.com/p/650394311?utm_campaign=&utm_medium=social&utm_oi=1164654518280302592&utm_psn=1675428969745444864&utm_source=qq)

将一个图像$x_0$不断添加符合正态分布的噪声，最终形成一个符合高斯分布的噪声图像$x_T,x_t \sim N(\mu,\sigma^2)$

反向过程就是从高斯分布中随机采样一个噪声，然后去噪，生成一个新的无噪声图像。

DDPM的关键过程在于Denoising

条件概率$P(x|y) = \frac{P(x,y)}{P(y)}$,先验概率是一种特殊的条件概率，在先验概率中，y是已知的条件。由“因”求“果”。

后验概率则与先验概率相反，是通过“果”，求“因”。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/19a.png" style="zoom:67%;" />

- **变分推断**

  变分推断的主要思想是将复杂的后验分布估计问题转化为一个优化问题。它通过引入一个称为变分分布的简单参数化分布来近似真实的后验分布。然后，它通过最小化这两个分布之间的差异，通常使用KL散度作为差异度量，来找到最佳的变分分布参数。这个优化问题可以使用梯度下降等优化算法来解决。

  由于$q(x_{t-1}|x_t)$ is unknown，因此使用$p_\theta(x_{t-1}|x_t)$ 通过神经网络去近似前者。 

- 高斯分布的KL散度公式

​		对于两个单一变量的高斯分布p和q而言，它们的KL散度为：

​		$KL(p,q) = log \frac{\sigma_1}{\sigma_2} + \frac{\sigma^2 + (\mu_1 -\mu_2)^2}{2\mu_2^2} - \frac{1}{2}$

​		KL散度距离越小，代表两个变量的距离越近。

- 重参数技巧

- “从一个带参数的分布中进行采样”转变到“从一个确定的分布中进行采样”，以解决梯度无法传递问题的方法，就被称为**“重参数”**

- $N(\mu,\sigma^2) = x = \sigma * z + \mu,z \in N(0,1)$ 

  

  

  若希望从高斯分布$N(\mu,\sigma^2)$中采样，可以先从标准分布$N(0,1)$采样出z，再得到$\sigma * z + \mu$，这就是我们想要采样的结果。

  **从原始加噪到** $\beta,\alpha$**加噪，再到** $\overline\alpha$**加噪**，**使得**$q(x_t|x_{t-1})$ **转换成** $q(x_t|x_0)$**的过程**，就被称为**重参数(Reparameterization)**

- timestep编码

​		使用u-net模型来学习逆向的过程，对于时间t可以通过sin、cos编码为feature vector，放入u-net中。（因为单纯将数字作为t的话，		机器无法识别这是时间）（类似于位置编码）

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/52a.png" style="zoom:80%;" />

1. 模型输出的是一个预测的噪声。
2. 因此重参数技巧，只需要sample一次噪声$\epsilon$，就可以直接从$x_0$得到$x_t$了。因此只需要一个$x_0$和一个timestep就可以得到一个$x_t$。
3. 因为需要让原图分布q和预测后得到的分布p尽量接近，因此等价于让$\mu_\Theta \rightarrow \mu_q,\sigma_\Theta \rightarrow \sigma_q$，$\sigma_q$其实是一个常量，之和超参有关，因此在ddpm中，只预测均值。
4. 推导可得，均值于噪声$\epsilon$有关，因此对于$p_\Theta(x_{t-1}|x_t)$，只需要在denoise的过程里，预测出$\epsilon_\Theta$，使得$\epsilon_\Theta \rightarrow \epsilon$。也就是说，只要在denoise的过程中，让模型去预测噪声，就可以达到让“模型产生图片的分布”和“真实世界的图片分布”逼近的目的。
5. 当前t下损失值是与x_t有关，基于马尔科夫链x_t可以转移到由x_0和t表达。所以可以理解为记住了当前样本集下不同time step的噪声

总结：

ddpm主要集中在denoising过程，ddpm分为两个过程，前向过程和逆向过程。

前向过程：这个过程可以不通过模型，（当然也可以建立一个模型进行加噪），主要操作是，对于一个真实的图片，逐步对其添加噪声，使其最终成为一个纯噪声图片，这个纯噪声图片符合标准正态分布（？），具体为：对一个图片分步添加噪声，该噪声符合高斯分布，添加噪声的过程是一个马尔科夫链，（因为添加的噪声服从高斯分布，在训练时无法传入梯度？，所以采用重参数技巧，先将噪声从标准正态分布中采样，然后经过一个线性变换得到真正的高斯噪声，这样可以传入梯度），并且可以由$x_0$直接得到$x_t$。

逆向过程：这个过程也是一个马尔科夫链，和正向过程相反，这个过程是从纯噪声图片中逐步去噪，最终得到一个无噪声的图片，由于从$x_T \rightarrow x_{T-1}$的过程是unknown的，所以使用变分推断将复杂的后验分布估计问题转化为一个优化问题，即求$p(x_{t-1}|x_t)$。

若想要最终生成的图片与最开始真实的图片尽可能相似，需要两张图片的分布尽可以相似，从第T个timestep开始，模型的输入为 $x_t$ 与当前timestep  **。**模型中蕴含一个噪声预测器（UNet），它会根据当前的输入预测出噪声，然后，将当前图片减去预测出来的噪声，就可以得到去噪后的图片。重复这个过程，直到还原出原始图片$x_0$为止。

因为预测时需要知道当前是第几步，因此将timestep编码传入unet模型中（只传数字模型无法识别含义），高斯分布由两个参数决定（均值和方差），在ddpm中方差是一个和超参相关的常量，因此只需要使得通过神经网络预测的均值与真实的均值尽可以一样，由推导可得，均值与从高斯分布中采样的噪声有关，因此只需要神经网络预测的噪声与真实噪声尽可能一致即可，具体的可以通过损失函数进行反向传播，调整相应参数。

**参考文章**

论文：Denoising Diffusion Probabilistic Models

[【论文精读】Diffusion Model 开山之作DDPM_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1WD4y157u3/?spm_id_from=333.337.search-card.all.click&vd_source=fa01bbb0bfe8eb4919bc52e6f75d6c5a)

[深入浅出扩散模型(Diffusion Model)系列：基石DDPM（模型架构篇），最详细的DDPM架构图解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/637815071)

[深入浅出扩散模型(Diffusion Model)系列：基石DDPM（人人都能看懂的数学原理篇） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/650394311?utm_campaign=&utm_medium=social&utm_oi=1164654518280302592&utm_psn=1675428969745444864&utm_source=qq)

[扩散模型之DDPM - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/563661713)

###### SGM （Score generating model）

###### Score SDE

#### LDM

![](https://raw.githubusercontent.com/poinne/md-pic/main/63a.png)

1. 左侧红色区域是一个AutoEncoder,图像$x$通过编码器进入潜在空间进行扩散和去噪，然后通过解码器获得最终所需$\widetilde{x}$。
2. 右侧白色区域就是论文中提到的特殊领域编码器（domain specific encoder）$\tau_\theta$，可将多种不同的条件编码为中间向量$\tau_\theta(y)$，通过cross-attention控制图像生成。
3. 中间绿色区域代表潜在空间，编码后的图像向量$z$进入潜在空间进行diffusion，经过T步得到纯噪声向量$x_T$。逆向过程与常规diffusion model一样，额外添加了cross-attention层，将编码后的中间向量$\tau_\theta(y)$放入cross-attention控制图像生成，经过T步后，得到生成的向量$z$，经过解码器得到最终的生成图像$\widetilde{x}$。

**特点**

- 与普通diffusion过程不同的是，diffusion过程是在潜在表示空间（latent sapce）上进行的。可以大大减少计算复杂度。（潜在扩散模型）
- 相比于其它空间压缩方法，论文提出的方法可以生成更细致的图像。（图片感知压缩）
- 在diffusion过程引入条件机制（Conditioning Mechanisms）。在unet上使用cross-attention的方法来实现多模态训练。包括类别条件图片生成（class-condition）, 文图生成（text-to-image）, 布局条件图片生成（layout-to-image）。（条件机制）
- LDM关键就在于这三个部分：图片感知压缩、潜在扩散模型、条件机制。
  - 图片感知压缩：感知压缩本质上是一个tradeoff（图像或数据的压缩和信息保留之间的平衡），主要利用一个预训练的自编码模型，该模型能够学习到一个在感知上等同于图像空间的潜在表示空间。
  - 潜在扩散模型：在潜在扩散模型中，引入了预训练的感知压缩模型，它包括一个编码器 $\varepsilon$ 和一个解码器 $\vartheta$。这样就可以利用在训练时就可以利用编码器得到 $z_t$，从而让模型在潜在表示空间中学习。
  - 条件机制：主要是通过拓展得到一个条件时序去噪自编码器（conditional denoising autoencoder）$\epsilon_\theta(z_t,t,y)$来实现的，这样一来就可通过$y$来控制图片合成的过程。做法是在UNet主干网络上增加cross-attention机制,为了能够从多个不同的模态预处理$y$，论文引入了一个领域专用编码器（domain specific encoder）$\tau_\theta$，它用来将$y$映射为一个中间表示 $\tau_\theta(y) \in \mathbb{R}^{M \times d_\tau}$ ，这样我们就可以很方便的引入各种形态的条件（文本、类别、layout等等）。最终模型就可以通过一个cross-attention层映射将控制信息融入到UNet的中间层。
- 基于感知压缩的扩散模型的训练本质上是一个两阶段训练的过程，第一阶段需要训练一个自编码器，第二阶段才需要训练扩散模型本身。这种方法的一个优势是只需要训练一个通用的自编码模型，就可以用于不同的扩散模型的训练，在不同的任务上使用。

$\epsilon_\theta(z_t,t) -> \epsilon_\theta(z_t,t,y)$

**参考文章**

论文：High-Resolution Image Synthesis with Latent Diffusion Models

[Latent Diffusion Models论文解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/582693939)

**Q&A**

- 什么是“高频细节”？

  "高频细节" 是指图像中的那些非常小、非常细微的特征或纹理，这些特征通常对图像的整体语义或内容来说并不关键，但对于图像的质感和细节来说非常重要。例如，对于人脸图像，高频细节可能包括皮肤上的微小皱纹或毛孔的细节。

#### 概率论

**条件概率**

定义：设A、B是两个事件，且P(A) > 0,则称 $P(B|A) = \frac{P(AB)}{P(A)}$

变形可得：

$P(A) > 0时,则P(AB) = P(A)P(B|A)$

$P(B) > 0时,则P(AB) = P(b)P(A|B)$

**全概率公式**

$P(B) = \sum^n_{i=1}{P(A_i)P(B|A_i)},其中A_1\cup A_2\cup ...A_n = \Omega,即：A_1,A_2,...,A_n为完备事件组$

以n = 3 为例：$P(B) = P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)$

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/54a.png" style="zoom:50%;" />

**贝叶斯公式**

$P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\sum^n_{i=1}P(A_i)P(B|A_i)},A_1\cup A_2\cup ...A_n = \Omega,即：A_1,A_2,...,A_n为完备事件组$

通常把$P(A_1),P(A_2),...,P(A_n)$叫做**先验概率**，就是做试验前的概率，就是经验了；而把$P(A_k|B)(k=1,2,...,n)$叫做**后验概率**

[<基础系列>1：先验概率 & 后验概率 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/38567891)

**期望**

在概率论和统计学中，一个离散型随机变量的期望值（或数学期望，亦简称期望），是试验中每次可能的结果乘以其结果概率的总和。换句话说，期望值像是随机试验在同样的机会下重复多次，所有那些可能状态的平均结果，基本上等同“期望值”所期望的数。期望值可能与每一个结果都不相等。换句话说，期望值是该变量输出值的加权平均。期望值并不一定包含于其分布的值域，也并不一定等于值域平均值。

如果两个随机变量的分布相同，则它们的期望值也相同。

离散期望：$E(x)=\mu=\sum^{\infty}_{k=1}{x_kp(x_k)}$

连续期望：$E(x) = \int^{+\infty}_{-\infty}{xp(x)dx}$

**似然函数**

[似然和似然函数详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/42598338)

#### 整理

**归一化**

将数据的值压缩到0到1之间，公式如下：$\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}(线性归一化)$

从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。

目的：

消除纲量，加快收敛：不同特征往往具有不同的量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据归一化处理，以解决**数据指标之间的可比性**。原始数据经过数据归一化处理后，各指标处于[0,1]之间的小数，适合进行综合对比评价。

**标准化**

将数据缩放为均值是0，方差为1的状态,公式如下：$\frac{x_i-x_{mean}}{x_{std}}$

标准化是**通过特征的平均值和标准差，将特征缩放成一个标准的正态分布，缩放后均值为0，方差为1**。但即使数据不服从正态分布，也可以用此法。特别适用于数据的最大值和最小值未知，或存在孤立点

目的：

1. 标准化是为了方便数据的下一步处理，而进行的数据缩放等变换，不同于归一化，**并不是为了方便与其他数据一同处理或比较**。
2. 标准化后的变量值围绕0上下波动，大于0说明高于平均水平，小于0说明低于平均水平。

**KL divergence**

KL散度的理论意义在于度量两个概率分布之间的差异程度，当KL散度越大的时候，说明两者的差异程度越大；而当KL散度小的时候，则说明两者的差异程度小。如果两者相同的话，则该KL散度应该为0。（注意：不是衡量两个分布的距离）。

连续随机变量：

设定两个概率分布分别为P和Q，在设定为连续随机变量的前提下，他们对应的概率密度函数分别是$p(x)$和$q(x)$。如果我们用$q(x)$去近似$p(x)$，则KL散度可以表示为：
$KL(P||Q) = \int p(x)(logp(x)-logq(x))dx$，即$KL(P||Q) = \int p(x)\frac{p(x)}{q(x)}dx$

离散随机变量：

$KL(P||Q) = \sum p(x)\frac{p(x)}{q(x)}$

展开可得：

$KL(P||Q) = \sum p(x)\frac{p(x)}{q(x)} = -\sum p(x)log{q(x)}+\sum p(x)log(log(p(x))) = H(P,Q) - H(P)$

最后得到的第一项称作P和Q的交叉熵（cross entropy），后面一项就是分布P熵。所以KL散度在信息论中还可以称为相对熵（relative entropy）

**期望形式：**概率分布P和分布Q之间的KL散度还可以表示为两个概率密度函数p和q之间对数差的期望。假设随机变量x为概率分布函数P的一个概率值，$\mathbb{E}$为期望，那么KL公式还可如下定义:
$D_{KL}(P||Q)=\mathbb{E}_{x \sim P(x)}[log\frac{p(x)}{q(x)}]$

**特点**

- KL散度具备非负性，即$KL(p||Q)>=0$

- KL散度不具备对称性，也就是说P对于Q的KL散度并不等于Q对于P的KL散度。因此，KL散度并不是一个度量（metric），即KL散度并非距离。
- 当KL散度的取值位于$(0,\infty)$，越接近于0，说明分布P和分布Q越匹配。



VI的主要内容懒得写了，具体看下面的参考文章。

**参考文章**

[【文生图系列】基础篇-变分推理（数学推导）_马鹤宁的博客-CSDN博客](https://blog.csdn.net/weixin_42111770/article/details/131272727#:~:text=变分推理，是机器学习中一种流行的方式，使用优化的技术估计复杂概率密度。 变分推理的工作原理： 首先选择一系列概率密度函数，然后采用KL散度作为优化度量找到最接近于概率密度的函数,。 引入evidence lower bound的方法更容易计算近似概率。)

#### 模型

###### Diffusion Model

[视频](https://www.bilibili.com/video/BV1tz4y1h7q1/?spm_id_from=333.337.search-card.all.click&vd_source=fa01bbb0bfe8eb4919bc52e6f75d6c5a)

正态分布：又叫做高斯分布，可以记为$N(\mu,\sigma^2)$,(因为通过这两个数就可以确定一个正态分布)。

贝叶斯公式：

**前向过程**(由原图$x_0$，通过加噪得到一个纯噪声图片$x_T$)

1. 对于一个样例图片A，首先生成一个与A尺寸和格式相同且服从正态分布的噪声图片B。

2. 通过将图片A与图片B组合对A添加噪声<img src="https://raw.githubusercontent.com/poinne/md-pic/main/18a.png" style="zoom:33%;" />

3. 使用上述公式，不断迭代，添加噪声

   <img src="https://raw.githubusercontent.com/poinne/md-pic/main/53a.png" style="zoom:33%;" />

4. 由于每次添加噪声后的图像都与前一次的图像有关，因此可以直接从$X_0$(无噪声)推导出$X_T$(第T次加噪)的结果，公式如下:

   <img src="https://raw.githubusercontent.com/poinne/md-pic/main/51a.png" style="zoom:50%;" />

5. 因此可以得出：对于任意时刻$x_t$的图像，都可以认为是从$x_0$直接加噪得来的。

**反向过程**(由前向过程最后得到的噪声图$x_T$得到最初的原图$x_0$)

1. 

扩散模型包括两个过程：前向过程（forward process）和反向过程（reverse process），其中前向过程又称为扩散过程（diffusion process）。无论是前向过程还是反向过程都是一个参数化的马尔可夫链（Markov chain），其中反向过程可用于生成数据样本（它的作用类似GAN中的生成器，只不过GAN生成器会有维度变化，而DDPM的反向过程没有维度变化）

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/61a.png" style="zoom:67%;" />

首先

###### Stable Diffusion

对于输入的文字，首先将其转换为token embeddings向量（通过clip），然后将该embeddings向量与一个初始化的多维数组组成的噪声作为输入，在信息（潜在）空间中进行扩散过程（unet），输出经过处理的信息数组，最后将该信息数组绘制为最终图像（通过autoencoder）

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/62a.png" style="zoom:67%;" />

通常来说，文生图模型遵循以下公式：

- **Text Encoder:** 一个能对输入文字做语义解析的Encoder，一般是一个预训练好的模型。在实际应用中，CLIP模型由于在训练过程中采用了图像和文字的对比学习，使得学得的文字特征对图像更加具有鲁棒性，因此它的text encoder常被直接用来做文生图模型的text encoder（比如DALLE2）
- **Generation Model**： 输入为文字token和图片噪声，输出为一个关于图片的压缩产物（latent space）。这里通常指的就是**扩散模型**，采用文字作为引导（guidance）的扩散模型原理。
- **Decoder：**用图片的中间产物作为输入，产出最终的图片。Decoder的选择也有很多，同样也能用一个扩散模型作为Decoder。

###### U-net

Unet属于Encoder-Decoder结构，结构简单但很有效。

Encoder负责特征提取，通过卷积、池化、特征融合等操作，得到feature map。

feature map 经过 Decoder 恢复原始分辨率，该过程除了卷积比较关键的步骤就是 **upsampling** 与 **skip-connection**。

- **upsampling** : 上采样常用的方式有两种：1.**[FCN](https://zhuanlan.zhihu.com/p/77201674)** **中介绍的反卷积**；2. **插值**。上采样一般采用反卷积会比直接双线性插值来的效果要好，当然如果模型处于过拟合的状态下应用反卷积就会起得适得其反的作用。
- **skip-connection** : **将下采样各个部分得到的特征图与上采样对应部分进行结合**，很好地解决了由于下采样操作所丢失掉的细节损失（比如边界信息，这对于语义分割这种dense 预测型任务来说是至关重要的），从而帮助网络更好的完成精确的定位

**FCN 中深层信息与浅层信息融合是通过对应像素相加的方式，而 Unet 是通过拼接的方式。**

**feature map 的维度没有变化，但每个维度都包含了更多特征，对于普通的分类任务这种不需要从 feature map 复原到原始分辨率的任务来说，这是一个高效的选择；而拼接则保留了更多的维度/位置 信息，这使得后面的 layer 可以在浅层特征与深层特征自由选择，这对语义分割任务来说更有优势。**

[UNet结构及代码实现](https://zhuanlan.zhihu.com/p/142985678)（文中特征融合部分写错了，不是padding，而是裁剪）

[skip connection](https://blog.csdn.net/weixin_43135178/article/details/119976995)

###### AutoEncoder

Autoencoder（自编码器）是一种无监督学习的神经网络模型，其主要作用和意义包括以下几个方面：

1. **降维和特征学习**：
   - Autoencoder 可以用于数据的降维，将高维数据映射到低维空间。通过训练，它可以学习到数据的最重要的特征和结构，从而提取有用的特征表示。
2. **去噪和数据复原**：
   - Autoencoder 可以用于去除数据中的噪音，因为它在学习编码和解码的过程中会尽量还原原始数据。这使得它在数据预处理中很有用，尤其是在图像去噪和信号处理方面。
3. **特征提取**：
   - Autoencoder 可以用作前期特征提取的工具。通过训练，它可以从原始数据中提取有用的特征，这些特征可以用于其他机器学习任务，如分类、聚类等。
4. **图像生成和重建**：
   - Autoencoder 可以用于图像生成，通过学习数据的概率分布来生成新的图像样本。它在生成对抗网络（GAN）的前身中扮演了重要角色。
   - 通过训练一个Autoencoder来重建输入数据，可以检查模型对数据的重构性能，这对于异常检测和数据完整性验证非常有用。
5. **无监督学习**：
   - Autoencoder 是**无监督学习**的一个例子，它不需要标记的训练数据。这使得它适用于大多数真实世界的数据，尤其是在没有大规模标记数据的情况下。
6. **数据压缩**：
   - Autoencoder 可以用于数据的压缩和编码，这对于存储和传输数据时非常有用。

###### VAE

VAE（Variational Autoencoder 变分自编码器），是一种无监督学习的神经网络模型，结合了**自编码器**（Autoencoder）和**概率图模型**的思想。它的主要目标是学习数据的潜在表示，并且能够生成新的数据样本，同时还能估计数据的分布。[参考文章](https://blog.csdn.net/smileyan9/article/details/107362252)

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/36a.png" style="zoom:80%;" />

一幅图像经过AutoEncoder得到的只对应code空间上的一个点，如下图的满月和半月，无法通过采样两点中间的点得到3/4月。实际的结果是，生成图片是模糊且无法辨认的乱码图。一个比较合理的解释是，因为编码和解码的过程使用了深度神经网络，这是一个非线性的变换过程，所以在code空间上点与点之间的迁移是非常没有规律的。

而图像经过VAE得到的对应code空间中的一个分布，当然，分布在原本点处的概率最大，在这种情况下，图像的编码就由原先离散的编码点变成了一条连续的编码分布曲线。此时再采样中间的点，就会得到类似3/4月。

![](https://raw.githubusercontent.com/poinne/md-pic/main/50a.png)

![](https://raw.githubusercontent.com/poinne/md-pic/main/49a.png)

VAE 中的隐变量z的生成过程就是一个**变分过程**，我们希望用简单的z来映射复杂的分布，这既是一个降维的过程，同时也是一个变分推断的过程。

VAE的损失函数除了要最小化重建图像与原图像之间的均方误差外，还要最大化每个分布和标准正态分布之间的相似度。	

生成模型，其实就是假设$z$来自标准正态分布，想要拟合分布$P(x|z)$（解码器），以得到$x$的分布（图像分布）。为了训练解码器，自编码器架构使用了一个编码器以描述$P(z|x)$。这样，从训练集里采样，等于是采样出了一个$x$。根据$P(z|x)$求出一个$z$，再根据$P(x|z)$试图重建$x$。优化这个过程，就是在优化编码器和解码器，也就是优化$P(z|x)$和$P(x|z)$。

**变分推断** （Variational Inference）（VI）

VI就是一套数学方法，用来让两个概率分布逼近，并且计算简单。

变分推断的主要思想是将复杂的后验分布估计问题转化为一个优化问题。它通过引入一个称为变分分布的简单参数化分布来近似真实的后验分布。然后，它通过最小化这两个分布之间的差异，通常使用KL散度作为差异度量，来找到最佳的变分分布参数。这个优化问题可以使用梯度下降等优化算法来解决。



VAE 模型能够通过隐变量来捕获输入数据中一些隐藏的特征，并且我们利用这些特征生成与输入数据相关但是又不相同的数据，AE 模型只是编码解码，完全不能实现这个功能。

**VAE与AE**

- VAE 中隐藏层服从高斯分布，AE 中的隐藏层无分布要求
- 训练时，AE 训练得到 Encoder 和 Decoder 模型，而 VAE 除了得到这两个模型，还获得了隐藏层的分布模型（即高斯分布的均值与方差）
- AE 只能重构输入数据X，而 VAE 可以生成含有输入数据某些特征与参数的新数据。
- VAE 与 AE 完全不同，但是从结构上看都含有 Decoder 和 Encoder 过程。

**总结**
VAE其实就是一个编码器-解码器架构，和U-Net以及部分NLP模型类似。然而，为了抑制自编码过程中的过拟合，VAE编码器的输出是一个正态分布，而不是一个具体的编码。同时，VAE的损失函数除了约束重建图像外，还约束了生成的分布。在这些改进下，VAE能够顺利地训练出一个解码器，以把来自正态分布的随机变量$z$画成一幅图像。

**VAE 的关键组成部分和工作原理：**

1. **编码器（Encoder）**：编码器将输入数据映射到潜在空间中的概率分布。通常，这个分布是一个多维高斯分布，其中均值和方差参数是编码器的输出。编码器的任务是学习如何最好地将输入数据编码成潜在空间中的表示。
2. **潜在空间（Latent Space）**：潜在空间是一个低维空间，其中每个点都对应于一个潜在表示。这些潜在表示捕捉了数据的关键特征和结构。
3. **解码器（Decoder）**：解码器将从潜在空间中采样的点映射回原始数据空间，尝试还原输入数据。解码器的输出是生成的数据样本。
4. **重参数化技巧（Reparameterization Trick）**：为了让模型可微分并且可以使用梯度下降进行训练，VAE 使用了一个重要的技巧，称为重参数化技巧。这个技巧允许从潜在空间中采样，同时保持梯度可导。
5. **损失函数**：VAE 的损失函数通常由两部分组成：一个是重建损失**（loss)**，用于衡量输入数据与解码器重建的数据之间的差异，另一个是正则化项**（reqular）**，用于鼓励潜在表示遵循预定的分布，通常是标准正态分布。这两个部分的组合构成了最终的损失函数。

VAE 的训练目标是最小化损失函数，以便编码器能够学习将输入数据映射到具有有用特征的潜在表示，同时解码器能够生成具有多样性的新数据样本。这使得 VAE 被广泛用于生成模型、数据降维、图像生成、文本生成和异常检测等任务。其能力在生成样本方面与生成对抗网络（GAN）类似，但它还提供了对潜在表示的建模和估计。

Variational Autoencoder（VAE）相对于普通的Autoencoder（AE）具有一些显著的优势，尤其是在生成模型和潜在空间建模方面：

1. **生成样本的能力**：
   - VAE 是一种生成模型，可以从潜在空间中采样，因此可以生成新的数据样本。这使得它在生成图像、文本和其他数据类型方面非常有用。
2. **潜在表示的连续性**：
   - VAE 通常具有连续的潜在表示。这意味着在潜在空间中可以进行插值和随机采样，从而生成具有多样性的新样本。这对于生成高质量的数据样本非常有帮助。
3. **概率性建模**：
   - VAE 建模了数据的概率分布，尤其是在潜在空间中。这使得它能够提供更加丰富的数据生成模型，包括估计数据的不确定性。
4. **正则化潜在空间**：
   - VAE 引入了正则化项，鼓励潜在表示遵循预定义的分布，通常是标准正态分布。这可以帮助更好地控制潜在空间的结构，使其更容易进行插值和采样。
5. **更好的概率推断**：
   - VAE 使用变分推断方法，更好地估计潜在变量的后验分布。这使得它对于估计不确定性和生成样本更具能力。
6. **应用领域**：
   - VAE 在生成模型、图像生成、自然语言处理（NLP）中的应用更为广泛。它在生成对抗网络（GAN）之前是生成图像的主要方法之一。

VAE模型通常用于以下几个主要应用领域：

1. **生成模型**：
   - VAE 可以用于生成数据，如图像、文本和音频。通过学习数据的概率分布，VAE 能够生成具有多样性的新样本，这在艺术创作、图像生成和自然语言生成等方面非常有用。
2. **数据压缩和重建**：
   - VAE 可以用于数据的压缩和编码，将高维数据编码成低维的潜在表示，然后再解码以还原原始数据。这对于数据压缩和重建、图像去噪和数据恢复等任务非常有用。
3. **特征学习**：
   - VAE 可以用作特征学习的工具，它可以从数据中提取有用的特征表示。这些特征表示可以用于其他机器学习任务，如分类、聚类和异常检测。
4. **无监督学习**：
   - VAE 是一种无监督学习方法，它可以在没有标记的训练数据的情况下学习数据的结构和表示。这对于探索和理解数据非常有帮助。
5. **图像重建和编辑**：
   - VAE 可以用于图像重建，通过在潜在空间中修改表示来编辑图像。这对于图像修复、风格迁移和图像合成等任务非常有用。
6. **异常检测**：
   - VAE 可以用于检测数据中的异常或异常模式。它可以学习正常数据的潜在表示，并通过比较新数据点的重建误差来检测异常。
7. **生成对抗网络（GAN）前身**：
   - 在生成对抗网络（GAN）兴起之前，VAE 是生成图像的主要方法之一。它为生成图像提供了一种有效的方法，尤其是在数据生成和图像合成方面。

**参考文章**

论文：[Auto-encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)

[[论文简析\]VAE: Auto-encoding Variational Bayes](https://www.bilibili.com/video/BV1q64y1y7J2/?spm_id_from=333.337.search-card.all.click&vd_source=fa01bbb0bfe8eb4919bc52e6f75d6c5a)

[Up-Fei的个人空间-Up-Fei](https://space.bilibili.com/388570355/)

[一文理解变分自编码器（VAE）](https://zhuanlan.zhihu.com/p/64485020)

[【15分钟】了解变分自编码器](https://www.bilibili.com/video/BV1Ns4y1J7tK/?spm_id_from=333.337.search-card.all.click&vd_source=fa01bbb0bfe8eb4919bc52e6f75d6c5a)

[【学习笔记】生成模型——变分自编码器 ](https://www.gwylab.com/note-vae.html)

[抛开数学，轻松学懂 VAE（附 PyTorch 实现） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/574208925)

[生成模型之VAE - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/452743042)

###### GAN

以前的模型是通过数据来学习数据的分布参数，确定最终 的分布。而GAN是通过模型得到分布来近似这个分布，好处是减少了计算量，坏处是不知道最终的分布是什么样子。

**参考文章**

[GAN论文逐段精读【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1rb4y187vD/?spm_id_from=333.337.search-card.all.click&vd_source=fa01bbb0bfe8eb4919bc52e6f75d6c5a)

###### Attention

- [如何理解attention中的Q、K、V](https://www.zhihu.com/question/298810062/answer/2274132657)

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/35a.png" style="zoom:50%;" />

attention的基本结构如上图所示，假设输入的向量为X，通过矩阵Q和矩阵K得到两个新的向量，记为Q向量和K向量，然后向量Q和K相乘得到一个新的向量Score，代表向量Q与向量K之间的相似度关系，然后通过一个归一化操作（softmax）后与（通过矩阵V进行线性变换之后的新向量V）相乘，得到新的向量Z（也就是下图中的$V'$）。

因为一个Q向量会分别与全部n个K向量做内积，因此会得到n个Score，每个Score通过softmax后得到一个概率值$p_i$，查询的Q与K相似度越高，则$p_i$越大。所有概率值之和为1，最终的$Z=p_1*V_1+p_2*V_2+...+p_n*V_n$。

上面是对一个Q做的运算，实际上可以同时查询多个Q，组成一个Q矩阵，然后与K矩阵进行运算。因此attention操作是**可以并行**的。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/32a.png" style="zoom:80%;" />

###### self-Attention

![](https://raw.githubusercontent.com/poinne/md-pic/main/40a.png)

self-Attention 基于Attention机制，不同点在于Q、K、V矩阵相乘的变量都是同一个（因为关注的是同一个序列），其余的部分同attention。

###### cross-Attention

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/64a.png" style="zoom:80%;" />

self-Attention 同样基于Attention机制，不同点在于K、V矩阵相乘的变量是同一个，其余的部分同attention。

###### self-Attention与cross-Attention比较

**相同点：**

1. **机制**：两者都使用了点积注意力机制（scaled dot-product attention）来计算注意力权重。
2. **参数**：无论是自注意力还是交叉注意力，它们都有查询（Query）、键（Key）和值（Value）的概念。
3. **计算**：两者都使用查询和键之间的点积，然后应用softmax函数来计算注意力权重。
4. **输出**：在计算完注意力权重后，两者都将这些权重应用于值来得到输出。
5. **可变性**：两者都可以通过掩码（masking）来控制某些位置不被其他位置关注

**不同点：**

**Self Attention:** 查询、键和值都来自同一个输入序列。这使得模型能够关注输入序列中的其他部分以产生一个位置的输出。主要目的是捕捉**输入序列内部的依赖关系**。在Transformer的编码器（Encoder）和解码器（Decoder）的每一层都有自注意力。它允许输入序列的每个部分关注序列中的其他部分。

**Cross Attention:** 查询来自一个输入序列，而键和值来自另一个输入序列。这在诸如序列到序列模型（如机器翻译）中很常见，其中一个序列需要“关注”另一个序列。目的是使一个序列能够关注另一个不同的序列。主要出现在Transformer的解码器。它**允许解码器关注编码器的输出**，这在机器翻译等任务中尤为重要。

###### Masked Self Attention

<img src="D:\AppData\Roaming\Typora\typora-user-images\image-20230911201412228a.png" alt="image-20230911201412228" style="zoom:50%;" />

假设在此之前我们已经通过 scale 之前的步骤得到了一个 attention map，**而 mask 就是沿着对角线把灰色的区域用0覆盖掉，不给模型看到未来的信息**，如下图所示：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/65a.png" style="zoom:50%;" />

详细来说：

1. "i" 作为第一个单词，只能有和 "i" 自己的 attention；
2. "have" 作为第二个单词，有和 "i、have" 前面两个单词的 attention；
3. "a" 作为第三个单词，有和 "i、have、a" 前面三个单词的 attention；
4. "dream" 作为最后一个单词，才有对整个句子 4 个单词的 attention。

并且在做完 softmax 之后，横轴结果合为 1。如下图所示：

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/38a.png" style="zoom:50%;" />

###### Multi-head Self Attention

是Attention机制的完善。不过从形式上看，它其实就再简单不过了，就是把Q,K,V通过**参数矩阵**映射一下，然后再做Attention，把这个过程重复做h次，结果拼接起来就行了，可谓“大道至简”了。具体来说

![](https://raw.githubusercontent.com/poinne/md-pic/main/39a.png)

![](https://raw.githubusercontent.com/poinne/md-pic/main/42a.png)

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/34a.png" style="zoom:50%;" />

![](https://raw.githubusercontent.com/poinne/md-pic/main/41a.png)

**多头相当于把原始信息 Source 放入了多个子空间中，也就是捕捉了多个信息，对于使用 multi-head（多头） attention 的简单回答就是，多头保证了 attention 可以注意到不同子空间的信息，捕捉到更加丰富的特征信息**。其实本质上是论文原作者发现这样效果确实好。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/46a.png" style="zoom:80%;" />

**所谓“多头”（Multi-Head），就是只多做几次同样的事情（参数不共享），然后把结果拼接**。

Multi-Head 指的是 Multi-Head Attention 中的多个头，也就是说，Multi-Head Attention 将注意力机制拆分成**多个头**，每个头都可以关注输入序列的**不同部分**，并计算出**不同的加权和**。

 在 **Multi-Head Attention** 中，每个头都是一个**独立的注意力机制**，它们可以关注不同的输入信息，从而提高模型的表征能力和[泛化能力](https://www.zhihu.com/search?q=泛化能力&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3025982464})。同时，通过**拆分头数**，我们可以控制模型的**计算复杂度**，从而在保证模型性能的同时，节约计算资源。

**e.g.** 如果不是多头的注意力机制，it 和 the animal 是相关度最高的，这符合我们的预期。但根据句子中 it was too tired 可知，it 除了指代 the animal 还是 tired 的。如果再引入一个 attention layer，这个 layer 就可能捕获 it 与 tired 的相关度。

###### Transformer

可以并行 （rnn不能并行）

多头 -》模拟卷积多通道的输出

**整体结构**

 Transformer 由 Encoder 和 Decoder 两个部分组成，Encoder 和 Decoder 都包含 6 个 block。Transformer 的工作流程大体如下：

- 第一步：获取输入句子的**每一个单词**的**表示向量** X
  - (“我” -> "21,34,121,545,121,3,11,2,0,34")
  - X由单词的 **Embedding**（Embedding就是从原始数据提取出来的**Feature**） 和单词位置的 Embedding 相加得到。得到综合embedding。
- 第二步：将得到的**单词表示向量矩阵** 传入 **Encoder**
  - ("21,34,121,545,121,3,11,2,0,34" -> encoder -> "31,54,1,5,341,33,121,23,23,0" -> encoder -> ... -> "23,112,43,232,54,23,2,32" )[最开始输入的是经过编码的向量，中间经过多个encoder block，每个encoder block输出的也是一个经过处理的向量]
  - 每一个 **Encoder block** 输出的**矩阵维度**与**输入 完全一致**。
- 第三步：将 Encoder 输出的编码信息矩阵 C传递到 Decoder 中 
  - Decoder 依次会根据**当前已经翻译过的单词**  i（I） **翻译下一个单词** i+1（**have**），如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作**遮盖住 i+1 之后的单词**。
  - 输入**来自上一个decoder block的输出**（或来自encoder的输出，对于第一个decoder block），以及来自encoder的输出。这两部分输入会被加权结合，通常通过自注意力机制（self-attention）来完成，以便模型可以在生成输出时考虑上下文信息和序列中其他位置的内容。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/44a.png" style="zoom:50%;" />

**Encoder结构**

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/45a.png" style="zoom:33%;" />

Encoder由三个主要部分组成：（Multi-Head Attention、Feed Forward、Add&Norm)

- Multi-Head Attention：多头注意力机制，其是由多个 Self-Attention 并行组成的，每个头的 Self-Attention 操作都有不同的权重矩阵，因此可以捕获输入序列中不同位置的关系和特征，而不会简单地将它们相加。

- **Feed Forward：**（可以看作是一个MLP）

  - Feed Forward 层比较简单，是一个**两层**的**[全连接层](https://www.zhihu.com/search?q=全连接层&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3025982464})**，第一层的[激活函数](https://www.zhihu.com/search?q=激活函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3025982464})为 **Relu**，第二层不使用激活函数，对应的公式如下：

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/43a.png" style="zoom:33%;" />

  - X是输入，Feed Forward 最终得到的输出矩阵的维度与X一致。

- **Add & Norm：**

  - Add: Add指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到，现在基本上已经成为了深层网络的标配：

    <img src="https://raw.githubusercontent.com/poinne/md-pic/main/22a.png" style="zoom:50%;" />

  - Norm指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。

  - "Add & Norm" 的操作使得 Transformer 模型能够更好地捕捉输入序列中的信息，同时确保输出的分布在训练过程中保持稳定，有助于提高模型的性能和训练效率。这一操作在每个子层中都被广泛使用，以构建深度的 Transformer 模型。

**Dencoder结构**

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/47a.png" style="zoom:33%;" />

与Encoder block不同之处在于：

- 包含两个 Multi-Head Attention 层。

- 第一个 Multi-Head Attention 层采用了 **Masked** 操作。

- 第二个 Multi-Head Attention 层的K、V矩阵使用 Encoder 的**编码信息矩阵C**进行计算，而Q使用上一个 Decoder block 的输出计算。[注意]

- 最后有一个 Softmax 层计算下一个翻译单词的**概率**。

注意，Deocder块中的第一个注意力层关联到Decoder的所有（过去的）输入，但是第二注意力层使用E的输出。因此，它可以访问整个输入句子，以最好地预测当前单词。这是非常有用的，因为不同的语言可以有语法规则将单词按不同的顺序排列，或者句子后面提供的一些上下文可能有助于确定给定单词的最佳翻译。[视频参考](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DugWDIIOHtPA%26list%3DPLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4%26index%3D62)

**位置Embedding**

因为Transformer没有采用RNN的结构，而是使用全局信息，不能利用单词的**顺序变化**，所以在transformer中使用**位置Embedding**保存单词在序列中的相对或绝对位置。

位置 Embedding 用 PE （Position Encoding）表示，PE 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。

<img src="https://raw.githubusercontent.com/poinne/md-pic/main/48a.png" style="zoom:50%;" />

- 维度一共是512维，在偶数位置0，2，4，6.....使用的[sin](https://www.zhihu.com/search?q=sin&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3025982464})，在奇数位置1，3，5，7.....使用的是cos。

- 使用这种公式计算 PE 有以下的好处：   使 PE 能够适应比训练集里面所有句子**更长的句子** 

- 假设训练集里面最长的句子是有 **20 个单词**，突然来了一个长度为 **21** 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding。    可以让模型容易地计算出**相对位置**，

- 对于固定长度的间距 k，PE(pos+k) 可以用 PE(pos) 计算得到。因为 Sin(A+B) = Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) = Cos(A)Cos(B) - Sin(A)Sin(B)。直接把相对位置转换为简单的[三角函数](https://www.zhihu.com/search?q=三角函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A3025982464})运算了

**Q&A**

- **多头注意力维度问题**

  论文中解码器的输出维度是固定的512维，如果只使用一个attention的话，正常算就可以了，输出就是512维。但论文中使用的是多头注意力，则需要将不同的Q、K、V通过一个可学习的$W^Q_i,W^K_i,W^V_i$投影到一个低维的向量上，再做attention操作。最终再将多个Z向量concat，得到最终的向量Z。例如论文中是8个注意力头，则投影是512/8 = 64维，再算attention function，通过一系列操作得到每个对应的Z向量（64维），然后通过concat得到512维的Z向量。

- **mask的作用？**
  - **Padding Mask：** Padding mask用于处理输入序列的不定长问题。当输入序列的长度不一致，通常需要通过在较短序列的末尾添加填充（padding）标记（通常是0）来对齐长度。Padding mask将这些填充标记的位置标记为1，而其他位置标记为0。这样，在计算自注意力时，模型会忽略填充标记的位置，不考虑填充标记与其他词之间的注意力关系。
  - **Look-Ahead Mask（解码器中）：** Look-Ahead Mask通常用于解码器（decoder）中，以确保在生成目标序列时不会查看未来的信息。它通过将未来位置的位置标记为1，而将当前及过去位置的位置标记为0来实现。这样，解码器只能看到已生成的部分序列，以便按照正确的顺序生成输出。
- **encoder为什么有多个block，一个不就可以完成对单词的编码了吗？**
  - encoder包含多个block的设计是为了增加模型的表示能力和泛化能力。虽然单个block可以完成对单词的编码，但使用多个block可以使模型更深、更复杂，从而更好地捕捉输入序列中的复杂关系和特征。具体的好处如下：
  - **增加表示能力：** 每个encoder block都包含多头自注意力机制和前馈神经网络，这些层可以学习不同的特征表示。通过堆叠多个block，模型可以逐渐构建更高层次的抽象特征，从而提高了模型的表示能力。
  - **更好的泛化：** 多个encoder block可以帮助模型更好地泛化到不同的输入序列。每个block都可以学习捕捉不同长度的依赖关系，因此多个block的组合可以处理长距离的依赖和复杂的序列结构。
  - **建模不同层次的信息：** 多个block允许模型在不同的抽象层次上建模信息。较浅的block可以捕获局部特征和近距离依赖，而较深的block可以处理更全局的信息和远距离依赖。
  - **提高鲁棒性：** 多个block可以提高模型的鲁棒性，使其能够更好地处理各种输入数据，不受噪声或干扰的影响。
- **在具体的代码实现中，内容 Embedding 和位置 Embedding 是直接相加吗？**
  
  - 是直接相加（按元素相加/或者说是按位相加）
- **Multi-Head Attention多头注意力机制，其是由多个 Self-Attention 并行组成的，这句话是不是说多头注意力机制就是简单的将几个self-attention的结果相加？**
  - 如果不使用Multi-Head，那么就直接是多个查询Q向量与K向量相乘（矩阵乘法），使用Multi-Head需要先做一个维度的降低，因此需要使用$W_q,W_k,W_v$矩阵进行线性变换，然后再在降维后的空间做attention操作。最终将多个head结果concat，得到原维度的向量。

  - 多头注意力机制的操作如下：

    1. 将输入序列分别通过多个不同的线性变换（通常是矩阵乘法）来生成多个不同的查询（Query）、键（Key）和值（Value）向量。

    2. 对每个生成的查询、键和值向量进行 Self-Attention 操作，得到多个注意力分布。每个 Self-Attention 操作使用不同的权重矩阵（注意力矩阵）来计算注意力分布。

    3. 将多个注意力分布相加，生成最终的多头注意力输出。

    4. 最后，将多头注意力输出通过另一个线性变换来获得最终的输出。

  - 关键点在于，每个头的 Self-Attention 操作都有不同的权重矩阵，因此可以捕获输入序列中不同位置的关系和特征，而不会简单地将它们相加。多头注意力允许模型在不同的注意力机制下学习不同的表示，从而提高了模型的表达能力。

  - 因此，多头注意力机制是一种并行化的注意力机制，它通过多个头（子注意力机制）来处理输入序列，以获得更丰富和更复杂的表示。它并不是简单地将几个 Self-Attention 的结果相加，而是在多个子注意力机制之间进行组合和加权，以获得最终的输出。
- **transformer里面decoder的理解**
  
  - [对NLP中transformer里面decoder的理解_nlp decoder_xiav的博客-CSDN博客](https://blog.csdn.net/qq_26628975/article/details/115331368#:~:text=decoder的输入是模型的前期翻译结果，也就是拿模型的前期翻译结果做一个Q去和encoder编码的全局语义信息生成的K、V做attention，相当于去找现有结果在全局语义信息里面的相关性，根据这个相关性去决定下一个翻译结果（比如翻译到“他是一个”的时候，去encoder给的信息里面算attention，发现接下来需要翻译的词跟“wears a,red t-shirt”相关性很强，那么就给出翻译“穿红色T恤”，接下来才是翻译“person”，也就是“的人”）。)
