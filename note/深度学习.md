# 深度学习

课程主页：https://courses.d2l.ai/zh-v2/ 教材：https://zh-v2.d2l.ai/

- 损失为什么要求平均？

  其实求不求平均都可以，求平均的话梯度不会那么大 ，求平均的话最终的梯度和样本数大小无关。如果不求平均，那么需要将学习率/n（求平均）

- 线性回归损失函数通常都是mse

- 如何找合适的学习率？

  - 找一个对学习率不敏感的算法
  - 通过方法快速找合适的学习率（未看）

- batchsize是否会影响最终模型结果？

  batchsize小的话效果更好，收敛更快。

- 使用yeild生成器生成数据有什么优势？相比return？

  使用生成器每次只会生成一部分，需要的时候再生成，可以节省内存。python中一般都是用yeild而不是return

- 如果样本大小不是批量数的整数倍，最后一部分样本怎么办？
  - 直接用
  - 把最后这部分丢掉
  - 从别的地方采样一些样本，将这个批次补全

- 回归VS分类
  - 回归估计一个连续值
  - 分类预测一个离散类别
- 损失函数
  - L2 Loss （也就是均方损失）
  - L1 Loss 
  - HUber's Robust Loss  ( if $|y - y'| > 1 is L1-Loss , else is L2- Loss$ )

- 使用多个cpu读取硬盘上的数据

  ```py
  def get_dataloader_workers():  #@save
      """使用4个进程来读取数据"""
      return 4
  
  train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                               num_workers=get_dataloader_workers())
                               
  '''
  这里的d2l是自定义的一个库，对pytorch的封装
  '''
  timer = d2l.Timer()
  for X, y in train_iter:
      continue
  f'{timer.stop():.2f} sec'
  ```

- 设X是一个矩阵，X.sum(0,keepdim=True)
  - 0:对列求和，相当于将矩阵从上到下拍平。
  - 1：对行求和，相当于将矩阵从左到右拍平。
  - keepdim = True：表示保留维度：
  - 若为False：[1,2]
  - 若为True：[[1,2]]
- 最小化损失 = 最大化似然函数
- 训练误差：模型在训练数据上的误差、泛化误差：模型在新数据上的误差
<<<<<<< HEAD
- <img src="https://raw.githubusercontent.com/poinne/md-pic/main/4b.png" style="zoom:67%;" />

- <img src="https://raw.githubusercontent.com/poinne/md-pic/main/5b.png" style="zoom:67%;" />

- 过拟合和欠拟合

  ![](https://raw.githubusercontent.com/poinne/md-pic/main/6b.png)
=======
- <img src="https://raw.githubusercontent.com/poinne/md-pic/main/4b.png" style="zoom:67%;" />

- <img src="https://raw.githubusercontent.com/poinne/md-pic/main/5b.png" style="zoom:67%;" />

- 过拟合和欠拟合

  ![](https://raw.githubusercontent.com/poinne/md-pic/main/6b.png)
>>>>>>> b90bd3d (实验室提交)

- 模型容量需要匹配数据复杂度，否则可能导致欠拟合和过拟合。
- 统计机器学习提供数学工具来衡量模型复杂度
- 实际中一般靠观察训练误差和验证误差
- 一般数据集需要划分为训练、测试、验证三个数据集。
  - 训练数据集用来训练模型的权重。
  - 验证数据集用来训练模型的超参数（如果数据集不大的话，可以用K-则交叉验证方法，但是这个训练成本大，所以大的数据集不要用）
  - 测试数据集用来判断模型训练的效果（只能用一次）

- K-交叉验证（K-fold cross-validation）是一种在机器学习和统计学中常用的模型评估技术。它用于评估模型的性能和泛化能力，特别是在数据集相对较小的情况下。K-交叉验证的基本思想是将原始数据分成K个子样本（通常是K等分），然后将模型训练K次，每次使用其中K-1个子样本作为训练数据，而留出的一个子样本用于测试。这个过程重复K次，每个子样本都会充当一次测试集，其他K-1个子样本作为训练集。

- **权重衰减**（Weight Decay）是一种常用于机器学习和神经网络训练的正则化技术。它的主要目的是防止模型过拟合训练数据，提高模型的泛化能力。权重衰减通过在模型的损失函数中增加一个附加项来实现，这个附加项惩罚了模型中的权重参数，使其不过分大，从而降低了模型的复杂度。

  在神经网络中，权重衰减通常是通过L1范数或L2范数来实现的。具体来说：

  1. L1权重衰减（L1 Regularization）：它向损失函数添加了权重参数的绝对值之和，乘以一个正则化超参数λ。这会促使模型中的一些权重变得接近于零，从而实现了稀疏性，即某些权重会被归零，起到特征选择的作用。
  2. L2权重衰减（L2 Regularization）：它向损失函数添加了权重参数的平方和，乘以一个正则化超参数λ。L2权重衰减有助于避免权重过大，从而防止模型过拟合。它也被称为权重衰减项或岭回归。
  3. 使用均方范数作为硬性限制
     - 通过限制参数值的选择范围来控制模型容量
     - 通常不限制偏移b

  通过调整正则化超参数λ的值，可以控制正则化的程度，从而平衡拟合训练数据和模型的复杂度之间的关系。

- 权重衰减对最优解的影响
<<<<<<< HEAD
- <img src="https://raw.githubusercontent.com/poinne/md-pic/main/7b.png" style="zoom:50%;" />
=======
- <img src="https://raw.githubusercontent.com/poinne/md-pic/main/7b.png" style="zoom:50%;" />
>>>>>>> b90bd3d (实验室提交)

未加入正则项时，权重的最优解是根据损失函数的最小值求得。

加入正则项后，权重的最优解是根据（损失函数+正则项）的最小值求得，因为离原点越近，正则项越小，所以最终最优解的值会被正则项拉的距离原点更近，因此对应的权重总体就会更小，模型的复杂度就降低了。（相当于限制了权重的取值范围，使得拟合的曲线相对在一个较小的范围波动，整体比较平滑）lamda的取值一般是1e-3、1e-4左右即可。

对于复杂的模型，权重衰退并不会起到多么大的作用。

- dropout:
<<<<<<< HEAD
  - <img src="https://raw.githubusercontent.com/poinne/md-pic/main/2b.png" style="zoom:50%;" />
=======
  - <img src="https://raw.githubusercontent.com/poinne/md-pic/main/2b.png" style="zoom:50%;" />
>>>>>>> b90bd3d (实验室提交)

 这样可以保持总的期望不变.

- 丢弃法将一些输出项随机置0来控制模型复杂度
- 常作用在多层感知机的隐藏层输出上
- 丢弃概率是控制模型复杂度的超参数

```py
# dropout实现

def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
        return X
    # mask是一个元素值为0或1的向量，0的个数为dropout的程度。
    # 最后 / 1 - dropout 是为了保持期望E不变
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)
```

- dropout 随机置0对求梯度和反向传播的影响是什么？

  每次被丢弃的节点将不会参与梯度更新

- dropout在训练时开，在推理时不开。因为开dropout是为了降低模型的复杂度，防止过拟合，推理时（预测时）模型的权重不会更新，索引不用开，但是想开也可以，只是开了之后因为会随机关闭一些节点，所以可能会导致预测不准。（当然，训练时开也会导致不准，但是因为训练时训练的次数足够多，因此最终的结果不会出现差错）【**推理时使用dropout可以试一下，在生成模型时有没有什么效果**】
- dropout主要在全连接层使用。
- 比如本来想要一个64的层，不使用dropout，可以设成128，开dropout = 0.5,可能比原来效果更好。



- 数值稳定性

<<<<<<< HEAD
  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/3b.png" style="zoom:50%;" />
=======
  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/3b.png" style="zoom:50%;" />
>>>>>>> b90bd3d (实验室提交)

- 参数化对称性。当神经网络的结构具有对称性时，意味着我们可以对网络的参数进行重排列，但最终的函数行为保持不变。当隐藏层节点初始化为相同的值时，这种对称性可能会降低网络的表达能力。

- nan一般是除0，inf一般是太大（这里都指梯度），解决方法：出现这种情况可以降低学习率，一直往小调，或者合理的初始化权重，使其满足均值为0，方差尽量小。

- 权重参数的初始化一般使用Xavier初始化比较好。
- torch.matmul可以用在更多维度的矩阵运算，mm只能在两纬



- 对全连接层使用平移不变性和局部性得到卷积层

- 卷积核一般选3\*3 或者 5\*5

- 填充

<<<<<<< HEAD
  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/14b.png" style="zoom:50%;" />
=======
  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/14b.png" style="zoom:50%;" />
>>>>>>> b90bd3d (实验室提交)

- 填充通常设成核 **-**1，这样可以使输入输出的尺寸相同，方便计算。
- 如果图片比较大，使用小的卷积核需要很多步（层）才能将图片变得比较小，那么可以设置步幅，（一般步幅取2）

- 卷积核一般不用自己设计，直接使用经典的网络结构，或对其进行微调即可。

- 1\*1卷积层不识别空间模型，只是融合通道

- 当以每像素为基础应用时，1×1卷积层相当于全连接层。

<<<<<<< HEAD
  ![](https://raw.githubusercontent.com/poinne/md-pic/main/11b.png)

- 完整的二维卷积层

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/12b.png" style="zoom:67%;" />
=======
  ![](https://raw.githubusercontent.com/poinne/md-pic/main/11b.png)

- 完整的二维卷积层

  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/12b.png" style="zoom:67%;" />
>>>>>>> b90bd3d (实验室提交)

  2d卷积，输入和输出都是三维，核需要是4维，其中$c_o$与输出通道相关，与输入无关，$c_i$与输入有关，与输出无关。

  同理，3d卷积，输入和输出都是四维，核是5维。

  

- 卷积时间复杂度

<<<<<<< HEAD
  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/15b.png" style="zoom:67%;" />
=======
  <img src="https://raw.githubusercontent.com/poinne/md-pic/main/15b.png" style="zoom:67%;" />
>>>>>>> b90bd3d (实验室提交)

- 普通的rgb图像，使用的是2d卷积，即conv2d，如果是一个rgb图像加上深度图，则需要使用3d卷积，即conv3d。

- 多输入通道与多输出通道是不同的。

  - **多输入通道：**当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。**最终得到的不同通道的特征图会进行相加，作为一个通道输出。**

<<<<<<< HEAD
    <img src="https://raw.githubusercontent.com/poinne/md-pic/main/13b.png" style="zoom:80%;" />
=======
    <img src="C:\Users\YS\Desktop\note\image_list\ML-image\14b.png" style="zoom:80%;" />
>>>>>>> b90bd3d (实验室提交)

  - **多输出通道：**直观地说，我们可以将每个通道看作对不同特征的响应。
  - 多输入通道相当于一个核有多个层（当然，每个层的参数是不同的），而多输出通道相当于有多个核。



- 池化

  - 池化层返回窗口中最大或平均值

  - 环节卷积层对位置的敏感性

  - 没有特征融合等操作，输入通道数一定等于输出通道数。（融合等操作交给卷积来做就可以了）

  - 可以指定池化层的填充和步幅

  - 深度学习框架（pytorch）中默认的步幅与池化窗口的大小相同。(即相邻两次采样的窗口不重叠)。但是可以手动设定

    ```py
    # 第一个参数是窗口大小
    pool2d = nn.MaxPool2d((2,3),padding=(1,1), stride=(2,3))
    ```

  - 池化层一般是放在卷积后面
  - 池化层可以减少计算量，使卷积层对位置不那么敏感，但是现在通常用一个卷积+步幅来减少计算量，现在通过一些操作使得卷积层不会过度拟合到某个位置，所以池化层现在用的不多了。
  - 卷积模型相比于mlp更加简洁



- LeNet
- AlexNet
- 相比于LeNet：
  - 使用dropout
  - 将sigmoid改为ReLU
  - 将Avgpooling改为MaxPooling
  - 计算机视觉方法论的改变(从人工特征提取--》通过CNN学习特征)
  - AlexNet是更大更深的LeNet，10 \*参数个数，260\*计算复杂度

<<<<<<< HEAD
<img src="https://raw.githubusercontent.com/poinne/md-pic/main/10b.png" style="zoom:67%;" />
=======
<img src="https://raw.githubusercontent.com/poinne/md-pic/main/10b.png" style="zoom:67%;" />
>>>>>>> b90bd3d (实验室提交)



- VGG
- vgg是更深的alexnet，将alexnet中新增的卷积和池化操作包装为一个vgg block，整体的结构是多个vgg block + 最后的三个全连接层。
- vgg使用可重复使用的卷积块来构建深度卷积神经网络
- 不同的卷积块个数和超参数可以得到不同复杂度的变种

**AlexNet的模型较为简单，参数较少，适合处理较小的图像数据集；而VGG则更加深层，参数更多，适合处理更大的图像数据集**。

- 图像尺寸减半，同时通道数指数增长，可以很好地保留特征。



- NiN

<<<<<<< HEAD
<img src="https://raw.githubusercontent.com/poinne/md-pic/main/16b.png" style="zoom:80%;" />
=======
<img src="https://raw.githubusercontent.com/poinne/md-pic/main/16b.png" style="zoom:80%;" />
>>>>>>> b90bd3d (实验室提交)

- NiN块使用卷积层加两个1\*1卷积层
  - 1\*1卷积层相当于参数受限的全连接层
  - 1\*1卷积层不改变输入输出通道，相当于n个输入通道进行特征融合后输出，共n个
  - 后者对每个像素增加了非线性性
- NiN使用全局平均池化层来代替VGG和AlexNet中的全连接层
  - 不容易过拟合，更少的参数个数